{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) 데이터 가져오기  \n",
    "sklearn.datasets의 load_diabetes에서 데이터를 가져와주세요.  \n",
    "diabetes의 data를 df_X에, target을 df_y에 저장해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = diabetes.data\n",
    "df_y = diabetes.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) 모델에 입력할 데이터 X 준비하기  \n",
    "df_X에 있는 값들을 numpy array로 변환해서 저장해주세요.\n",
    "\n",
    "## (3) 모델에 예측할 데이터 y 준비하기  \n",
    "df_y에 있는 값들을 numpy array로 변환해서 저장해주세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape_x (442, 10)\n",
      "shape_y (442,)\n",
      "feature 1 : age\n",
      "feature 2 : sex\n",
      "feature 3 : bmi\n",
      "feature 4 : bp\n",
      "feature 5 : s1\n",
      "feature 6 : s2\n",
      "feature 7 : s3\n",
      "feature 8 : s4\n",
      "feature 9 : s5\n",
      "feature 10 : s6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# np array 변경 \n",
    "df_X = np.array(df_X)\n",
    "df_y = np.array(df_y)\n",
    "\n",
    "W = np.random.rand(10)\n",
    "b = np.random.rand()\n",
    "\n",
    "# 데이터 shape, feature 확인\n",
    "print('shape_x',df_X.shape)\n",
    "print('shape_y',df_y.shape)\n",
    "\n",
    "for i, feature_name in enumerate(diabetes.feature_names):\n",
    "    print(f'feature {i+1} : {feature_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s1 - s6 : 당뇨병 진행과 관련된 혈액 내 물질의 농도를 의미하며, 혈당 수치와 관련된 대사 지표로 사용. 다음과 같은 생화학적 수치를 나타낸다.\n",
    "\n",
    "\ts1: 총 콜레스테롤\n",
    "\ts2: 저밀도 지질단백질(LDL) 콜레스테롤\n",
    "\ts3: 고밀도 지질단백질(HDL) 콜레스테롤\n",
    "\ts4: 혈청 트리글리세라이드(중성지방) 수치\n",
    "\ts5: 혈당 수치\n",
    "\ts6: 당화혈색소(HbA1c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('<x_data[0]> : ',x_data[0])\n",
    "print()\n",
    "print('<y_data[0]> : ',y_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) train 데이터와 test 데이터로 분리하기  \n",
    "X와 y 데이터를 각각 train 데이터와 test 데이터로 분리해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.13531305, 0.88097429, 0.48214157, 0.20702874, 0.75024677,\n",
       "        0.83619975, 0.58298429, 0.98317353, 0.56397309, 0.25428648]),\n",
       " 0.44928268358536183)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W, b # 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) 모델 준비하기  \n",
    "입력 데이터 개수에 맞는 가중치 W와 b를 준비해주세요.  \n",
    "모델 함수를 구현해주세요.\n",
    "\n",
    "1. 당뇨병 수치 -> 선형회귀 모델 선정\n",
    "2. feature 10개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_diabetes(X, W, b): # y = w1x1 + w2x2 ... w10x10 + b \n",
    "                             # 우리는 결국 w와 b 값을 찾고 싶은 것\n",
    "    predictions = 0\n",
    "    for i in range(10):\n",
    "        predictions += X[:, i] * W[i]\n",
    "    predictions += b\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) 손실함수 loss 정의하기  \n",
    "손실함수를 MSE 함수로 정의해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(a, b):\n",
    "    mse = ((a - b) ** 2).mean()  # 두 값의 차이의 제곱의 평균\n",
    "    return mse\n",
    "\n",
    "def loss(X, W, b, y):\n",
    "    predictions = model_diabetes(X, W, b)\n",
    "    L = MSE(predictions, y)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7) 기울기를 구하는 gradient 함수 구현하기  \n",
    "기울기를 계산하는 gradient 함수를 구현해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, W, b, y):\n",
    "    # N은 데이터 포인트의 개수\n",
    "    N = len(y)\n",
    "    \n",
    "    # y_pred 준비\n",
    "    y_pred = model_diabetes(X, W, b)\n",
    "    \n",
    "    # 공식에 맞게 gradient 계산\n",
    "    dW = 1/N * 2 * X.T.dot(y_pred - y)\n",
    "        \n",
    "    # b의 gradient 계산\n",
    "    db = 2 * (y_pred - y).mean()\n",
    "    return dW, db\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(8) 하이퍼 파라미터인 학습률 설정하기  \n",
    "학습률, learning rate 를 설정해주세요  \n",
    "만약 학습이 잘 되지 않는다면 learning rate 값을 한번 여러 가지로 설정하며 실험해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate 값 3가지 설정 0.01 ~ 0.0001\n",
    "LEARNING_RATE = 0.0001\n",
    "LEARNING_RATE2 = 0.001\n",
    "LEARNING_RATE3 = 0.01\n",
    "LEARNING_RATE4 = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(9) 모델 학습하기  \n",
    "정의된 손실함수와 기울기 함수로 모델을 학습해주세요.  \n",
    "loss값이 충분히 떨어질 때까지 학습을 진행해주세요.  \n",
    "입력하는 데이터인 X에 들어가는 특성 컬럼들을 몇 개 빼도 괜찮습니다. 다양한 데이터로 실험해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 : Loss 21743.8498\n",
      "Iteration 20 : Loss 16520.9271\n",
      "Iteration 30 : Loss 13031.4754\n",
      "Iteration 40 : Loss 10699.3043\n",
      "Iteration 50 : Loss 9139.7470\n",
      "Iteration 60 : Loss 8095.9981\n",
      "Iteration 70 : Loss 7396.6132\n",
      "Iteration 80 : Loss 6927.1353\n",
      "Iteration 90 : Loss 6611.1524\n",
      "Iteration 100 : Loss 6397.6515\n",
      "Iteration 110 : Loss 6252.5761\n",
      "Iteration 120 : Loss 6153.1898\n",
      "Iteration 130 : Loss 6084.3138\n",
      "Iteration 140 : Loss 6035.8145\n",
      "Iteration 150 : Loss 6000.9268\n",
      "Iteration 160 : Loss 5975.1341\n",
      "Iteration 170 : Loss 5955.4212\n",
      "Iteration 180 : Loss 5939.7750\n",
      "Iteration 190 : Loss 5926.8516\n",
      "Iteration 200 : Loss 5915.7537\n",
      "Iteration 210 : Loss 5905.8824\n",
      "Iteration 220 : Loss 5896.8377\n",
      "Iteration 230 : Loss 5888.3525\n",
      "Iteration 240 : Loss 5880.2486\n",
      "Iteration 250 : Loss 5872.4068\n",
      "Iteration 260 : Loss 5864.7478\n",
      "Iteration 270 : Loss 5857.2183\n",
      "Iteration 280 : Loss 5849.7830\n",
      "Iteration 290 : Loss 5842.4180\n",
      "Iteration 300 : Loss 5835.1075\n",
      "Iteration 310 : Loss 5827.8410\n",
      "Iteration 320 : Loss 5820.6113\n",
      "Iteration 330 : Loss 5813.4136\n",
      "Iteration 340 : Loss 5806.2447\n",
      "Iteration 350 : Loss 5799.1024\n",
      "Iteration 360 : Loss 5791.9854\n",
      "Iteration 370 : Loss 5784.8925\n",
      "Iteration 380 : Loss 5777.8230\n",
      "Iteration 390 : Loss 5770.7766\n",
      "Iteration 400 : Loss 5763.7527\n",
      "Iteration 410 : Loss 5756.7512\n",
      "Iteration 420 : Loss 5749.7719\n",
      "Iteration 430 : Loss 5742.8146\n",
      "Iteration 440 : Loss 5735.8791\n",
      "Iteration 450 : Loss 5728.9654\n",
      "Iteration 460 : Loss 5722.0734\n",
      "Iteration 470 : Loss 5715.2029\n",
      "Iteration 480 : Loss 5708.3539\n",
      "Iteration 490 : Loss 5701.5264\n",
      "Iteration 500 : Loss 5694.7201\n",
      "Iteration 510 : Loss 5687.9351\n",
      "Iteration 520 : Loss 5681.1712\n",
      "Iteration 530 : Loss 5674.4285\n",
      "Iteration 540 : Loss 5667.7068\n",
      "Iteration 550 : Loss 5661.0060\n",
      "Iteration 560 : Loss 5654.3261\n",
      "Iteration 570 : Loss 5647.6671\n",
      "Iteration 580 : Loss 5641.0287\n",
      "Iteration 590 : Loss 5634.4110\n",
      "Iteration 600 : Loss 5627.8139\n",
      "Iteration 610 : Loss 5621.2374\n",
      "Iteration 620 : Loss 5614.6813\n",
      "Iteration 630 : Loss 5608.1455\n",
      "Iteration 640 : Loss 5601.6301\n",
      "Iteration 650 : Loss 5595.1349\n",
      "Iteration 660 : Loss 5588.6599\n",
      "Iteration 670 : Loss 5582.2050\n",
      "Iteration 680 : Loss 5575.7701\n",
      "Iteration 690 : Loss 5569.3552\n",
      "Iteration 700 : Loss 5562.9602\n",
      "Iteration 710 : Loss 5556.5850\n",
      "Iteration 720 : Loss 5550.2296\n",
      "Iteration 730 : Loss 5543.8939\n",
      "Iteration 740 : Loss 5537.5778\n",
      "Iteration 750 : Loss 5531.2812\n",
      "Iteration 760 : Loss 5525.0042\n",
      "Iteration 770 : Loss 5518.7466\n",
      "Iteration 780 : Loss 5512.5083\n",
      "Iteration 790 : Loss 5506.2893\n",
      "Iteration 800 : Loss 5500.0895\n",
      "Iteration 810 : Loss 5493.9089\n",
      "Iteration 820 : Loss 5487.7474\n",
      "Iteration 830 : Loss 5481.6050\n",
      "Iteration 840 : Loss 5475.4814\n",
      "Iteration 850 : Loss 5469.3768\n",
      "Iteration 860 : Loss 5463.2910\n",
      "Iteration 870 : Loss 5457.2240\n",
      "Iteration 880 : Loss 5451.1757\n",
      "Iteration 890 : Loss 5445.1460\n",
      "Iteration 900 : Loss 5439.1349\n",
      "Iteration 910 : Loss 5433.1423\n",
      "Iteration 920 : Loss 5427.1682\n",
      "Iteration 930 : Loss 5421.2124\n",
      "Iteration 940 : Loss 5415.2750\n",
      "Iteration 950 : Loss 5409.3558\n",
      "Iteration 960 : Loss 5403.4549\n",
      "Iteration 970 : Loss 5397.5720\n",
      "Iteration 980 : Loss 5391.7073\n",
      "Iteration 990 : Loss 5385.8605\n",
      "Iteration 1000 : Loss 5380.0317\n",
      "Iteration 1010 : Loss 5374.2208\n",
      "Iteration 1020 : Loss 5368.4277\n",
      "Iteration 1030 : Loss 5362.6524\n",
      "Iteration 1040 : Loss 5356.8948\n",
      "Iteration 1050 : Loss 5351.1548\n",
      "Iteration 1060 : Loss 5345.4324\n",
      "Iteration 1070 : Loss 5339.7276\n",
      "Iteration 1080 : Loss 5334.0402\n",
      "Iteration 1090 : Loss 5328.3702\n",
      "Iteration 1100 : Loss 5322.7175\n",
      "Iteration 1110 : Loss 5317.0822\n",
      "Iteration 1120 : Loss 5311.4641\n",
      "Iteration 1130 : Loss 5305.8631\n",
      "Iteration 1140 : Loss 5300.2793\n",
      "Iteration 1150 : Loss 5294.7125\n",
      "Iteration 1160 : Loss 5289.1627\n",
      "Iteration 1170 : Loss 5283.6298\n",
      "Iteration 1180 : Loss 5278.1138\n",
      "Iteration 1190 : Loss 5272.6146\n",
      "Iteration 1200 : Loss 5267.1322\n",
      "Iteration 1210 : Loss 5261.6665\n",
      "Iteration 1220 : Loss 5256.2175\n",
      "Iteration 1230 : Loss 5250.7850\n",
      "Iteration 1240 : Loss 5245.3691\n",
      "Iteration 1250 : Loss 5239.9696\n",
      "Iteration 1260 : Loss 5234.5866\n",
      "Iteration 1270 : Loss 5229.2200\n",
      "Iteration 1280 : Loss 5223.8696\n",
      "Iteration 1290 : Loss 5218.5355\n",
      "Iteration 1300 : Loss 5213.2176\n",
      "Iteration 1310 : Loss 5207.9159\n",
      "Iteration 1320 : Loss 5202.6303\n",
      "Iteration 1330 : Loss 5197.3606\n",
      "Iteration 1340 : Loss 5192.1070\n",
      "Iteration 1350 : Loss 5186.8693\n",
      "Iteration 1360 : Loss 5181.6474\n",
      "Iteration 1370 : Loss 5176.4414\n",
      "Iteration 1380 : Loss 5171.2512\n",
      "Iteration 1390 : Loss 5166.0766\n",
      "Iteration 1400 : Loss 5160.9177\n",
      "Iteration 1410 : Loss 5155.7744\n",
      "Iteration 1420 : Loss 5150.6467\n",
      "Iteration 1430 : Loss 5145.5345\n",
      "Iteration 1440 : Loss 5140.4377\n",
      "Iteration 1450 : Loss 5135.3563\n",
      "Iteration 1460 : Loss 5130.2902\n",
      "Iteration 1470 : Loss 5125.2394\n",
      "Iteration 1480 : Loss 5120.2039\n",
      "Iteration 1490 : Loss 5115.1836\n",
      "Iteration 1500 : Loss 5110.1783\n",
      "Iteration 1510 : Loss 5105.1882\n",
      "Iteration 1520 : Loss 5100.2131\n",
      "Iteration 1530 : Loss 5095.2530\n",
      "Iteration 1540 : Loss 5090.3078\n",
      "Iteration 1550 : Loss 5085.3775\n",
      "Iteration 1560 : Loss 5080.4620\n",
      "Iteration 1570 : Loss 5075.5612\n",
      "Iteration 1580 : Loss 5070.6752\n",
      "Iteration 1590 : Loss 5065.8039\n",
      "Iteration 1600 : Loss 5060.9472\n",
      "Iteration 1610 : Loss 5056.1051\n",
      "Iteration 1620 : Loss 5051.2775\n",
      "Iteration 1630 : Loss 5046.4644\n",
      "Iteration 1640 : Loss 5041.6658\n",
      "Iteration 1650 : Loss 5036.8815\n",
      "Iteration 1660 : Loss 5032.1115\n",
      "Iteration 1670 : Loss 5027.3558\n",
      "Iteration 1680 : Loss 5022.6144\n",
      "Iteration 1690 : Loss 5017.8871\n",
      "Iteration 1700 : Loss 5013.1740\n",
      "Iteration 1710 : Loss 5008.4750\n",
      "Iteration 1720 : Loss 5003.7900\n",
      "Iteration 1730 : Loss 4999.1190\n",
      "Iteration 1740 : Loss 4994.4619\n",
      "Iteration 1750 : Loss 4989.8188\n",
      "Iteration 1760 : Loss 4985.1895\n",
      "Iteration 1770 : Loss 4980.5740\n",
      "Iteration 1780 : Loss 4975.9723\n",
      "Iteration 1790 : Loss 4971.3843\n",
      "Iteration 1800 : Loss 4966.8099\n",
      "Iteration 1810 : Loss 4962.2492\n",
      "Iteration 1820 : Loss 4957.7021\n",
      "Iteration 1830 : Loss 4953.1684\n",
      "Iteration 1840 : Loss 4948.6483\n",
      "Iteration 1850 : Loss 4944.1416\n",
      "Iteration 1860 : Loss 4939.6483\n",
      "Iteration 1870 : Loss 4935.1683\n",
      "Iteration 1880 : Loss 4930.7017\n",
      "Iteration 1890 : Loss 4926.2483\n",
      "Iteration 1900 : Loss 4921.8081\n",
      "Iteration 1910 : Loss 4917.3811\n",
      "Iteration 1920 : Loss 4912.9672\n",
      "Iteration 1930 : Loss 4908.5663\n",
      "Iteration 1940 : Loss 4904.1785\n",
      "Iteration 1950 : Loss 4899.8037\n",
      "Iteration 1960 : Loss 4895.4419\n",
      "Iteration 1970 : Loss 4891.0929\n",
      "Iteration 1980 : Loss 4886.7568\n",
      "Iteration 1990 : Loss 4882.4336\n",
      "Iteration 2000 : Loss 4878.1231\n",
      "Iteration 2010 : Loss 4873.8253\n",
      "Iteration 2020 : Loss 4869.5402\n",
      "Iteration 2030 : Loss 4865.2678\n",
      "Iteration 2040 : Loss 4861.0079\n",
      "Iteration 2050 : Loss 4856.7606\n",
      "Iteration 2060 : Loss 4852.5259\n",
      "Iteration 2070 : Loss 4848.3036\n",
      "Iteration 2080 : Loss 4844.0937\n",
      "Iteration 2090 : Loss 4839.8963\n",
      "Iteration 2100 : Loss 4835.7111\n",
      "Iteration 2110 : Loss 4831.5383\n",
      "Iteration 2120 : Loss 4827.3778\n",
      "Iteration 2130 : Loss 4823.2294\n",
      "Iteration 2140 : Loss 4819.0933\n",
      "Iteration 2150 : Loss 4814.9693\n",
      "Iteration 2160 : Loss 4810.8574\n",
      "Iteration 2170 : Loss 4806.7576\n",
      "Iteration 2180 : Loss 4802.6697\n",
      "Iteration 2190 : Loss 4798.5939\n",
      "Iteration 2200 : Loss 4794.5300\n",
      "Iteration 2210 : Loss 4790.4780\n",
      "Iteration 2220 : Loss 4786.4379\n",
      "Iteration 2230 : Loss 4782.4095\n",
      "Iteration 2240 : Loss 4778.3930\n",
      "Iteration 2250 : Loss 4774.3882\n",
      "Iteration 2260 : Loss 4770.3951\n",
      "Iteration 2270 : Loss 4766.4137\n",
      "Iteration 2280 : Loss 4762.4438\n",
      "Iteration 2290 : Loss 4758.4856\n",
      "Iteration 2300 : Loss 4754.5389\n",
      "Iteration 2310 : Loss 4750.6037\n",
      "Iteration 2320 : Loss 4746.6800\n",
      "Iteration 2330 : Loss 4742.7677\n",
      "Iteration 2340 : Loss 4738.8668\n",
      "Iteration 2350 : Loss 4734.9773\n",
      "Iteration 2360 : Loss 4731.0990\n",
      "Iteration 2370 : Loss 4727.2321\n",
      "Iteration 2380 : Loss 4723.3763\n",
      "Iteration 2390 : Loss 4719.5318\n",
      "Iteration 2400 : Loss 4715.6985\n",
      "Iteration 2410 : Loss 4711.8762\n",
      "Iteration 2420 : Loss 4708.0651\n",
      "Iteration 2430 : Loss 4704.2650\n",
      "Iteration 2440 : Loss 4700.4759\n",
      "Iteration 2450 : Loss 4696.6978\n",
      "Iteration 2460 : Loss 4692.9306\n",
      "Iteration 2470 : Loss 4689.1744\n",
      "Iteration 2480 : Loss 4685.4290\n",
      "Iteration 2490 : Loss 4681.6944\n",
      "Iteration 2500 : Loss 4677.9707\n",
      "Iteration 2510 : Loss 4674.2577\n",
      "Iteration 2520 : Loss 4670.5554\n",
      "Iteration 2530 : Loss 4666.8638\n",
      "Iteration 2540 : Loss 4663.1828\n",
      "Iteration 2550 : Loss 4659.5125\n",
      "Iteration 2560 : Loss 4655.8527\n",
      "Iteration 2570 : Loss 4652.2035\n",
      "Iteration 2580 : Loss 4648.5648\n",
      "Iteration 2590 : Loss 4644.9366\n",
      "Iteration 2600 : Loss 4641.3188\n",
      "Iteration 2610 : Loss 4637.7114\n",
      "Iteration 2620 : Loss 4634.1144\n",
      "Iteration 2630 : Loss 4630.5277\n",
      "Iteration 2640 : Loss 4626.9513\n",
      "Iteration 2650 : Loss 4623.3852\n",
      "Iteration 2660 : Loss 4619.8293\n",
      "Iteration 2670 : Loss 4616.2837\n",
      "Iteration 2680 : Loss 4612.7481\n",
      "Iteration 2690 : Loss 4609.2227\n",
      "Iteration 2700 : Loss 4605.7074\n",
      "Iteration 2710 : Loss 4602.2022\n",
      "Iteration 2720 : Loss 4598.7070\n",
      "Iteration 2730 : Loss 4595.2218\n",
      "Iteration 2740 : Loss 4591.7466\n",
      "Iteration 2750 : Loss 4588.2812\n",
      "Iteration 2760 : Loss 4584.8258\n",
      "Iteration 2770 : Loss 4581.3803\n",
      "Iteration 2780 : Loss 4577.9445\n",
      "Iteration 2790 : Loss 4574.5186\n",
      "Iteration 2800 : Loss 4571.1024\n",
      "Iteration 2810 : Loss 4567.6960\n",
      "Iteration 2820 : Loss 4564.2992\n",
      "Iteration 2830 : Loss 4560.9122\n",
      "Iteration 2840 : Loss 4557.5347\n",
      "Iteration 2850 : Loss 4554.1669\n",
      "Iteration 2860 : Loss 4550.8086\n",
      "Iteration 2870 : Loss 4547.4599\n",
      "Iteration 2880 : Loss 4544.1207\n",
      "Iteration 2890 : Loss 4540.7910\n",
      "Iteration 2900 : Loss 4537.4707\n",
      "Iteration 2910 : Loss 4534.1598\n",
      "Iteration 2920 : Loss 4530.8583\n",
      "Iteration 2930 : Loss 4527.5662\n",
      "Iteration 2940 : Loss 4524.2833\n",
      "Iteration 2950 : Loss 4521.0098\n",
      "Iteration 2960 : Loss 4517.7455\n",
      "Iteration 2970 : Loss 4514.4905\n",
      "Iteration 2980 : Loss 4511.2446\n",
      "Iteration 2990 : Loss 4508.0079\n",
      "Iteration 3000 : Loss 4504.7804\n",
      "Iteration 3010 : Loss 4501.5619\n",
      "Iteration 3020 : Loss 4498.3526\n",
      "Iteration 3030 : Loss 4495.1522\n",
      "Iteration 3040 : Loss 4491.9609\n",
      "Iteration 3050 : Loss 4488.7786\n",
      "Iteration 3060 : Loss 4485.6052\n",
      "Iteration 3070 : Loss 4482.4408\n",
      "Iteration 3080 : Loss 4479.2852\n",
      "Iteration 3090 : Loss 4476.1386\n",
      "Iteration 3100 : Loss 4473.0007\n",
      "Iteration 3110 : Loss 4469.8717\n",
      "Iteration 3120 : Loss 4466.7514\n",
      "Iteration 3130 : Loss 4463.6399\n",
      "Iteration 3140 : Loss 4460.5372\n",
      "Iteration 3150 : Loss 4457.4431\n",
      "Iteration 3160 : Loss 4454.3576\n",
      "Iteration 3170 : Loss 4451.2809\n",
      "Iteration 3180 : Loss 4448.2127\n",
      "Iteration 3190 : Loss 4445.1531\n",
      "Iteration 3200 : Loss 4442.1020\n",
      "Iteration 3210 : Loss 4439.0595\n",
      "Iteration 3220 : Loss 4436.0254\n",
      "Iteration 3230 : Loss 4432.9999\n",
      "Iteration 3240 : Loss 4429.9827\n",
      "Iteration 3250 : Loss 4426.9740\n",
      "Iteration 3260 : Loss 4423.9737\n",
      "Iteration 3270 : Loss 4420.9817\n",
      "Iteration 3280 : Loss 4417.9980\n",
      "Iteration 3290 : Loss 4415.0226\n",
      "Iteration 3300 : Loss 4412.0555\n",
      "Iteration 3310 : Loss 4409.0967\n",
      "Iteration 3320 : Loss 4406.1461\n",
      "Iteration 3330 : Loss 4403.2036\n",
      "Iteration 3340 : Loss 4400.2693\n",
      "Iteration 3350 : Loss 4397.3432\n",
      "Iteration 3360 : Loss 4394.4252\n",
      "Iteration 3370 : Loss 4391.5152\n",
      "Iteration 3380 : Loss 4388.6133\n",
      "Iteration 3390 : Loss 4385.7195\n",
      "Iteration 3400 : Loss 4382.8336\n",
      "Iteration 3410 : Loss 4379.9557\n",
      "Iteration 3420 : Loss 4377.0858\n",
      "Iteration 3430 : Loss 4374.2238\n",
      "Iteration 3440 : Loss 4371.3697\n",
      "Iteration 3450 : Loss 4368.5235\n",
      "Iteration 3460 : Loss 4365.6851\n",
      "Iteration 3470 : Loss 4362.8545\n",
      "Iteration 3480 : Loss 4360.0318\n",
      "Iteration 3490 : Loss 4357.2168\n",
      "Iteration 3500 : Loss 4354.4095\n",
      "Iteration 3510 : Loss 4351.6100\n",
      "Iteration 3520 : Loss 4348.8181\n",
      "Iteration 3530 : Loss 4346.0340\n",
      "Iteration 3540 : Loss 4343.2574\n",
      "Iteration 3550 : Loss 4340.4885\n",
      "Iteration 3560 : Loss 4337.7272\n",
      "Iteration 3570 : Loss 4334.9735\n",
      "Iteration 3580 : Loss 4332.2273\n",
      "Iteration 3590 : Loss 4329.4886\n",
      "Iteration 3600 : Loss 4326.7574\n",
      "Iteration 3610 : Loss 4324.0337\n",
      "Iteration 3620 : Loss 4321.3174\n",
      "Iteration 3630 : Loss 4318.6085\n",
      "Iteration 3640 : Loss 4315.9071\n",
      "Iteration 3650 : Loss 4313.2130\n",
      "Iteration 3660 : Loss 4310.5262\n",
      "Iteration 3670 : Loss 4307.8468\n",
      "Iteration 3680 : Loss 4305.1747\n",
      "Iteration 3690 : Loss 4302.5099\n",
      "Iteration 3700 : Loss 4299.8523\n",
      "Iteration 3710 : Loss 4297.2019\n",
      "Iteration 3720 : Loss 4294.5588\n",
      "Iteration 3730 : Loss 4291.9228\n",
      "Iteration 3740 : Loss 4289.2940\n",
      "Iteration 3750 : Loss 4286.6723\n",
      "Iteration 3760 : Loss 4284.0578\n",
      "Iteration 3770 : Loss 4281.4503\n",
      "Iteration 3780 : Loss 4278.8499\n",
      "Iteration 3790 : Loss 4276.2565\n",
      "Iteration 3800 : Loss 4273.6702\n",
      "Iteration 3810 : Loss 4271.0908\n",
      "Iteration 3820 : Loss 4268.5185\n",
      "Iteration 3830 : Loss 4265.9530\n",
      "Iteration 3840 : Loss 4263.3945\n",
      "Iteration 3850 : Loss 4260.8429\n",
      "Iteration 3860 : Loss 4258.2982\n",
      "Iteration 3870 : Loss 4255.7604\n",
      "Iteration 3880 : Loss 4253.2294\n",
      "Iteration 3890 : Loss 4250.7052\n",
      "Iteration 3900 : Loss 4248.1878\n",
      "Iteration 3910 : Loss 4245.6772\n",
      "Iteration 3920 : Loss 4243.1733\n",
      "Iteration 3930 : Loss 4240.6761\n",
      "Iteration 3940 : Loss 4238.1857\n",
      "Iteration 3950 : Loss 4235.7019\n",
      "Iteration 3960 : Loss 4233.2248\n",
      "Iteration 3970 : Loss 4230.7544\n",
      "Iteration 3980 : Loss 4228.2905\n",
      "Iteration 3990 : Loss 4225.8333\n",
      "Iteration 4000 : Loss 4223.3826\n",
      "Iteration 4010 : Loss 4220.9385\n",
      "Iteration 4020 : Loss 4218.5010\n",
      "Iteration 4030 : Loss 4216.0699\n",
      "Iteration 4040 : Loss 4213.6453\n",
      "Iteration 4050 : Loss 4211.2272\n",
      "Iteration 4060 : Loss 4208.8156\n",
      "Iteration 4070 : Loss 4206.4104\n",
      "Iteration 4080 : Loss 4204.0116\n",
      "Iteration 4090 : Loss 4201.6192\n",
      "Iteration 4100 : Loss 4199.2331\n",
      "Iteration 4110 : Loss 4196.8534\n",
      "Iteration 4120 : Loss 4194.4800\n",
      "Iteration 4130 : Loss 4192.1129\n",
      "Iteration 4140 : Loss 4189.7521\n",
      "Iteration 4150 : Loss 4187.3976\n",
      "Iteration 4160 : Loss 4185.0493\n",
      "Iteration 4170 : Loss 4182.7072\n",
      "Iteration 4180 : Loss 4180.3714\n",
      "Iteration 4190 : Loss 4178.0417\n",
      "Iteration 4200 : Loss 4175.7182\n",
      "Iteration 4210 : Loss 4173.4008\n",
      "Iteration 4220 : Loss 4171.0896\n",
      "Iteration 4230 : Loss 4168.7844\n",
      "Iteration 4240 : Loss 4166.4854\n",
      "Iteration 4250 : Loss 4164.1924\n",
      "Iteration 4260 : Loss 4161.9054\n",
      "Iteration 4270 : Loss 4159.6245\n",
      "Iteration 4280 : Loss 4157.3496\n",
      "Iteration 4290 : Loss 4155.0807\n",
      "Iteration 4300 : Loss 4152.8177\n",
      "Iteration 4310 : Loss 4150.5607\n",
      "Iteration 4320 : Loss 4148.3096\n",
      "Iteration 4330 : Loss 4146.0644\n",
      "Iteration 4340 : Loss 4143.8251\n",
      "Iteration 4350 : Loss 4141.5917\n",
      "Iteration 4360 : Loss 4139.3642\n",
      "Iteration 4370 : Loss 4137.1425\n",
      "Iteration 4380 : Loss 4134.9265\n",
      "Iteration 4390 : Loss 4132.7164\n",
      "Iteration 4400 : Loss 4130.5121\n",
      "Iteration 4410 : Loss 4128.3135\n",
      "Iteration 4420 : Loss 4126.1207\n",
      "Iteration 4430 : Loss 4123.9336\n",
      "Iteration 4440 : Loss 4121.7522\n",
      "Iteration 4450 : Loss 4119.5765\n",
      "Iteration 4460 : Loss 4117.4064\n",
      "Iteration 4470 : Loss 4115.2420\n",
      "Iteration 4480 : Loss 4113.0832\n",
      "Iteration 4490 : Loss 4110.9301\n",
      "Iteration 4500 : Loss 4108.7825\n",
      "Iteration 4510 : Loss 4106.6405\n",
      "Iteration 4520 : Loss 4104.5041\n",
      "Iteration 4530 : Loss 4102.3732\n",
      "Iteration 4540 : Loss 4100.2479\n",
      "Iteration 4550 : Loss 4098.1280\n",
      "Iteration 4560 : Loss 4096.0137\n",
      "Iteration 4570 : Loss 4093.9048\n",
      "Iteration 4580 : Loss 4091.8014\n",
      "Iteration 4590 : Loss 4089.7034\n",
      "Iteration 4600 : Loss 4087.6108\n",
      "Iteration 4610 : Loss 4085.5236\n",
      "Iteration 4620 : Loss 4083.4418\n",
      "Iteration 4630 : Loss 4081.3654\n",
      "Iteration 4640 : Loss 4079.2943\n",
      "Iteration 4650 : Loss 4077.2286\n",
      "Iteration 4660 : Loss 4075.1681\n",
      "Iteration 4670 : Loss 4073.1130\n",
      "Iteration 4680 : Loss 4071.0631\n",
      "Iteration 4690 : Loss 4069.0186\n",
      "Iteration 4700 : Loss 4066.9792\n",
      "Iteration 4710 : Loss 4064.9451\n",
      "Iteration 4720 : Loss 4062.9162\n",
      "Iteration 4730 : Loss 4060.8926\n",
      "Iteration 4740 : Loss 4058.8740\n",
      "Iteration 4750 : Loss 4056.8607\n",
      "Iteration 4760 : Loss 4054.8525\n",
      "Iteration 4770 : Loss 4052.8495\n",
      "Iteration 4780 : Loss 4050.8515\n",
      "Iteration 4790 : Loss 4048.8587\n",
      "Iteration 4800 : Loss 4046.8709\n",
      "Iteration 4810 : Loss 4044.8882\n",
      "Iteration 4820 : Loss 4042.9106\n",
      "Iteration 4830 : Loss 4040.9380\n",
      "Iteration 4840 : Loss 4038.9704\n",
      "Iteration 4850 : Loss 4037.0078\n",
      "Iteration 4860 : Loss 4035.0502\n",
      "Iteration 4870 : Loss 4033.0976\n",
      "Iteration 4880 : Loss 4031.1499\n",
      "Iteration 4890 : Loss 4029.2072\n",
      "Iteration 4900 : Loss 4027.2694\n",
      "Iteration 4910 : Loss 4025.3365\n",
      "Iteration 4920 : Loss 4023.4085\n",
      "Iteration 4930 : Loss 4021.4854\n",
      "Iteration 4940 : Loss 4019.5671\n",
      "Iteration 4950 : Loss 4017.6537\n",
      "Iteration 4960 : Loss 4015.7451\n",
      "Iteration 4970 : Loss 4013.8414\n",
      "Iteration 4980 : Loss 4011.9424\n",
      "Iteration 4990 : Loss 4010.0482\n",
      "Iteration 5000 : Loss 4008.1588\n",
      "Iteration 5010 : Loss 4006.2742\n",
      "Iteration 5020 : Loss 4004.3942\n",
      "Iteration 5030 : Loss 4002.5191\n",
      "Iteration 5040 : Loss 4000.6486\n",
      "Iteration 5050 : Loss 3998.7828\n",
      "Iteration 5060 : Loss 3996.9217\n",
      "Iteration 5070 : Loss 3995.0653\n",
      "Iteration 5080 : Loss 3993.2135\n",
      "Iteration 5090 : Loss 3991.3664\n",
      "Iteration 5100 : Loss 3989.5239\n",
      "Iteration 5110 : Loss 3987.6859\n",
      "Iteration 5120 : Loss 3985.8526\n",
      "Iteration 5130 : Loss 3984.0239\n",
      "Iteration 5140 : Loss 3982.1997\n",
      "Iteration 5150 : Loss 3980.3801\n",
      "Iteration 5160 : Loss 3978.5650\n",
      "Iteration 5170 : Loss 3976.7544\n",
      "Iteration 5180 : Loss 3974.9484\n",
      "Iteration 5190 : Loss 3973.1468\n",
      "Iteration 5200 : Loss 3971.3497\n",
      "Iteration 5210 : Loss 3969.5571\n",
      "Iteration 5220 : Loss 3967.7689\n",
      "Iteration 5230 : Loss 3965.9852\n",
      "Iteration 5240 : Loss 3964.2059\n",
      "Iteration 5250 : Loss 3962.4310\n",
      "Iteration 5260 : Loss 3960.6605\n",
      "Iteration 5270 : Loss 3958.8944\n",
      "Iteration 5280 : Loss 3957.1326\n",
      "Iteration 5290 : Loss 3955.3752\n",
      "Iteration 5300 : Loss 3953.6221\n",
      "Iteration 5310 : Loss 3951.8734\n",
      "Iteration 5320 : Loss 3950.1290\n",
      "Iteration 5330 : Loss 3948.3889\n",
      "Iteration 5340 : Loss 3946.6530\n",
      "Iteration 5350 : Loss 3944.9215\n",
      "Iteration 5360 : Loss 3943.1942\n",
      "Iteration 5370 : Loss 3941.4711\n",
      "Iteration 5380 : Loss 3939.7523\n",
      "Iteration 5390 : Loss 3938.0377\n",
      "Iteration 5400 : Loss 3936.3273\n",
      "Iteration 5410 : Loss 3934.6211\n",
      "Iteration 5420 : Loss 3932.9190\n",
      "Iteration 5430 : Loss 3931.2212\n",
      "Iteration 5440 : Loss 3929.5275\n",
      "Iteration 5450 : Loss 3927.8379\n",
      "Iteration 5460 : Loss 3926.1525\n",
      "Iteration 5470 : Loss 3924.4712\n",
      "Iteration 5480 : Loss 3922.7939\n",
      "Iteration 5490 : Loss 3921.1208\n",
      "Iteration 5500 : Loss 3919.4518\n",
      "Iteration 5510 : Loss 3917.7868\n",
      "Iteration 5520 : Loss 3916.1259\n",
      "Iteration 5530 : Loss 3914.4690\n",
      "Iteration 5540 : Loss 3912.8161\n",
      "Iteration 5550 : Loss 3911.1672\n",
      "Iteration 5560 : Loss 3909.5224\n",
      "Iteration 5570 : Loss 3907.8815\n",
      "Iteration 5580 : Loss 3906.2447\n",
      "Iteration 5590 : Loss 3904.6117\n",
      "Iteration 5600 : Loss 3902.9828\n",
      "Iteration 5610 : Loss 3901.3577\n",
      "Iteration 5620 : Loss 3899.7366\n",
      "Iteration 5630 : Loss 3898.1195\n",
      "Iteration 5640 : Loss 3896.5062\n",
      "Iteration 5650 : Loss 3894.8968\n",
      "Iteration 5660 : Loss 3893.2913\n",
      "Iteration 5670 : Loss 3891.6896\n",
      "Iteration 5680 : Loss 3890.0918\n",
      "Iteration 5690 : Loss 3888.4979\n",
      "Iteration 5700 : Loss 3886.9078\n",
      "Iteration 5710 : Loss 3885.3215\n",
      "Iteration 5720 : Loss 3883.7390\n",
      "Iteration 5730 : Loss 3882.1603\n",
      "Iteration 5740 : Loss 3880.5854\n",
      "Iteration 5750 : Loss 3879.0143\n",
      "Iteration 5760 : Loss 3877.4469\n",
      "Iteration 5770 : Loss 3875.8833\n",
      "Iteration 5780 : Loss 3874.3234\n",
      "Iteration 5790 : Loss 3872.7673\n",
      "Iteration 5800 : Loss 3871.2148\n",
      "Iteration 5810 : Loss 3869.6661\n",
      "Iteration 5820 : Loss 3868.1210\n",
      "Iteration 5830 : Loss 3866.5797\n",
      "Iteration 5840 : Loss 3865.0420\n",
      "Iteration 5850 : Loss 3863.5079\n",
      "Iteration 5860 : Loss 3861.9775\n",
      "Iteration 5870 : Loss 3860.4508\n",
      "Iteration 5880 : Loss 3858.9276\n",
      "Iteration 5890 : Loss 3857.4081\n",
      "Iteration 5900 : Loss 3855.8922\n",
      "Iteration 5910 : Loss 3854.3799\n",
      "Iteration 5920 : Loss 3852.8711\n",
      "Iteration 5930 : Loss 3851.3659\n",
      "Iteration 5940 : Loss 3849.8643\n",
      "Iteration 5950 : Loss 3848.3662\n",
      "Iteration 5960 : Loss 3846.8717\n",
      "Iteration 5970 : Loss 3845.3807\n",
      "Iteration 5980 : Loss 3843.8932\n",
      "Iteration 5990 : Loss 3842.4092\n",
      "Iteration 6000 : Loss 3840.9287\n",
      "Iteration 6010 : Loss 3839.4517\n",
      "Iteration 6020 : Loss 3837.9781\n",
      "Iteration 6030 : Loss 3836.5081\n",
      "Iteration 6040 : Loss 3835.0414\n",
      "Iteration 6050 : Loss 3833.5782\n",
      "Iteration 6060 : Loss 3832.1185\n",
      "Iteration 6070 : Loss 3830.6621\n",
      "Iteration 6080 : Loss 3829.2092\n",
      "Iteration 6090 : Loss 3827.7596\n",
      "Iteration 6100 : Loss 3826.3135\n",
      "Iteration 6110 : Loss 3824.8707\n",
      "Iteration 6120 : Loss 3823.4313\n",
      "Iteration 6130 : Loss 3821.9953\n",
      "Iteration 6140 : Loss 3820.5625\n",
      "Iteration 6150 : Loss 3819.1332\n",
      "Iteration 6160 : Loss 3817.7071\n",
      "Iteration 6170 : Loss 3816.2844\n",
      "Iteration 6180 : Loss 3814.8650\n",
      "Iteration 6190 : Loss 3813.4489\n",
      "Iteration 6200 : Loss 3812.0360\n",
      "Iteration 6210 : Loss 3810.6265\n",
      "Iteration 6220 : Loss 3809.2202\n",
      "Iteration 6230 : Loss 3807.8171\n",
      "Iteration 6240 : Loss 3806.4173\n",
      "Iteration 6250 : Loss 3805.0208\n",
      "Iteration 6260 : Loss 3803.6275\n",
      "Iteration 6270 : Loss 3802.2374\n",
      "Iteration 6280 : Loss 3800.8504\n",
      "Iteration 6290 : Loss 3799.4667\n",
      "Iteration 6300 : Loss 3798.0862\n",
      "Iteration 6310 : Loss 3796.7089\n",
      "Iteration 6320 : Loss 3795.3347\n",
      "Iteration 6330 : Loss 3793.9637\n",
      "Iteration 6340 : Loss 3792.5958\n",
      "Iteration 6350 : Loss 3791.2311\n",
      "Iteration 6360 : Loss 3789.8695\n",
      "Iteration 6370 : Loss 3788.5110\n",
      "Iteration 6380 : Loss 3787.1557\n",
      "Iteration 6390 : Loss 3785.8034\n",
      "Iteration 6400 : Loss 3784.4542\n",
      "Iteration 6410 : Loss 3783.1081\n",
      "Iteration 6420 : Loss 3781.7651\n",
      "Iteration 6430 : Loss 3780.4252\n",
      "Iteration 6440 : Loss 3779.0883\n",
      "Iteration 6450 : Loss 3777.7544\n",
      "Iteration 6460 : Loss 3776.4236\n",
      "Iteration 6470 : Loss 3775.0959\n",
      "Iteration 6480 : Loss 3773.7711\n",
      "Iteration 6490 : Loss 3772.4493\n",
      "Iteration 6500 : Loss 3771.1306\n",
      "Iteration 6510 : Loss 3769.8148\n",
      "Iteration 6520 : Loss 3768.5021\n",
      "Iteration 6530 : Loss 3767.1923\n",
      "Iteration 6540 : Loss 3765.8854\n",
      "Iteration 6550 : Loss 3764.5815\n",
      "Iteration 6560 : Loss 3763.2806\n",
      "Iteration 6570 : Loss 3761.9826\n",
      "Iteration 6580 : Loss 3760.6875\n",
      "Iteration 6590 : Loss 3759.3954\n",
      "Iteration 6600 : Loss 3758.1061\n",
      "Iteration 6610 : Loss 3756.8198\n",
      "Iteration 6620 : Loss 3755.5364\n",
      "Iteration 6630 : Loss 3754.2558\n",
      "Iteration 6640 : Loss 3752.9782\n",
      "Iteration 6650 : Loss 3751.7033\n",
      "Iteration 6660 : Loss 3750.4314\n",
      "Iteration 6670 : Loss 3749.1623\n",
      "Iteration 6680 : Loss 3747.8961\n",
      "Iteration 6690 : Loss 3746.6326\n",
      "Iteration 6700 : Loss 3745.3720\n",
      "Iteration 6710 : Loss 3744.1143\n",
      "Iteration 6720 : Loss 3742.8593\n",
      "Iteration 6730 : Loss 3741.6071\n",
      "Iteration 6740 : Loss 3740.3578\n",
      "Iteration 6750 : Loss 3739.1112\n",
      "Iteration 6760 : Loss 3737.8674\n",
      "Iteration 6770 : Loss 3736.6263\n",
      "Iteration 6780 : Loss 3735.3881\n",
      "Iteration 6790 : Loss 3734.1525\n",
      "Iteration 6800 : Loss 3732.9197\n",
      "Iteration 6810 : Loss 3731.6897\n",
      "Iteration 6820 : Loss 3730.4624\n",
      "Iteration 6830 : Loss 3729.2377\n",
      "Iteration 6840 : Loss 3728.0158\n",
      "Iteration 6850 : Loss 3726.7967\n",
      "Iteration 6860 : Loss 3725.5802\n",
      "Iteration 6870 : Loss 3724.3663\n",
      "Iteration 6880 : Loss 3723.1552\n",
      "Iteration 6890 : Loss 3721.9467\n",
      "Iteration 6900 : Loss 3720.7409\n",
      "Iteration 6910 : Loss 3719.5378\n",
      "Iteration 6920 : Loss 3718.3373\n",
      "Iteration 6930 : Loss 3717.1394\n",
      "Iteration 6940 : Loss 3715.9442\n",
      "Iteration 6950 : Loss 3714.7516\n",
      "Iteration 6960 : Loss 3713.5616\n",
      "Iteration 6970 : Loss 3712.3742\n",
      "Iteration 6980 : Loss 3711.1894\n",
      "Iteration 6990 : Loss 3710.0073\n",
      "Iteration 7000 : Loss 3708.8277\n",
      "Iteration 7010 : Loss 3707.6506\n",
      "Iteration 7020 : Loss 3706.4762\n",
      "Iteration 7030 : Loss 3705.3043\n",
      "Iteration 7040 : Loss 3704.1349\n",
      "Iteration 7050 : Loss 3702.9682\n",
      "Iteration 7060 : Loss 3701.8039\n",
      "Iteration 7070 : Loss 3700.6422\n",
      "Iteration 7080 : Loss 3699.4830\n",
      "Iteration 7090 : Loss 3698.3263\n",
      "Iteration 7100 : Loss 3697.1722\n",
      "Iteration 7110 : Loss 3696.0205\n",
      "Iteration 7120 : Loss 3694.8713\n",
      "Iteration 7130 : Loss 3693.7246\n",
      "Iteration 7140 : Loss 3692.5804\n",
      "Iteration 7150 : Loss 3691.4387\n",
      "Iteration 7160 : Loss 3690.2995\n",
      "Iteration 7170 : Loss 3689.1627\n",
      "Iteration 7180 : Loss 3688.0283\n",
      "Iteration 7190 : Loss 3686.8964\n",
      "Iteration 7200 : Loss 3685.7670\n",
      "Iteration 7210 : Loss 3684.6399\n",
      "Iteration 7220 : Loss 3683.5153\n",
      "Iteration 7230 : Loss 3682.3931\n",
      "Iteration 7240 : Loss 3681.2733\n",
      "Iteration 7250 : Loss 3680.1560\n",
      "Iteration 7260 : Loss 3679.0410\n",
      "Iteration 7270 : Loss 3677.9284\n",
      "Iteration 7280 : Loss 3676.8182\n",
      "Iteration 7290 : Loss 3675.7103\n",
      "Iteration 7300 : Loss 3674.6048\n",
      "Iteration 7310 : Loss 3673.5017\n",
      "Iteration 7320 : Loss 3672.4010\n",
      "Iteration 7330 : Loss 3671.3026\n",
      "Iteration 7340 : Loss 3670.2065\n",
      "Iteration 7350 : Loss 3669.1128\n",
      "Iteration 7360 : Loss 3668.0213\n",
      "Iteration 7370 : Loss 3666.9322\n",
      "Iteration 7380 : Loss 3665.8455\n",
      "Iteration 7390 : Loss 3664.7610\n",
      "Iteration 7400 : Loss 3663.6788\n",
      "Iteration 7410 : Loss 3662.5989\n",
      "Iteration 7420 : Loss 3661.5214\n",
      "Iteration 7430 : Loss 3660.4460\n",
      "Iteration 7440 : Loss 3659.3730\n",
      "Iteration 7450 : Loss 3658.3022\n",
      "Iteration 7460 : Loss 3657.2337\n",
      "Iteration 7470 : Loss 3656.1675\n",
      "Iteration 7480 : Loss 3655.1034\n",
      "Iteration 7490 : Loss 3654.0417\n",
      "Iteration 7500 : Loss 3652.9821\n",
      "Iteration 7510 : Loss 3651.9248\n",
      "Iteration 7520 : Loss 3650.8697\n",
      "Iteration 7530 : Loss 3649.8169\n",
      "Iteration 7540 : Loss 3648.7662\n",
      "Iteration 7550 : Loss 3647.7177\n",
      "Iteration 7560 : Loss 3646.6715\n",
      "Iteration 7570 : Loss 3645.6274\n",
      "Iteration 7580 : Loss 3644.5855\n",
      "Iteration 7590 : Loss 3643.5458\n",
      "Iteration 7600 : Loss 3642.5082\n",
      "Iteration 7610 : Loss 3641.4729\n",
      "Iteration 7620 : Loss 3640.4396\n",
      "Iteration 7630 : Loss 3639.4086\n",
      "Iteration 7640 : Loss 3638.3796\n",
      "Iteration 7650 : Loss 3637.3529\n",
      "Iteration 7660 : Loss 3636.3282\n",
      "Iteration 7670 : Loss 3635.3057\n",
      "Iteration 7680 : Loss 3634.2853\n",
      "Iteration 7690 : Loss 3633.2670\n",
      "Iteration 7700 : Loss 3632.2508\n",
      "Iteration 7710 : Loss 3631.2367\n",
      "Iteration 7720 : Loss 3630.2248\n",
      "Iteration 7730 : Loss 3629.2149\n",
      "Iteration 7740 : Loss 3628.2071\n",
      "Iteration 7750 : Loss 3627.2014\n",
      "Iteration 7760 : Loss 3626.1977\n",
      "Iteration 7770 : Loss 3625.1961\n",
      "Iteration 7780 : Loss 3624.1966\n",
      "Iteration 7790 : Loss 3623.1992\n",
      "Iteration 7800 : Loss 3622.2038\n",
      "Iteration 7810 : Loss 3621.2104\n",
      "Iteration 7820 : Loss 3620.2191\n",
      "Iteration 7830 : Loss 3619.2298\n",
      "Iteration 7840 : Loss 3618.2425\n",
      "Iteration 7850 : Loss 3617.2572\n",
      "Iteration 7860 : Loss 3616.2740\n",
      "Iteration 7870 : Loss 3615.2928\n",
      "Iteration 7880 : Loss 3614.3136\n",
      "Iteration 7890 : Loss 3613.3364\n",
      "Iteration 7900 : Loss 3612.3611\n",
      "Iteration 7910 : Loss 3611.3879\n",
      "Iteration 7920 : Loss 3610.4166\n",
      "Iteration 7930 : Loss 3609.4474\n",
      "Iteration 7940 : Loss 3608.4800\n",
      "Iteration 7950 : Loss 3607.5147\n",
      "Iteration 7960 : Loss 3606.5513\n",
      "Iteration 7970 : Loss 3605.5899\n",
      "Iteration 7980 : Loss 3604.6304\n",
      "Iteration 7990 : Loss 3603.6728\n",
      "Iteration 8000 : Loss 3602.7172\n",
      "Iteration 8010 : Loss 3601.7636\n",
      "Iteration 8020 : Loss 3600.8118\n",
      "Iteration 8030 : Loss 3599.8620\n",
      "Iteration 8040 : Loss 3598.9141\n",
      "Iteration 8050 : Loss 3597.9681\n",
      "Iteration 8060 : Loss 3597.0240\n",
      "Iteration 8070 : Loss 3596.0818\n",
      "Iteration 8080 : Loss 3595.1415\n",
      "Iteration 8090 : Loss 3594.2031\n",
      "Iteration 8100 : Loss 3593.2665\n",
      "Iteration 8110 : Loss 3592.3319\n",
      "Iteration 8120 : Loss 3591.3991\n",
      "Iteration 8130 : Loss 3590.4682\n",
      "Iteration 8140 : Loss 3589.5391\n",
      "Iteration 8150 : Loss 3588.6119\n",
      "Iteration 8160 : Loss 3587.6866\n",
      "Iteration 8170 : Loss 3586.7631\n",
      "Iteration 8180 : Loss 3585.8415\n",
      "Iteration 8190 : Loss 3584.9216\n",
      "Iteration 8200 : Loss 3584.0037\n",
      "Iteration 8210 : Loss 3583.0875\n",
      "Iteration 8220 : Loss 3582.1732\n",
      "Iteration 8230 : Loss 3581.2606\n",
      "Iteration 8240 : Loss 3580.3499\n",
      "Iteration 8250 : Loss 3579.4410\n",
      "Iteration 8260 : Loss 3578.5339\n",
      "Iteration 8270 : Loss 3577.6286\n",
      "Iteration 8280 : Loss 3576.7251\n",
      "Iteration 8290 : Loss 3575.8234\n",
      "Iteration 8300 : Loss 3574.9234\n",
      "Iteration 8310 : Loss 3574.0253\n",
      "Iteration 8320 : Loss 3573.1289\n",
      "Iteration 8330 : Loss 3572.2342\n",
      "Iteration 8340 : Loss 3571.3414\n",
      "Iteration 8350 : Loss 3570.4502\n",
      "Iteration 8360 : Loss 3569.5609\n",
      "Iteration 8370 : Loss 3568.6733\n",
      "Iteration 8380 : Loss 3567.7874\n",
      "Iteration 8390 : Loss 3566.9033\n",
      "Iteration 8400 : Loss 3566.0209\n",
      "Iteration 8410 : Loss 3565.1402\n",
      "Iteration 8420 : Loss 3564.2612\n",
      "Iteration 8430 : Loss 3563.3840\n",
      "Iteration 8440 : Loss 3562.5085\n",
      "Iteration 8450 : Loss 3561.6347\n",
      "Iteration 8460 : Loss 3560.7626\n",
      "Iteration 8470 : Loss 3559.8922\n",
      "Iteration 8480 : Loss 3559.0234\n",
      "Iteration 8490 : Loss 3558.1564\n",
      "Iteration 8500 : Loss 3557.2911\n",
      "Iteration 8510 : Loss 3556.4274\n",
      "Iteration 8520 : Loss 3555.5654\n",
      "Iteration 8530 : Loss 3554.7051\n",
      "Iteration 8540 : Loss 3553.8465\n",
      "Iteration 8550 : Loss 3552.9895\n",
      "Iteration 8560 : Loss 3552.1342\n",
      "Iteration 8570 : Loss 3551.2805\n",
      "Iteration 8580 : Loss 3550.4285\n",
      "Iteration 8590 : Loss 3549.5781\n",
      "Iteration 8600 : Loss 3548.7294\n",
      "Iteration 8610 : Loss 3547.8823\n",
      "Iteration 8620 : Loss 3547.0368\n",
      "Iteration 8630 : Loss 3546.1930\n",
      "Iteration 8640 : Loss 3545.3508\n",
      "Iteration 8650 : Loss 3544.5102\n",
      "Iteration 8660 : Loss 3543.6712\n",
      "Iteration 8670 : Loss 3542.8338\n",
      "Iteration 8680 : Loss 3541.9980\n",
      "Iteration 8690 : Loss 3541.1639\n",
      "Iteration 8700 : Loss 3540.3313\n",
      "Iteration 8710 : Loss 3539.5003\n",
      "Iteration 8720 : Loss 3538.6709\n",
      "Iteration 8730 : Loss 3537.8431\n",
      "Iteration 8740 : Loss 3537.0168\n",
      "Iteration 8750 : Loss 3536.1922\n",
      "Iteration 8760 : Loss 3535.3691\n",
      "Iteration 8770 : Loss 3534.5475\n",
      "Iteration 8780 : Loss 3533.7276\n",
      "Iteration 8790 : Loss 3532.9092\n",
      "Iteration 8800 : Loss 3532.0923\n",
      "Iteration 8810 : Loss 3531.2770\n",
      "Iteration 8820 : Loss 3530.4632\n",
      "Iteration 8830 : Loss 3529.6510\n",
      "Iteration 8840 : Loss 3528.8403\n",
      "Iteration 8850 : Loss 3528.0312\n",
      "Iteration 8860 : Loss 3527.2236\n",
      "Iteration 8870 : Loss 3526.4175\n",
      "Iteration 8880 : Loss 3525.6129\n",
      "Iteration 8890 : Loss 3524.8098\n",
      "Iteration 8900 : Loss 3524.0083\n",
      "Iteration 8910 : Loss 3523.2082\n",
      "Iteration 8920 : Loss 3522.4097\n",
      "Iteration 8930 : Loss 3521.6126\n",
      "Iteration 8940 : Loss 3520.8171\n",
      "Iteration 8950 : Loss 3520.0230\n",
      "Iteration 8960 : Loss 3519.2305\n",
      "Iteration 8970 : Loss 3518.4394\n",
      "Iteration 8980 : Loss 3517.6498\n",
      "Iteration 8990 : Loss 3516.8616\n",
      "Iteration 9000 : Loss 3516.0750\n",
      "Iteration 9010 : Loss 3515.2898\n",
      "Iteration 9020 : Loss 3514.5061\n",
      "Iteration 9030 : Loss 3513.7238\n",
      "Iteration 9040 : Loss 3512.9430\n",
      "Iteration 9050 : Loss 3512.1637\n",
      "Iteration 9060 : Loss 3511.3858\n",
      "Iteration 9070 : Loss 3510.6093\n",
      "Iteration 9080 : Loss 3509.8343\n",
      "Iteration 9090 : Loss 3509.0607\n",
      "Iteration 9100 : Loss 3508.2885\n",
      "Iteration 9110 : Loss 3507.5178\n",
      "Iteration 9120 : Loss 3506.7485\n",
      "Iteration 9130 : Loss 3505.9806\n",
      "Iteration 9140 : Loss 3505.2142\n",
      "Iteration 9150 : Loss 3504.4491\n",
      "Iteration 9160 : Loss 3503.6855\n",
      "Iteration 9170 : Loss 3502.9233\n",
      "Iteration 9180 : Loss 3502.1625\n",
      "Iteration 9190 : Loss 3501.4030\n",
      "Iteration 9200 : Loss 3500.6450\n",
      "Iteration 9210 : Loss 3499.8884\n",
      "Iteration 9220 : Loss 3499.1331\n",
      "Iteration 9230 : Loss 3498.3793\n",
      "Iteration 9240 : Loss 3497.6268\n",
      "Iteration 9250 : Loss 3496.8757\n",
      "Iteration 9260 : Loss 3496.1260\n",
      "Iteration 9270 : Loss 3495.3776\n",
      "Iteration 9280 : Loss 3494.6306\n",
      "Iteration 9290 : Loss 3493.8850\n",
      "Iteration 9300 : Loss 3493.1407\n",
      "Iteration 9310 : Loss 3492.3978\n",
      "Iteration 9320 : Loss 3491.6562\n",
      "Iteration 9330 : Loss 3490.9160\n",
      "Iteration 9340 : Loss 3490.1771\n",
      "Iteration 9350 : Loss 3489.4396\n",
      "Iteration 9360 : Loss 3488.7034\n",
      "Iteration 9370 : Loss 3487.9686\n",
      "Iteration 9380 : Loss 3487.2350\n",
      "Iteration 9390 : Loss 3486.5028\n",
      "Iteration 9400 : Loss 3485.7720\n",
      "Iteration 9410 : Loss 3485.0424\n",
      "Iteration 9420 : Loss 3484.3142\n",
      "Iteration 9430 : Loss 3483.5873\n",
      "Iteration 9440 : Loss 3482.8617\n",
      "Iteration 9450 : Loss 3482.1374\n",
      "Iteration 9460 : Loss 3481.4144\n",
      "Iteration 9470 : Loss 3480.6927\n",
      "Iteration 9480 : Loss 3479.9723\n",
      "Iteration 9490 : Loss 3479.2531\n",
      "Iteration 9500 : Loss 3478.5353\n",
      "Iteration 9510 : Loss 3477.8188\n",
      "Iteration 9520 : Loss 3477.1035\n",
      "Iteration 9530 : Loss 3476.3896\n",
      "Iteration 9540 : Loss 3475.6769\n",
      "Iteration 9550 : Loss 3474.9655\n",
      "Iteration 9560 : Loss 3474.2553\n",
      "Iteration 9570 : Loss 3473.5464\n",
      "Iteration 9580 : Loss 3472.8388\n",
      "Iteration 9590 : Loss 3472.1325\n",
      "Iteration 9600 : Loss 3471.4274\n",
      "Iteration 9610 : Loss 3470.7235\n",
      "Iteration 9620 : Loss 3470.0209\n",
      "Iteration 9630 : Loss 3469.3196\n",
      "Iteration 9640 : Loss 3468.6195\n",
      "Iteration 9650 : Loss 3467.9206\n",
      "Iteration 9660 : Loss 3467.2230\n",
      "Iteration 9670 : Loss 3466.5266\n",
      "Iteration 9680 : Loss 3465.8315\n",
      "Iteration 9690 : Loss 3465.1375\n",
      "Iteration 9700 : Loss 3464.4448\n",
      "Iteration 9710 : Loss 3463.7533\n",
      "Iteration 9720 : Loss 3463.0631\n",
      "Iteration 9730 : Loss 3462.3740\n",
      "Iteration 9740 : Loss 3461.6862\n",
      "Iteration 9750 : Loss 3460.9996\n",
      "Iteration 9760 : Loss 3460.3142\n",
      "Iteration 9770 : Loss 3459.6299\n",
      "Iteration 9780 : Loss 3458.9469\n",
      "Iteration 9790 : Loss 3458.2651\n",
      "Iteration 9800 : Loss 3457.5845\n",
      "Iteration 9810 : Loss 3456.9051\n",
      "Iteration 9820 : Loss 3456.2268\n",
      "Iteration 9830 : Loss 3455.5498\n",
      "Iteration 9840 : Loss 3454.8739\n",
      "Iteration 9850 : Loss 3454.1992\n",
      "Iteration 9860 : Loss 3453.5257\n",
      "Iteration 9870 : Loss 3452.8533\n",
      "Iteration 9880 : Loss 3452.1821\n",
      "Iteration 9890 : Loss 3451.5121\n",
      "Iteration 9900 : Loss 3450.8433\n",
      "Iteration 9910 : Loss 3450.1756\n",
      "Iteration 9920 : Loss 3449.5091\n",
      "Iteration 9930 : Loss 3448.8437\n",
      "Iteration 9940 : Loss 3448.1795\n",
      "Iteration 9950 : Loss 3447.5164\n",
      "Iteration 9960 : Loss 3446.8545\n",
      "Iteration 9970 : Loss 3446.1937\n",
      "Iteration 9980 : Loss 3445.5341\n",
      "Iteration 9990 : Loss 3444.8756\n",
      "Iteration 10000 : Loss 3444.2182\n",
      "Iteration 10010 : Loss 3443.5620\n",
      "Iteration 10020 : Loss 3442.9069\n",
      "Iteration 10030 : Loss 3442.2529\n",
      "Iteration 10040 : Loss 3441.6001\n",
      "Iteration 10050 : Loss 3440.9484\n",
      "Iteration 10060 : Loss 3440.2977\n",
      "Iteration 10070 : Loss 3439.6482\n",
      "Iteration 10080 : Loss 3438.9999\n",
      "Iteration 10090 : Loss 3438.3526\n",
      "Iteration 10100 : Loss 3437.7064\n",
      "Iteration 10110 : Loss 3437.0614\n",
      "Iteration 10120 : Loss 3436.4174\n",
      "Iteration 10130 : Loss 3435.7746\n",
      "Iteration 10140 : Loss 3435.1328\n",
      "Iteration 10150 : Loss 3434.4921\n",
      "Iteration 10160 : Loss 3433.8525\n",
      "Iteration 10170 : Loss 3433.2140\n",
      "Iteration 10180 : Loss 3432.5766\n",
      "Iteration 10190 : Loss 3431.9403\n",
      "Iteration 10200 : Loss 3431.3051\n",
      "Iteration 10210 : Loss 3430.6709\n",
      "Iteration 10220 : Loss 3430.0378\n",
      "Iteration 10230 : Loss 3429.4058\n",
      "Iteration 10240 : Loss 3428.7748\n",
      "Iteration 10250 : Loss 3428.1450\n",
      "Iteration 10260 : Loss 3427.5161\n",
      "Iteration 10270 : Loss 3426.8884\n",
      "Iteration 10280 : Loss 3426.2617\n",
      "Iteration 10290 : Loss 3425.6360\n",
      "Iteration 10300 : Loss 3425.0114\n",
      "Iteration 10310 : Loss 3424.3879\n",
      "Iteration 10320 : Loss 3423.7654\n",
      "Iteration 10330 : Loss 3423.1440\n",
      "Iteration 10340 : Loss 3422.5236\n",
      "Iteration 10350 : Loss 3421.9042\n",
      "Iteration 10360 : Loss 3421.2859\n",
      "Iteration 10370 : Loss 3420.6686\n",
      "Iteration 10380 : Loss 3420.0523\n",
      "Iteration 10390 : Loss 3419.4371\n",
      "Iteration 10400 : Loss 3418.8229\n",
      "Iteration 10410 : Loss 3418.2097\n",
      "Iteration 10420 : Loss 3417.5976\n",
      "Iteration 10430 : Loss 3416.9865\n",
      "Iteration 10440 : Loss 3416.3764\n",
      "Iteration 10450 : Loss 3415.7673\n",
      "Iteration 10460 : Loss 3415.1592\n",
      "Iteration 10470 : Loss 3414.5521\n",
      "Iteration 10480 : Loss 3413.9460\n",
      "Iteration 10490 : Loss 3413.3410\n",
      "Iteration 10500 : Loss 3412.7369\n",
      "Iteration 10510 : Loss 3412.1339\n",
      "Iteration 10520 : Loss 3411.5318\n",
      "Iteration 10530 : Loss 3410.9308\n",
      "Iteration 10540 : Loss 3410.3307\n",
      "Iteration 10550 : Loss 3409.7316\n",
      "Iteration 10560 : Loss 3409.1335\n",
      "Iteration 10570 : Loss 3408.5364\n",
      "Iteration 10580 : Loss 3407.9403\n",
      "Iteration 10590 : Loss 3407.3452\n",
      "Iteration 10600 : Loss 3406.7510\n",
      "Iteration 10610 : Loss 3406.1578\n",
      "Iteration 10620 : Loss 3405.5656\n",
      "Iteration 10630 : Loss 3404.9744\n",
      "Iteration 10640 : Loss 3404.3841\n",
      "Iteration 10650 : Loss 3403.7948\n",
      "Iteration 10660 : Loss 3403.2065\n",
      "Iteration 10670 : Loss 3402.6191\n",
      "Iteration 10680 : Loss 3402.0327\n",
      "Iteration 10690 : Loss 3401.4473\n",
      "Iteration 10700 : Loss 3400.8628\n",
      "Iteration 10710 : Loss 3400.2792\n",
      "Iteration 10720 : Loss 3399.6966\n",
      "Iteration 10730 : Loss 3399.1150\n",
      "Iteration 10740 : Loss 3398.5343\n",
      "Iteration 10750 : Loss 3397.9545\n",
      "Iteration 10760 : Loss 3397.3757\n",
      "Iteration 10770 : Loss 3396.7978\n",
      "Iteration 10780 : Loss 3396.2209\n",
      "Iteration 10790 : Loss 3395.6449\n",
      "Iteration 10800 : Loss 3395.0698\n",
      "Iteration 10810 : Loss 3394.4957\n",
      "Iteration 10820 : Loss 3393.9225\n",
      "Iteration 10830 : Loss 3393.3502\n",
      "Iteration 10840 : Loss 3392.7789\n",
      "Iteration 10850 : Loss 3392.2084\n",
      "Iteration 10860 : Loss 3391.6389\n",
      "Iteration 10870 : Loss 3391.0703\n",
      "Iteration 10880 : Loss 3390.5026\n",
      "Iteration 10890 : Loss 3389.9359\n",
      "Iteration 10900 : Loss 3389.3700\n",
      "Iteration 10910 : Loss 3388.8051\n",
      "Iteration 10920 : Loss 3388.2410\n",
      "Iteration 10930 : Loss 3387.6779\n",
      "Iteration 10940 : Loss 3387.1157\n",
      "Iteration 10950 : Loss 3386.5543\n",
      "Iteration 10960 : Loss 3385.9939\n",
      "Iteration 10970 : Loss 3385.4343\n",
      "Iteration 10980 : Loss 3384.8757\n",
      "Iteration 10990 : Loss 3384.3179\n",
      "Iteration 11000 : Loss 3383.7611\n",
      "Iteration 11010 : Loss 3383.2051\n",
      "Iteration 11020 : Loss 3382.6500\n",
      "Iteration 11030 : Loss 3382.0958\n",
      "Iteration 11040 : Loss 3381.5425\n",
      "Iteration 11050 : Loss 3380.9900\n",
      "Iteration 11060 : Loss 3380.4385\n",
      "Iteration 11070 : Loss 3379.8878\n",
      "Iteration 11080 : Loss 3379.3380\n",
      "Iteration 11090 : Loss 3378.7890\n",
      "Iteration 11100 : Loss 3378.2409\n",
      "Iteration 11110 : Loss 3377.6937\n",
      "Iteration 11120 : Loss 3377.1474\n",
      "Iteration 11130 : Loss 3376.6019\n",
      "Iteration 11140 : Loss 3376.0573\n",
      "Iteration 11150 : Loss 3375.5135\n",
      "Iteration 11160 : Loss 3374.9706\n",
      "Iteration 11170 : Loss 3374.4285\n",
      "Iteration 11180 : Loss 3373.8873\n",
      "Iteration 11190 : Loss 3373.3470\n",
      "Iteration 11200 : Loss 3372.8075\n",
      "Iteration 11210 : Loss 3372.2688\n",
      "Iteration 11220 : Loss 3371.7310\n",
      "Iteration 11230 : Loss 3371.1941\n",
      "Iteration 11240 : Loss 3370.6580\n",
      "Iteration 11250 : Loss 3370.1227\n",
      "Iteration 11260 : Loss 3369.5882\n",
      "Iteration 11270 : Loss 3369.0546\n",
      "Iteration 11280 : Loss 3368.5218\n",
      "Iteration 11290 : Loss 3367.9899\n",
      "Iteration 11300 : Loss 3367.4588\n",
      "Iteration 11310 : Loss 3366.9285\n",
      "Iteration 11320 : Loss 3366.3990\n",
      "Iteration 11330 : Loss 3365.8704\n",
      "Iteration 11340 : Loss 3365.3426\n",
      "Iteration 11350 : Loss 3364.8156\n",
      "Iteration 11360 : Loss 3364.2894\n",
      "Iteration 11370 : Loss 3363.7641\n",
      "Iteration 11380 : Loss 3363.2395\n",
      "Iteration 11390 : Loss 3362.7158\n",
      "Iteration 11400 : Loss 3362.1929\n",
      "Iteration 11410 : Loss 3361.6708\n",
      "Iteration 11420 : Loss 3361.1494\n",
      "Iteration 11430 : Loss 3360.6289\n",
      "Iteration 11440 : Loss 3360.1093\n",
      "Iteration 11450 : Loss 3359.5904\n",
      "Iteration 11460 : Loss 3359.0723\n",
      "Iteration 11470 : Loss 3358.5550\n",
      "Iteration 11480 : Loss 3358.0385\n",
      "Iteration 11490 : Loss 3357.5228\n",
      "Iteration 11500 : Loss 3357.0078\n",
      "Iteration 11510 : Loss 3356.4937\n",
      "Iteration 11520 : Loss 3355.9804\n",
      "Iteration 11530 : Loss 3355.4678\n",
      "Iteration 11540 : Loss 3354.9561\n",
      "Iteration 11550 : Loss 3354.4451\n",
      "Iteration 11560 : Loss 3353.9349\n",
      "Iteration 11570 : Loss 3353.4255\n",
      "Iteration 11580 : Loss 3352.9169\n",
      "Iteration 11590 : Loss 3352.4090\n",
      "Iteration 11600 : Loss 3351.9019\n",
      "Iteration 11610 : Loss 3351.3956\n",
      "Iteration 11620 : Loss 3350.8901\n",
      "Iteration 11630 : Loss 3350.3853\n",
      "Iteration 11640 : Loss 3349.8813\n",
      "Iteration 11650 : Loss 3349.3781\n",
      "Iteration 11660 : Loss 3348.8756\n",
      "Iteration 11670 : Loss 3348.3739\n",
      "Iteration 11680 : Loss 3347.8729\n",
      "Iteration 11690 : Loss 3347.3727\n",
      "Iteration 11700 : Loss 3346.8733\n",
      "Iteration 11710 : Loss 3346.3746\n",
      "Iteration 11720 : Loss 3345.8767\n",
      "Iteration 11730 : Loss 3345.3795\n",
      "Iteration 11740 : Loss 3344.8831\n",
      "Iteration 11750 : Loss 3344.3874\n",
      "Iteration 11760 : Loss 3343.8925\n",
      "Iteration 11770 : Loss 3343.3984\n",
      "Iteration 11780 : Loss 3342.9049\n",
      "Iteration 11790 : Loss 3342.4122\n",
      "Iteration 11800 : Loss 3341.9203\n",
      "Iteration 11810 : Loss 3341.4291\n",
      "Iteration 11820 : Loss 3340.9386\n",
      "Iteration 11830 : Loss 3340.4489\n",
      "Iteration 11840 : Loss 3339.9599\n",
      "Iteration 11850 : Loss 3339.4716\n",
      "Iteration 11860 : Loss 3338.9841\n",
      "Iteration 11870 : Loss 3338.4973\n",
      "Iteration 11880 : Loss 3338.0112\n",
      "Iteration 11890 : Loss 3337.5258\n",
      "Iteration 11900 : Loss 3337.0412\n",
      "Iteration 11910 : Loss 3336.5573\n",
      "Iteration 11920 : Loss 3336.0741\n",
      "Iteration 11930 : Loss 3335.5916\n",
      "Iteration 11940 : Loss 3335.1099\n",
      "Iteration 11950 : Loss 3334.6289\n",
      "Iteration 11960 : Loss 3334.1486\n",
      "Iteration 11970 : Loss 3333.6690\n",
      "Iteration 11980 : Loss 3333.1901\n",
      "Iteration 11990 : Loss 3332.7119\n",
      "Iteration 12000 : Loss 3332.2344\n",
      "Iteration 12010 : Loss 3331.7577\n",
      "Iteration 12020 : Loss 3331.2816\n",
      "Iteration 12030 : Loss 3330.8063\n",
      "Iteration 12040 : Loss 3330.3316\n",
      "Iteration 12050 : Loss 3329.8577\n",
      "Iteration 12060 : Loss 3329.3844\n",
      "Iteration 12070 : Loss 3328.9119\n",
      "Iteration 12080 : Loss 3328.4400\n",
      "Iteration 12090 : Loss 3327.9689\n",
      "Iteration 12100 : Loss 3327.4984\n",
      "Iteration 12110 : Loss 3327.0286\n",
      "Iteration 12120 : Loss 3326.5595\n",
      "Iteration 12130 : Loss 3326.0911\n",
      "Iteration 12140 : Loss 3325.6234\n",
      "Iteration 12150 : Loss 3325.1564\n",
      "Iteration 12160 : Loss 3324.6901\n",
      "Iteration 12170 : Loss 3324.2244\n",
      "Iteration 12180 : Loss 3323.7595\n",
      "Iteration 12190 : Loss 3323.2952\n",
      "Iteration 12200 : Loss 3322.8316\n",
      "Iteration 12210 : Loss 3322.3686\n",
      "Iteration 12220 : Loss 3321.9064\n",
      "Iteration 12230 : Loss 3321.4448\n",
      "Iteration 12240 : Loss 3320.9839\n",
      "Iteration 12250 : Loss 3320.5237\n",
      "Iteration 12260 : Loss 3320.0641\n",
      "Iteration 12270 : Loss 3319.6052\n",
      "Iteration 12280 : Loss 3319.1470\n",
      "Iteration 12290 : Loss 3318.6894\n",
      "Iteration 12300 : Loss 3318.2325\n",
      "Iteration 12310 : Loss 3317.7763\n",
      "Iteration 12320 : Loss 3317.3207\n",
      "Iteration 12330 : Loss 3316.8658\n",
      "Iteration 12340 : Loss 3316.4115\n",
      "Iteration 12350 : Loss 3315.9579\n",
      "Iteration 12360 : Loss 3315.5049\n",
      "Iteration 12370 : Loss 3315.0527\n",
      "Iteration 12380 : Loss 3314.6010\n",
      "Iteration 12390 : Loss 3314.1500\n",
      "Iteration 12400 : Loss 3313.6997\n",
      "Iteration 12410 : Loss 3313.2500\n",
      "Iteration 12420 : Loss 3312.8010\n",
      "Iteration 12430 : Loss 3312.3526\n",
      "Iteration 12440 : Loss 3311.9048\n",
      "Iteration 12450 : Loss 3311.4577\n",
      "Iteration 12460 : Loss 3311.0113\n",
      "Iteration 12470 : Loss 3310.5654\n",
      "Iteration 12480 : Loss 3310.1203\n",
      "Iteration 12490 : Loss 3309.6757\n",
      "Iteration 12500 : Loss 3309.2318\n",
      "Iteration 12510 : Loss 3308.7885\n",
      "Iteration 12520 : Loss 3308.3459\n",
      "Iteration 12530 : Loss 3307.9039\n",
      "Iteration 12540 : Loss 3307.4625\n",
      "Iteration 12550 : Loss 3307.0218\n",
      "Iteration 12560 : Loss 3306.5816\n",
      "Iteration 12570 : Loss 3306.1422\n",
      "Iteration 12580 : Loss 3305.7033\n",
      "Iteration 12590 : Loss 3305.2651\n",
      "Iteration 12600 : Loss 3304.8274\n",
      "Iteration 12610 : Loss 3304.3904\n",
      "Iteration 12620 : Loss 3303.9541\n",
      "Iteration 12630 : Loss 3303.5183\n",
      "Iteration 12640 : Loss 3303.0832\n",
      "Iteration 12650 : Loss 3302.6487\n",
      "Iteration 12660 : Loss 3302.2148\n",
      "Iteration 12670 : Loss 3301.7815\n",
      "Iteration 12680 : Loss 3301.3488\n",
      "Iteration 12690 : Loss 3300.9167\n",
      "Iteration 12700 : Loss 3300.4853\n",
      "Iteration 12710 : Loss 3300.0545\n",
      "Iteration 12720 : Loss 3299.6242\n",
      "Iteration 12730 : Loss 3299.1946\n",
      "Iteration 12740 : Loss 3298.7656\n",
      "Iteration 12750 : Loss 3298.3371\n",
      "Iteration 12760 : Loss 3297.9093\n",
      "Iteration 12770 : Loss 3297.4821\n",
      "Iteration 12780 : Loss 3297.0555\n",
      "Iteration 12790 : Loss 3296.6295\n",
      "Iteration 12800 : Loss 3296.2041\n",
      "Iteration 12810 : Loss 3295.7793\n",
      "Iteration 12820 : Loss 3295.3550\n",
      "Iteration 12830 : Loss 3294.9314\n",
      "Iteration 12840 : Loss 3294.5084\n",
      "Iteration 12850 : Loss 3294.0859\n",
      "Iteration 12860 : Loss 3293.6641\n",
      "Iteration 12870 : Loss 3293.2428\n",
      "Iteration 12880 : Loss 3292.8221\n",
      "Iteration 12890 : Loss 3292.4021\n",
      "Iteration 12900 : Loss 3291.9826\n",
      "Iteration 12910 : Loss 3291.5636\n",
      "Iteration 12920 : Loss 3291.1453\n",
      "Iteration 12930 : Loss 3290.7276\n",
      "Iteration 12940 : Loss 3290.3104\n",
      "Iteration 12950 : Loss 3289.8938\n",
      "Iteration 12960 : Loss 3289.4778\n",
      "Iteration 12970 : Loss 3289.0623\n",
      "Iteration 12980 : Loss 3288.6475\n",
      "Iteration 12990 : Loss 3288.2332\n",
      "Iteration 13000 : Loss 3287.8195\n",
      "Iteration 13010 : Loss 3287.4064\n",
      "Iteration 13020 : Loss 3286.9938\n",
      "Iteration 13030 : Loss 3286.5818\n",
      "Iteration 13040 : Loss 3286.1704\n",
      "Iteration 13050 : Loss 3285.7595\n",
      "Iteration 13060 : Loss 3285.3492\n",
      "Iteration 13070 : Loss 3284.9395\n",
      "Iteration 13080 : Loss 3284.5304\n",
      "Iteration 13090 : Loss 3284.1218\n",
      "Iteration 13100 : Loss 3283.7137\n",
      "Iteration 13110 : Loss 3283.3063\n",
      "Iteration 13120 : Loss 3282.8994\n",
      "Iteration 13130 : Loss 3282.4930\n",
      "Iteration 13140 : Loss 3282.0872\n",
      "Iteration 13150 : Loss 3281.6820\n",
      "Iteration 13160 : Loss 3281.2773\n",
      "Iteration 13170 : Loss 3280.8732\n",
      "Iteration 13180 : Loss 3280.4696\n",
      "Iteration 13190 : Loss 3280.0666\n",
      "Iteration 13200 : Loss 3279.6641\n",
      "Iteration 13210 : Loss 3279.2622\n",
      "Iteration 13220 : Loss 3278.8608\n",
      "Iteration 13230 : Loss 3278.4600\n",
      "Iteration 13240 : Loss 3278.0597\n",
      "Iteration 13250 : Loss 3277.6600\n",
      "Iteration 13260 : Loss 3277.2608\n",
      "Iteration 13270 : Loss 3276.8622\n",
      "Iteration 13280 : Loss 3276.4641\n",
      "Iteration 13290 : Loss 3276.0666\n",
      "Iteration 13300 : Loss 3275.6695\n",
      "Iteration 13310 : Loss 3275.2731\n",
      "Iteration 13320 : Loss 3274.8771\n",
      "Iteration 13330 : Loss 3274.4817\n",
      "Iteration 13340 : Loss 3274.0869\n",
      "Iteration 13350 : Loss 3273.6926\n",
      "Iteration 13360 : Loss 3273.2988\n",
      "Iteration 13370 : Loss 3272.9055\n",
      "Iteration 13380 : Loss 3272.5128\n",
      "Iteration 13390 : Loss 3272.1206\n",
      "Iteration 13400 : Loss 3271.7289\n",
      "Iteration 13410 : Loss 3271.3378\n",
      "Iteration 13420 : Loss 3270.9472\n",
      "Iteration 13430 : Loss 3270.5571\n",
      "Iteration 13440 : Loss 3270.1675\n",
      "Iteration 13450 : Loss 3269.7785\n",
      "Iteration 13460 : Loss 3269.3900\n",
      "Iteration 13470 : Loss 3269.0020\n",
      "Iteration 13480 : Loss 3268.6145\n",
      "Iteration 13490 : Loss 3268.2276\n",
      "Iteration 13500 : Loss 3267.8412\n",
      "Iteration 13510 : Loss 3267.4553\n",
      "Iteration 13520 : Loss 3267.0699\n",
      "Iteration 13530 : Loss 3266.6850\n",
      "Iteration 13540 : Loss 3266.3007\n",
      "Iteration 13550 : Loss 3265.9168\n",
      "Iteration 13560 : Loss 3265.5335\n",
      "Iteration 13570 : Loss 3265.1507\n",
      "Iteration 13580 : Loss 3264.7684\n",
      "Iteration 13590 : Loss 3264.3866\n",
      "Iteration 13600 : Loss 3264.0053\n",
      "Iteration 13610 : Loss 3263.6245\n",
      "Iteration 13620 : Loss 3263.2443\n",
      "Iteration 13630 : Loss 3262.8645\n",
      "Iteration 13640 : Loss 3262.4853\n",
      "Iteration 13650 : Loss 3262.1065\n",
      "Iteration 13660 : Loss 3261.7283\n",
      "Iteration 13670 : Loss 3261.3505\n",
      "Iteration 13680 : Loss 3260.9733\n",
      "Iteration 13690 : Loss 3260.5966\n",
      "Iteration 13700 : Loss 3260.2203\n",
      "Iteration 13710 : Loss 3259.8446\n",
      "Iteration 13720 : Loss 3259.4693\n",
      "Iteration 13730 : Loss 3259.0946\n",
      "Iteration 13740 : Loss 3258.7203\n",
      "Iteration 13750 : Loss 3258.3466\n",
      "Iteration 13760 : Loss 3257.9733\n",
      "Iteration 13770 : Loss 3257.6006\n",
      "Iteration 13780 : Loss 3257.2283\n",
      "Iteration 13790 : Loss 3256.8565\n",
      "Iteration 13800 : Loss 3256.4852\n",
      "Iteration 13810 : Loss 3256.1144\n",
      "Iteration 13820 : Loss 3255.7441\n",
      "Iteration 13830 : Loss 3255.3743\n",
      "Iteration 13840 : Loss 3255.0049\n",
      "Iteration 13850 : Loss 3254.6361\n",
      "Iteration 13860 : Loss 3254.2677\n",
      "Iteration 13870 : Loss 3253.8998\n",
      "Iteration 13880 : Loss 3253.5324\n",
      "Iteration 13890 : Loss 3253.1655\n",
      "Iteration 13900 : Loss 3252.7991\n",
      "Iteration 13910 : Loss 3252.4331\n",
      "Iteration 13920 : Loss 3252.0676\n",
      "Iteration 13930 : Loss 3251.7026\n",
      "Iteration 13940 : Loss 3251.3381\n",
      "Iteration 13950 : Loss 3250.9741\n",
      "Iteration 13960 : Loss 3250.6105\n",
      "Iteration 13970 : Loss 3250.2474\n",
      "Iteration 13980 : Loss 3249.8848\n",
      "Iteration 13990 : Loss 3249.5226\n",
      "Iteration 14000 : Loss 3249.1610\n",
      "Iteration 14010 : Loss 3248.7998\n",
      "Iteration 14020 : Loss 3248.4390\n",
      "Iteration 14030 : Loss 3248.0788\n",
      "Iteration 14040 : Loss 3247.7190\n",
      "Iteration 14050 : Loss 3247.3596\n",
      "Iteration 14060 : Loss 3247.0008\n",
      "Iteration 14070 : Loss 3246.6424\n",
      "Iteration 14080 : Loss 3246.2845\n",
      "Iteration 14090 : Loss 3245.9270\n",
      "Iteration 14100 : Loss 3245.5700\n",
      "Iteration 14110 : Loss 3245.2135\n",
      "Iteration 14120 : Loss 3244.8574\n",
      "Iteration 14130 : Loss 3244.5018\n",
      "Iteration 14140 : Loss 3244.1466\n",
      "Iteration 14150 : Loss 3243.7919\n",
      "Iteration 14160 : Loss 3243.4377\n",
      "Iteration 14170 : Loss 3243.0839\n",
      "Iteration 14180 : Loss 3242.7306\n",
      "Iteration 14190 : Loss 3242.3777\n",
      "Iteration 14200 : Loss 3242.0253\n",
      "Iteration 14210 : Loss 3241.6733\n",
      "Iteration 14220 : Loss 3241.3218\n",
      "Iteration 14230 : Loss 3240.9708\n",
      "Iteration 14240 : Loss 3240.6202\n",
      "Iteration 14250 : Loss 3240.2700\n",
      "Iteration 14260 : Loss 3239.9203\n",
      "Iteration 14270 : Loss 3239.5711\n",
      "Iteration 14280 : Loss 3239.2223\n",
      "Iteration 14290 : Loss 3238.8739\n",
      "Iteration 14300 : Loss 3238.5260\n",
      "Iteration 14310 : Loss 3238.1785\n",
      "Iteration 14320 : Loss 3237.8315\n",
      "Iteration 14330 : Loss 3237.4849\n",
      "Iteration 14340 : Loss 3237.1388\n",
      "Iteration 14350 : Loss 3236.7931\n",
      "Iteration 14360 : Loss 3236.4479\n",
      "Iteration 14370 : Loss 3236.1031\n",
      "Iteration 14380 : Loss 3235.7587\n",
      "Iteration 14390 : Loss 3235.4148\n",
      "Iteration 14400 : Loss 3235.0713\n",
      "Iteration 14410 : Loss 3234.7282\n",
      "Iteration 14420 : Loss 3234.3856\n",
      "Iteration 14430 : Loss 3234.0434\n",
      "Iteration 14440 : Loss 3233.7017\n",
      "Iteration 14450 : Loss 3233.3604\n",
      "Iteration 14460 : Loss 3233.0195\n",
      "Iteration 14470 : Loss 3232.6791\n",
      "Iteration 14480 : Loss 3232.3391\n",
      "Iteration 14490 : Loss 3231.9995\n",
      "Iteration 14500 : Loss 3231.6603\n",
      "Iteration 14510 : Loss 3231.3216\n",
      "Iteration 14520 : Loss 3230.9833\n",
      "Iteration 14530 : Loss 3230.6455\n",
      "Iteration 14540 : Loss 3230.3081\n",
      "Iteration 14550 : Loss 3229.9710\n",
      "Iteration 14560 : Loss 3229.6345\n",
      "Iteration 14570 : Loss 3229.2983\n",
      "Iteration 14580 : Loss 3228.9626\n",
      "Iteration 14590 : Loss 3228.6273\n",
      "Iteration 14600 : Loss 3228.2924\n",
      "Iteration 14610 : Loss 3227.9579\n",
      "Iteration 14620 : Loss 3227.6239\n",
      "Iteration 14630 : Loss 3227.2903\n",
      "Iteration 14640 : Loss 3226.9571\n",
      "Iteration 14650 : Loss 3226.6243\n",
      "Iteration 14660 : Loss 3226.2920\n",
      "Iteration 14670 : Loss 3225.9600\n",
      "Iteration 14680 : Loss 3225.6285\n",
      "Iteration 14690 : Loss 3225.2974\n",
      "Iteration 14700 : Loss 3224.9667\n",
      "Iteration 14710 : Loss 3224.6364\n",
      "Iteration 14720 : Loss 3224.3065\n",
      "Iteration 14730 : Loss 3223.9771\n",
      "Iteration 14740 : Loss 3223.6481\n",
      "Iteration 14750 : Loss 3223.3194\n",
      "Iteration 14760 : Loss 3222.9912\n",
      "Iteration 14770 : Loss 3222.6634\n",
      "Iteration 14780 : Loss 3222.3360\n",
      "Iteration 14790 : Loss 3222.0090\n",
      "Iteration 14800 : Loss 3221.6825\n",
      "Iteration 14810 : Loss 3221.3563\n",
      "Iteration 14820 : Loss 3221.0305\n",
      "Iteration 14830 : Loss 3220.7052\n",
      "Iteration 14840 : Loss 3220.3802\n",
      "Iteration 14850 : Loss 3220.0557\n",
      "Iteration 14860 : Loss 3219.7315\n",
      "Iteration 14870 : Loss 3219.4078\n",
      "Iteration 14880 : Loss 3219.0844\n",
      "Iteration 14890 : Loss 3218.7615\n",
      "Iteration 14900 : Loss 3218.4390\n",
      "Iteration 14910 : Loss 3218.1168\n",
      "Iteration 14920 : Loss 3217.7951\n",
      "Iteration 14930 : Loss 3217.4738\n",
      "Iteration 14940 : Loss 3217.1528\n",
      "Iteration 14950 : Loss 3216.8323\n",
      "Iteration 14960 : Loss 3216.5122\n",
      "Iteration 14970 : Loss 3216.1924\n",
      "Iteration 14980 : Loss 3215.8731\n",
      "Iteration 14990 : Loss 3215.5541\n",
      "Iteration 15000 : Loss 3215.2355\n",
      "Iteration 15010 : Loss 3214.9174\n",
      "Iteration 15020 : Loss 3214.5996\n",
      "Iteration 15030 : Loss 3214.2822\n",
      "Iteration 15040 : Loss 3213.9652\n",
      "Iteration 15050 : Loss 3213.6486\n",
      "Iteration 15060 : Loss 3213.3324\n",
      "Iteration 15070 : Loss 3213.0166\n",
      "Iteration 15080 : Loss 3212.7012\n",
      "Iteration 15090 : Loss 3212.3861\n",
      "Iteration 15100 : Loss 3212.0715\n",
      "Iteration 15110 : Loss 3211.7572\n",
      "Iteration 15120 : Loss 3211.4433\n",
      "Iteration 15130 : Loss 3211.1298\n",
      "Iteration 15140 : Loss 3210.8167\n",
      "Iteration 15150 : Loss 3210.5040\n",
      "Iteration 15160 : Loss 3210.1916\n",
      "Iteration 15170 : Loss 3209.8796\n",
      "Iteration 15180 : Loss 3209.5681\n",
      "Iteration 15190 : Loss 3209.2569\n",
      "Iteration 15200 : Loss 3208.9460\n",
      "Iteration 15210 : Loss 3208.6356\n",
      "Iteration 15220 : Loss 3208.3255\n",
      "Iteration 15230 : Loss 3208.0159\n",
      "Iteration 15240 : Loss 3207.7066\n",
      "Iteration 15250 : Loss 3207.3976\n",
      "Iteration 15260 : Loss 3207.0891\n",
      "Iteration 15270 : Loss 3206.7809\n",
      "Iteration 15280 : Loss 3206.4731\n",
      "Iteration 15290 : Loss 3206.1657\n",
      "Iteration 15300 : Loss 3205.8587\n",
      "Iteration 15310 : Loss 3205.5520\n",
      "Iteration 15320 : Loss 3205.2457\n",
      "Iteration 15330 : Loss 3204.9398\n",
      "Iteration 15340 : Loss 3204.6342\n",
      "Iteration 15350 : Loss 3204.3290\n",
      "Iteration 15360 : Loss 3204.0242\n",
      "Iteration 15370 : Loss 3203.7198\n",
      "Iteration 15380 : Loss 3203.4157\n",
      "Iteration 15390 : Loss 3203.1120\n",
      "Iteration 15400 : Loss 3202.8086\n",
      "Iteration 15410 : Loss 3202.5057\n",
      "Iteration 15420 : Loss 3202.2031\n",
      "Iteration 15430 : Loss 3201.9008\n",
      "Iteration 15440 : Loss 3201.5990\n",
      "Iteration 15450 : Loss 3201.2975\n",
      "Iteration 15460 : Loss 3200.9963\n",
      "Iteration 15470 : Loss 3200.6955\n",
      "Iteration 15480 : Loss 3200.3951\n",
      "Iteration 15490 : Loss 3200.0951\n",
      "Iteration 15500 : Loss 3199.7954\n",
      "Iteration 15510 : Loss 3199.4961\n",
      "Iteration 15520 : Loss 3199.1971\n",
      "Iteration 15530 : Loss 3198.8985\n",
      "Iteration 15540 : Loss 3198.6002\n",
      "Iteration 15550 : Loss 3198.3023\n",
      "Iteration 15560 : Loss 3198.0048\n",
      "Iteration 15570 : Loss 3197.7076\n",
      "Iteration 15580 : Loss 3197.4108\n",
      "Iteration 15590 : Loss 3197.1144\n",
      "Iteration 15600 : Loss 3196.8183\n",
      "Iteration 15610 : Loss 3196.5225\n",
      "Iteration 15620 : Loss 3196.2271\n",
      "Iteration 15630 : Loss 3195.9321\n",
      "Iteration 15640 : Loss 3195.6374\n",
      "Iteration 15650 : Loss 3195.3431\n",
      "Iteration 15660 : Loss 3195.0491\n",
      "Iteration 15670 : Loss 3194.7554\n",
      "Iteration 15680 : Loss 3194.4622\n",
      "Iteration 15690 : Loss 3194.1692\n",
      "Iteration 15700 : Loss 3193.8767\n",
      "Iteration 15710 : Loss 3193.5844\n",
      "Iteration 15720 : Loss 3193.2926\n",
      "Iteration 15730 : Loss 3193.0010\n",
      "Iteration 15740 : Loss 3192.7099\n",
      "Iteration 15750 : Loss 3192.4190\n",
      "Iteration 15760 : Loss 3192.1285\n",
      "Iteration 15770 : Loss 3191.8384\n",
      "Iteration 15780 : Loss 3191.5486\n",
      "Iteration 15790 : Loss 3191.2591\n",
      "Iteration 15800 : Loss 3190.9700\n",
      "Iteration 15810 : Loss 3190.6813\n",
      "Iteration 15820 : Loss 3190.3929\n",
      "Iteration 15830 : Loss 3190.1048\n",
      "Iteration 15840 : Loss 3189.8171\n",
      "Iteration 15850 : Loss 3189.5297\n",
      "Iteration 15860 : Loss 3189.2426\n",
      "Iteration 15870 : Loss 3188.9559\n",
      "Iteration 15880 : Loss 3188.6695\n",
      "Iteration 15890 : Loss 3188.3835\n",
      "Iteration 15900 : Loss 3188.0978\n",
      "Iteration 15910 : Loss 3187.8125\n",
      "Iteration 15920 : Loss 3187.5274\n",
      "Iteration 15930 : Loss 3187.2428\n",
      "Iteration 15940 : Loss 3186.9584\n",
      "Iteration 15950 : Loss 3186.6744\n",
      "Iteration 15960 : Loss 3186.3907\n",
      "Iteration 15970 : Loss 3186.1074\n",
      "Iteration 15980 : Loss 3185.8244\n",
      "Iteration 15990 : Loss 3185.5417\n",
      "Iteration 16000 : Loss 3185.2594\n",
      "Iteration 16010 : Loss 3184.9774\n",
      "Iteration 16020 : Loss 3184.6957\n",
      "Iteration 16030 : Loss 3184.4144\n",
      "Iteration 16040 : Loss 3184.1334\n",
      "Iteration 16050 : Loss 3183.8527\n",
      "Iteration 16060 : Loss 3183.5724\n",
      "Iteration 16070 : Loss 3183.2924\n",
      "Iteration 16080 : Loss 3183.0127\n",
      "Iteration 16090 : Loss 3182.7333\n",
      "Iteration 16100 : Loss 3182.4543\n",
      "Iteration 16110 : Loss 3182.1756\n",
      "Iteration 16120 : Loss 3181.8972\n",
      "Iteration 16130 : Loss 3181.6192\n",
      "Iteration 16140 : Loss 3181.3414\n",
      "Iteration 16150 : Loss 3181.0641\n",
      "Iteration 16160 : Loss 3180.7870\n",
      "Iteration 16170 : Loss 3180.5102\n",
      "Iteration 16180 : Loss 3180.2338\n",
      "Iteration 16190 : Loss 3179.9577\n",
      "Iteration 16200 : Loss 3179.6819\n",
      "Iteration 16210 : Loss 3179.4065\n",
      "Iteration 16220 : Loss 3179.1314\n",
      "Iteration 16230 : Loss 3178.8565\n",
      "Iteration 16240 : Loss 3178.5821\n",
      "Iteration 16250 : Loss 3178.3079\n",
      "Iteration 16260 : Loss 3178.0340\n",
      "Iteration 16270 : Loss 3177.7605\n",
      "Iteration 16280 : Loss 3177.4873\n",
      "Iteration 16290 : Loss 3177.2144\n",
      "Iteration 16300 : Loss 3176.9418\n",
      "Iteration 16310 : Loss 3176.6696\n",
      "Iteration 16320 : Loss 3176.3976\n",
      "Iteration 16330 : Loss 3176.1260\n",
      "Iteration 16340 : Loss 3175.8547\n",
      "Iteration 16350 : Loss 3175.5837\n",
      "Iteration 16360 : Loss 3175.3130\n",
      "Iteration 16370 : Loss 3175.0427\n",
      "Iteration 16380 : Loss 3174.7726\n",
      "Iteration 16390 : Loss 3174.5029\n",
      "Iteration 16400 : Loss 3174.2335\n",
      "Iteration 16410 : Loss 3173.9644\n",
      "Iteration 16420 : Loss 3173.6956\n",
      "Iteration 16430 : Loss 3173.4271\n",
      "Iteration 16440 : Loss 3173.1589\n",
      "Iteration 16450 : Loss 3172.8910\n",
      "Iteration 16460 : Loss 3172.6235\n",
      "Iteration 16470 : Loss 3172.3562\n",
      "Iteration 16480 : Loss 3172.0893\n",
      "Iteration 16490 : Loss 3171.8227\n",
      "Iteration 16500 : Loss 3171.5563\n",
      "Iteration 16510 : Loss 3171.2903\n",
      "Iteration 16520 : Loss 3171.0246\n",
      "Iteration 16530 : Loss 3170.7592\n",
      "Iteration 16540 : Loss 3170.4941\n",
      "Iteration 16550 : Loss 3170.2293\n",
      "Iteration 16560 : Loss 3169.9649\n",
      "Iteration 16570 : Loss 3169.7007\n",
      "Iteration 16580 : Loss 3169.4368\n",
      "Iteration 16590 : Loss 3169.1732\n",
      "Iteration 16600 : Loss 3168.9100\n",
      "Iteration 16610 : Loss 3168.6470\n",
      "Iteration 16620 : Loss 3168.3844\n",
      "Iteration 16630 : Loss 3168.1220\n",
      "Iteration 16640 : Loss 3167.8599\n",
      "Iteration 16650 : Loss 3167.5982\n",
      "Iteration 16660 : Loss 3167.3367\n",
      "Iteration 16670 : Loss 3167.0756\n",
      "Iteration 16680 : Loss 3166.8147\n",
      "Iteration 16690 : Loss 3166.5542\n",
      "Iteration 16700 : Loss 3166.2939\n",
      "Iteration 16710 : Loss 3166.0339\n",
      "Iteration 16720 : Loss 3165.7743\n",
      "Iteration 16730 : Loss 3165.5149\n",
      "Iteration 16740 : Loss 3165.2558\n",
      "Iteration 16750 : Loss 3164.9971\n",
      "Iteration 16760 : Loss 3164.7386\n",
      "Iteration 16770 : Loss 3164.4804\n",
      "Iteration 16780 : Loss 3164.2225\n",
      "Iteration 16790 : Loss 3163.9649\n",
      "Iteration 16800 : Loss 3163.7077\n",
      "Iteration 16810 : Loss 3163.4506\n",
      "Iteration 16820 : Loss 3163.1939\n",
      "Iteration 16830 : Loss 3162.9375\n",
      "Iteration 16840 : Loss 3162.6814\n",
      "Iteration 16850 : Loss 3162.4256\n",
      "Iteration 16860 : Loss 3162.1700\n",
      "Iteration 16870 : Loss 3161.9148\n",
      "Iteration 16880 : Loss 3161.6598\n",
      "Iteration 16890 : Loss 3161.4052\n",
      "Iteration 16900 : Loss 3161.1508\n",
      "Iteration 16910 : Loss 3160.8967\n",
      "Iteration 16920 : Loss 3160.6429\n",
      "Iteration 16930 : Loss 3160.3894\n",
      "Iteration 16940 : Loss 3160.1361\n",
      "Iteration 16950 : Loss 3159.8832\n",
      "Iteration 16960 : Loss 3159.6306\n",
      "Iteration 16970 : Loss 3159.3782\n",
      "Iteration 16980 : Loss 3159.1261\n",
      "Iteration 16990 : Loss 3158.8743\n",
      "Iteration 17000 : Loss 3158.6228\n",
      "Iteration 17010 : Loss 3158.3716\n",
      "Iteration 17020 : Loss 3158.1207\n",
      "Iteration 17030 : Loss 3157.8700\n",
      "Iteration 17040 : Loss 3157.6196\n",
      "Iteration 17050 : Loss 3157.3696\n",
      "Iteration 17060 : Loss 3157.1198\n",
      "Iteration 17070 : Loss 3156.8702\n",
      "Iteration 17080 : Loss 3156.6210\n",
      "Iteration 17090 : Loss 3156.3721\n",
      "Iteration 17100 : Loss 3156.1234\n",
      "Iteration 17110 : Loss 3155.8750\n",
      "Iteration 17120 : Loss 3155.6269\n",
      "Iteration 17130 : Loss 3155.3790\n",
      "Iteration 17140 : Loss 3155.1315\n",
      "Iteration 17150 : Loss 3154.8842\n",
      "Iteration 17160 : Loss 3154.6372\n",
      "Iteration 17170 : Loss 3154.3905\n",
      "Iteration 17180 : Loss 3154.1441\n",
      "Iteration 17190 : Loss 3153.8979\n",
      "Iteration 17200 : Loss 3153.6520\n",
      "Iteration 17210 : Loss 3153.4064\n",
      "Iteration 17220 : Loss 3153.1611\n",
      "Iteration 17230 : Loss 3152.9160\n",
      "Iteration 17240 : Loss 3152.6712\n",
      "Iteration 17250 : Loss 3152.4267\n",
      "Iteration 17260 : Loss 3152.1825\n",
      "Iteration 17270 : Loss 3151.9385\n",
      "Iteration 17280 : Loss 3151.6948\n",
      "Iteration 17290 : Loss 3151.4514\n",
      "Iteration 17300 : Loss 3151.2083\n",
      "Iteration 17310 : Loss 3150.9654\n",
      "Iteration 17320 : Loss 3150.7228\n",
      "Iteration 17330 : Loss 3150.4805\n",
      "Iteration 17340 : Loss 3150.2385\n",
      "Iteration 17350 : Loss 3149.9967\n",
      "Iteration 17360 : Loss 3149.7552\n",
      "Iteration 17370 : Loss 3149.5139\n",
      "Iteration 17380 : Loss 3149.2730\n",
      "Iteration 17390 : Loss 3149.0323\n",
      "Iteration 17400 : Loss 3148.7918\n",
      "Iteration 17410 : Loss 3148.5517\n",
      "Iteration 17420 : Loss 3148.3118\n",
      "Iteration 17430 : Loss 3148.0722\n",
      "Iteration 17440 : Loss 3147.8328\n",
      "Iteration 17450 : Loss 3147.5937\n",
      "Iteration 17460 : Loss 3147.3549\n",
      "Iteration 17470 : Loss 3147.1163\n",
      "Iteration 17480 : Loss 3146.8780\n",
      "Iteration 17490 : Loss 3146.6400\n",
      "Iteration 17500 : Loss 3146.4022\n",
      "Iteration 17510 : Loss 3146.1647\n",
      "Iteration 17520 : Loss 3145.9275\n",
      "Iteration 17530 : Loss 3145.6905\n",
      "Iteration 17540 : Loss 3145.4538\n",
      "Iteration 17550 : Loss 3145.2174\n",
      "Iteration 17560 : Loss 3144.9812\n",
      "Iteration 17570 : Loss 3144.7453\n",
      "Iteration 17580 : Loss 3144.5096\n",
      "Iteration 17590 : Loss 3144.2743\n",
      "Iteration 17600 : Loss 3144.0391\n",
      "Iteration 17610 : Loss 3143.8043\n",
      "Iteration 17620 : Loss 3143.5696\n",
      "Iteration 17630 : Loss 3143.3353\n",
      "Iteration 17640 : Loss 3143.1012\n",
      "Iteration 17650 : Loss 3142.8674\n",
      "Iteration 17660 : Loss 3142.6338\n",
      "Iteration 17670 : Loss 3142.4005\n",
      "Iteration 17680 : Loss 3142.1674\n",
      "Iteration 17690 : Loss 3141.9346\n",
      "Iteration 17700 : Loss 3141.7021\n",
      "Iteration 17710 : Loss 3141.4698\n",
      "Iteration 17720 : Loss 3141.2378\n",
      "Iteration 17730 : Loss 3141.0060\n",
      "Iteration 17740 : Loss 3140.7745\n",
      "Iteration 17750 : Loss 3140.5432\n",
      "Iteration 17760 : Loss 3140.3122\n",
      "Iteration 17770 : Loss 3140.0815\n",
      "Iteration 17780 : Loss 3139.8510\n",
      "Iteration 17790 : Loss 3139.6208\n",
      "Iteration 17800 : Loss 3139.3908\n",
      "Iteration 17810 : Loss 3139.1610\n",
      "Iteration 17820 : Loss 3138.9316\n",
      "Iteration 17830 : Loss 3138.7023\n",
      "Iteration 17840 : Loss 3138.4734\n",
      "Iteration 17850 : Loss 3138.2447\n",
      "Iteration 17860 : Loss 3138.0162\n",
      "Iteration 17870 : Loss 3137.7880\n",
      "Iteration 17880 : Loss 3137.5600\n",
      "Iteration 17890 : Loss 3137.3323\n",
      "Iteration 17900 : Loss 3137.1048\n",
      "Iteration 17910 : Loss 3136.8776\n",
      "Iteration 17920 : Loss 3136.6506\n",
      "Iteration 17930 : Loss 3136.4239\n",
      "Iteration 17940 : Loss 3136.1975\n",
      "Iteration 17950 : Loss 3135.9712\n",
      "Iteration 17960 : Loss 3135.7453\n",
      "Iteration 17970 : Loss 3135.5195\n",
      "Iteration 17980 : Loss 3135.2941\n",
      "Iteration 17990 : Loss 3135.0688\n",
      "Iteration 18000 : Loss 3134.8439\n",
      "Iteration 18010 : Loss 3134.6191\n",
      "Iteration 18020 : Loss 3134.3946\n",
      "Iteration 18030 : Loss 3134.1704\n",
      "Iteration 18040 : Loss 3133.9464\n",
      "Iteration 18050 : Loss 3133.7226\n",
      "Iteration 18060 : Loss 3133.4991\n",
      "Iteration 18070 : Loss 3133.2759\n",
      "Iteration 18080 : Loss 3133.0529\n",
      "Iteration 18090 : Loss 3132.8301\n",
      "Iteration 18100 : Loss 3132.6075\n",
      "Iteration 18110 : Loss 3132.3853\n",
      "Iteration 18120 : Loss 3132.1632\n",
      "Iteration 18130 : Loss 3131.9414\n",
      "Iteration 18140 : Loss 3131.7198\n",
      "Iteration 18150 : Loss 3131.4985\n",
      "Iteration 18160 : Loss 3131.2774\n",
      "Iteration 18170 : Loss 3131.0566\n",
      "Iteration 18180 : Loss 3130.8360\n",
      "Iteration 18190 : Loss 3130.6157\n",
      "Iteration 18200 : Loss 3130.3955\n",
      "Iteration 18210 : Loss 3130.1757\n",
      "Iteration 18220 : Loss 3129.9560\n",
      "Iteration 18230 : Loss 3129.7366\n",
      "Iteration 18240 : Loss 3129.5175\n",
      "Iteration 18250 : Loss 3129.2985\n",
      "Iteration 18260 : Loss 3129.0799\n",
      "Iteration 18270 : Loss 3128.8614\n",
      "Iteration 18280 : Loss 3128.6432\n",
      "Iteration 18290 : Loss 3128.4252\n",
      "Iteration 18300 : Loss 3128.2075\n",
      "Iteration 18310 : Loss 3127.9900\n",
      "Iteration 18320 : Loss 3127.7727\n",
      "Iteration 18330 : Loss 3127.5557\n",
      "Iteration 18340 : Loss 3127.3389\n",
      "Iteration 18350 : Loss 3127.1224\n",
      "Iteration 18360 : Loss 3126.9060\n",
      "Iteration 18370 : Loss 3126.6899\n",
      "Iteration 18380 : Loss 3126.4741\n",
      "Iteration 18390 : Loss 3126.2585\n",
      "Iteration 18400 : Loss 3126.0431\n",
      "Iteration 18410 : Loss 3125.8279\n",
      "Iteration 18420 : Loss 3125.6130\n",
      "Iteration 18430 : Loss 3125.3983\n",
      "Iteration 18440 : Loss 3125.1839\n",
      "Iteration 18450 : Loss 3124.9696\n",
      "Iteration 18460 : Loss 3124.7557\n",
      "Iteration 18470 : Loss 3124.5419\n",
      "Iteration 18480 : Loss 3124.3284\n",
      "Iteration 18490 : Loss 3124.1151\n",
      "Iteration 18500 : Loss 3123.9020\n",
      "Iteration 18510 : Loss 3123.6892\n",
      "Iteration 18520 : Loss 3123.4765\n",
      "Iteration 18530 : Loss 3123.2642\n",
      "Iteration 18540 : Loss 3123.0520\n",
      "Iteration 18550 : Loss 3122.8401\n",
      "Iteration 18560 : Loss 3122.6284\n",
      "Iteration 18570 : Loss 3122.4169\n",
      "Iteration 18580 : Loss 3122.2057\n",
      "Iteration 18590 : Loss 3121.9947\n",
      "Iteration 18600 : Loss 3121.7839\n",
      "Iteration 18610 : Loss 3121.5733\n",
      "Iteration 18620 : Loss 3121.3630\n",
      "Iteration 18630 : Loss 3121.1529\n",
      "Iteration 18640 : Loss 3120.9430\n",
      "Iteration 18650 : Loss 3120.7334\n",
      "Iteration 18660 : Loss 3120.5239\n",
      "Iteration 18670 : Loss 3120.3147\n",
      "Iteration 18680 : Loss 3120.1058\n",
      "Iteration 18690 : Loss 3119.8970\n",
      "Iteration 18700 : Loss 3119.6885\n",
      "Iteration 18710 : Loss 3119.4802\n",
      "Iteration 18720 : Loss 3119.2721\n",
      "Iteration 18730 : Loss 3119.0642\n",
      "Iteration 18740 : Loss 3118.8566\n",
      "Iteration 18750 : Loss 3118.6492\n",
      "Iteration 18760 : Loss 3118.4420\n",
      "Iteration 18770 : Loss 3118.2350\n",
      "Iteration 18780 : Loss 3118.0283\n",
      "Iteration 18790 : Loss 3117.8218\n",
      "Iteration 18800 : Loss 3117.6155\n",
      "Iteration 18810 : Loss 3117.4094\n",
      "Iteration 18820 : Loss 3117.2035\n",
      "Iteration 18830 : Loss 3116.9979\n",
      "Iteration 18840 : Loss 3116.7924\n",
      "Iteration 18850 : Loss 3116.5872\n",
      "Iteration 18860 : Loss 3116.3823\n",
      "Iteration 18870 : Loss 3116.1775\n",
      "Iteration 18880 : Loss 3115.9730\n",
      "Iteration 18890 : Loss 3115.7686\n",
      "Iteration 18900 : Loss 3115.5645\n",
      "Iteration 18910 : Loss 3115.3606\n",
      "Iteration 18920 : Loss 3115.1570\n",
      "Iteration 18930 : Loss 3114.9535\n",
      "Iteration 18940 : Loss 3114.7503\n",
      "Iteration 18950 : Loss 3114.5472\n",
      "Iteration 18960 : Loss 3114.3444\n",
      "Iteration 18970 : Loss 3114.1418\n",
      "Iteration 18980 : Loss 3113.9395\n",
      "Iteration 18990 : Loss 3113.7373\n",
      "Iteration 19000 : Loss 3113.5354\n",
      "Iteration 19010 : Loss 3113.3336\n",
      "Iteration 19020 : Loss 3113.1321\n",
      "Iteration 19030 : Loss 3112.9308\n",
      "Iteration 19040 : Loss 3112.7297\n",
      "Iteration 19050 : Loss 3112.5289\n",
      "Iteration 19060 : Loss 3112.3282\n",
      "Iteration 19070 : Loss 3112.1278\n",
      "Iteration 19080 : Loss 3111.9275\n",
      "Iteration 19090 : Loss 3111.7275\n",
      "Iteration 19100 : Loss 3111.5277\n",
      "Iteration 19110 : Loss 3111.3281\n",
      "Iteration 19120 : Loss 3111.1287\n",
      "Iteration 19130 : Loss 3110.9296\n",
      "Iteration 19140 : Loss 3110.7306\n",
      "Iteration 19150 : Loss 3110.5319\n",
      "Iteration 19160 : Loss 3110.3333\n",
      "Iteration 19170 : Loss 3110.1350\n",
      "Iteration 19180 : Loss 3109.9369\n",
      "Iteration 19190 : Loss 3109.7390\n",
      "Iteration 19200 : Loss 3109.5413\n",
      "Iteration 19210 : Loss 3109.3438\n",
      "Iteration 19220 : Loss 3109.1465\n",
      "Iteration 19230 : Loss 3108.9494\n",
      "Iteration 19240 : Loss 3108.7526\n",
      "Iteration 19250 : Loss 3108.5559\n",
      "Iteration 19260 : Loss 3108.3595\n",
      "Iteration 19270 : Loss 3108.1632\n",
      "Iteration 19280 : Loss 3107.9672\n",
      "Iteration 19290 : Loss 3107.7714\n",
      "Iteration 19300 : Loss 3107.5758\n",
      "Iteration 19310 : Loss 3107.3804\n",
      "Iteration 19320 : Loss 3107.1852\n",
      "Iteration 19330 : Loss 3106.9902\n",
      "Iteration 19340 : Loss 3106.7954\n",
      "Iteration 19350 : Loss 3106.6008\n",
      "Iteration 19360 : Loss 3106.4064\n",
      "Iteration 19370 : Loss 3106.2122\n",
      "Iteration 19380 : Loss 3106.0183\n",
      "Iteration 19390 : Loss 3105.8245\n",
      "Iteration 19400 : Loss 3105.6309\n",
      "Iteration 19410 : Loss 3105.4376\n",
      "Iteration 19420 : Loss 3105.2444\n",
      "Iteration 19430 : Loss 3105.0515\n",
      "Iteration 19440 : Loss 3104.8587\n",
      "Iteration 19450 : Loss 3104.6662\n",
      "Iteration 19460 : Loss 3104.4739\n",
      "Iteration 19470 : Loss 3104.2817\n",
      "Iteration 19480 : Loss 3104.0898\n",
      "Iteration 19490 : Loss 3103.8981\n",
      "Iteration 19500 : Loss 3103.7065\n",
      "Iteration 19510 : Loss 3103.5152\n",
      "Iteration 19520 : Loss 3103.3241\n",
      "Iteration 19530 : Loss 3103.1331\n",
      "Iteration 19540 : Loss 3102.9424\n",
      "Iteration 19550 : Loss 3102.7519\n",
      "Iteration 19560 : Loss 3102.5615\n",
      "Iteration 19570 : Loss 3102.3714\n",
      "Iteration 19580 : Loss 3102.1815\n",
      "Iteration 19590 : Loss 3101.9918\n",
      "Iteration 19600 : Loss 3101.8022\n",
      "Iteration 19610 : Loss 3101.6129\n",
      "Iteration 19620 : Loss 3101.4238\n",
      "Iteration 19630 : Loss 3101.2348\n",
      "Iteration 19640 : Loss 3101.0461\n",
      "Iteration 19650 : Loss 3100.8576\n",
      "Iteration 19660 : Loss 3100.6692\n",
      "Iteration 19670 : Loss 3100.4811\n",
      "Iteration 19680 : Loss 3100.2931\n",
      "Iteration 19690 : Loss 3100.1054\n",
      "Iteration 19700 : Loss 3099.9178\n",
      "Iteration 19710 : Loss 3099.7305\n",
      "Iteration 19720 : Loss 3099.5433\n",
      "Iteration 19730 : Loss 3099.3563\n",
      "Iteration 19740 : Loss 3099.1696\n",
      "Iteration 19750 : Loss 3098.9830\n",
      "Iteration 19760 : Loss 3098.7966\n",
      "Iteration 19770 : Loss 3098.6104\n",
      "Iteration 19780 : Loss 3098.4244\n",
      "Iteration 19790 : Loss 3098.2387\n",
      "Iteration 19800 : Loss 3098.0531\n",
      "Iteration 19810 : Loss 3097.8676\n",
      "Iteration 19820 : Loss 3097.6824\n",
      "Iteration 19830 : Loss 3097.4974\n",
      "Iteration 19840 : Loss 3097.3126\n",
      "Iteration 19850 : Loss 3097.1279\n",
      "Iteration 19860 : Loss 3096.9435\n",
      "Iteration 19870 : Loss 3096.7593\n",
      "Iteration 19880 : Loss 3096.5752\n",
      "Iteration 19890 : Loss 3096.3913\n",
      "Iteration 19900 : Loss 3096.2077\n",
      "Iteration 19910 : Loss 3096.0242\n",
      "Iteration 19920 : Loss 3095.8409\n",
      "Iteration 19930 : Loss 3095.6578\n",
      "Iteration 19940 : Loss 3095.4749\n",
      "Iteration 19950 : Loss 3095.2921\n",
      "Iteration 19960 : Loss 3095.1096\n",
      "Iteration 19970 : Loss 3094.9273\n",
      "Iteration 19980 : Loss 3094.7451\n",
      "Iteration 19990 : Loss 3094.5632\n",
      "Iteration 20000 : Loss 3094.3814\n",
      "Iteration 20010 : Loss 3094.1998\n",
      "Iteration 20020 : Loss 3094.0184\n",
      "Iteration 20030 : Loss 3093.8372\n",
      "Iteration 20040 : Loss 3093.6562\n",
      "Iteration 20050 : Loss 3093.4754\n",
      "Iteration 20060 : Loss 3093.2947\n",
      "Iteration 20070 : Loss 3093.1143\n",
      "Iteration 20080 : Loss 3092.9340\n",
      "Iteration 20090 : Loss 3092.7539\n",
      "Iteration 20100 : Loss 3092.5740\n",
      "Iteration 20110 : Loss 3092.3943\n",
      "Iteration 20120 : Loss 3092.2148\n",
      "Iteration 20130 : Loss 3092.0355\n",
      "Iteration 20140 : Loss 3091.8563\n",
      "Iteration 20150 : Loss 3091.6773\n",
      "Iteration 20160 : Loss 3091.4986\n",
      "Iteration 20170 : Loss 3091.3200\n",
      "Iteration 20180 : Loss 3091.1416\n",
      "Iteration 20190 : Loss 3090.9633\n",
      "Iteration 20200 : Loss 3090.7853\n",
      "Iteration 20210 : Loss 3090.6075\n",
      "Iteration 20220 : Loss 3090.4298\n",
      "Iteration 20230 : Loss 3090.2523\n",
      "Iteration 20240 : Loss 3090.0750\n",
      "Iteration 20250 : Loss 3089.8979\n",
      "Iteration 20260 : Loss 3089.7209\n",
      "Iteration 20270 : Loss 3089.5442\n",
      "Iteration 20280 : Loss 3089.3676\n",
      "Iteration 20290 : Loss 3089.1912\n",
      "Iteration 20300 : Loss 3089.0150\n",
      "Iteration 20310 : Loss 3088.8390\n",
      "Iteration 20320 : Loss 3088.6632\n",
      "Iteration 20330 : Loss 3088.4875\n",
      "Iteration 20340 : Loss 3088.3120\n",
      "Iteration 20350 : Loss 3088.1367\n",
      "Iteration 20360 : Loss 3087.9616\n",
      "Iteration 20370 : Loss 3087.7867\n",
      "Iteration 20380 : Loss 3087.6119\n",
      "Iteration 20390 : Loss 3087.4373\n",
      "Iteration 20400 : Loss 3087.2629\n",
      "Iteration 20410 : Loss 3087.0887\n",
      "Iteration 20420 : Loss 3086.9147\n",
      "Iteration 20430 : Loss 3086.7408\n",
      "Iteration 20440 : Loss 3086.5671\n",
      "Iteration 20450 : Loss 3086.3936\n",
      "Iteration 20460 : Loss 3086.2203\n",
      "Iteration 20470 : Loss 3086.0472\n",
      "Iteration 20480 : Loss 3085.8742\n",
      "Iteration 20490 : Loss 3085.7014\n",
      "Iteration 20500 : Loss 3085.5288\n",
      "Iteration 20510 : Loss 3085.3564\n",
      "Iteration 20520 : Loss 3085.1841\n",
      "Iteration 20530 : Loss 3085.0121\n",
      "Iteration 20540 : Loss 3084.8402\n",
      "Iteration 20550 : Loss 3084.6684\n",
      "Iteration 20560 : Loss 3084.4969\n",
      "Iteration 20570 : Loss 3084.3255\n",
      "Iteration 20580 : Loss 3084.1543\n",
      "Iteration 20590 : Loss 3083.9833\n",
      "Iteration 20600 : Loss 3083.8125\n",
      "Iteration 20610 : Loss 3083.6418\n",
      "Iteration 20620 : Loss 3083.4713\n",
      "Iteration 20630 : Loss 3083.3010\n",
      "Iteration 20640 : Loss 3083.1308\n",
      "Iteration 20650 : Loss 3082.9609\n",
      "Iteration 20660 : Loss 3082.7911\n",
      "Iteration 20670 : Loss 3082.6215\n",
      "Iteration 20680 : Loss 3082.4520\n",
      "Iteration 20690 : Loss 3082.2827\n",
      "Iteration 20700 : Loss 3082.1136\n",
      "Iteration 20710 : Loss 3081.9447\n",
      "Iteration 20720 : Loss 3081.7760\n",
      "Iteration 20730 : Loss 3081.6074\n",
      "Iteration 20740 : Loss 3081.4390\n",
      "Iteration 20750 : Loss 3081.2707\n",
      "Iteration 20760 : Loss 3081.1027\n",
      "Iteration 20770 : Loss 3080.9348\n",
      "Iteration 20780 : Loss 3080.7671\n",
      "Iteration 20790 : Loss 3080.5995\n",
      "Iteration 20800 : Loss 3080.4321\n",
      "Iteration 20810 : Loss 3080.2649\n",
      "Iteration 20820 : Loss 3080.0979\n",
      "Iteration 20830 : Loss 3079.9310\n",
      "Iteration 20840 : Loss 3079.7643\n",
      "Iteration 20850 : Loss 3079.5978\n",
      "Iteration 20860 : Loss 3079.4315\n",
      "Iteration 20870 : Loss 3079.2653\n",
      "Iteration 20880 : Loss 3079.0993\n",
      "Iteration 20890 : Loss 3078.9334\n",
      "Iteration 20900 : Loss 3078.7677\n",
      "Iteration 20910 : Loss 3078.6022\n",
      "Iteration 20920 : Loss 3078.4369\n",
      "Iteration 20930 : Loss 3078.2717\n",
      "Iteration 20940 : Loss 3078.1067\n",
      "Iteration 20950 : Loss 3077.9419\n",
      "Iteration 20960 : Loss 3077.7772\n",
      "Iteration 20970 : Loss 3077.6128\n",
      "Iteration 20980 : Loss 3077.4484\n",
      "Iteration 20990 : Loss 3077.2843\n",
      "Iteration 21000 : Loss 3077.1203\n",
      "Iteration 21010 : Loss 3076.9564\n",
      "Iteration 21020 : Loss 3076.7928\n",
      "Iteration 21030 : Loss 3076.6293\n",
      "Iteration 21040 : Loss 3076.4660\n",
      "Iteration 21050 : Loss 3076.3028\n",
      "Iteration 21060 : Loss 3076.1398\n",
      "Iteration 21070 : Loss 3075.9770\n",
      "Iteration 21080 : Loss 3075.8143\n",
      "Iteration 21090 : Loss 3075.6518\n",
      "Iteration 21100 : Loss 3075.4895\n",
      "Iteration 21110 : Loss 3075.3273\n",
      "Iteration 21120 : Loss 3075.1653\n",
      "Iteration 21130 : Loss 3075.0035\n",
      "Iteration 21140 : Loss 3074.8418\n",
      "Iteration 21150 : Loss 3074.6803\n",
      "Iteration 21160 : Loss 3074.5190\n",
      "Iteration 21170 : Loss 3074.3578\n",
      "Iteration 21180 : Loss 3074.1968\n",
      "Iteration 21190 : Loss 3074.0359\n",
      "Iteration 21200 : Loss 3073.8752\n",
      "Iteration 21210 : Loss 3073.7147\n",
      "Iteration 21220 : Loss 3073.5543\n",
      "Iteration 21230 : Loss 3073.3941\n",
      "Iteration 21240 : Loss 3073.2341\n",
      "Iteration 21250 : Loss 3073.0742\n",
      "Iteration 21260 : Loss 3072.9145\n",
      "Iteration 21270 : Loss 3072.7549\n",
      "Iteration 21280 : Loss 3072.5956\n",
      "Iteration 21290 : Loss 3072.4363\n",
      "Iteration 21300 : Loss 3072.2773\n",
      "Iteration 21310 : Loss 3072.1184\n",
      "Iteration 21320 : Loss 3071.9596\n",
      "Iteration 21330 : Loss 3071.8010\n",
      "Iteration 21340 : Loss 3071.6426\n",
      "Iteration 21350 : Loss 3071.4843\n",
      "Iteration 21360 : Loss 3071.3262\n",
      "Iteration 21370 : Loss 3071.1683\n",
      "Iteration 21380 : Loss 3071.0105\n",
      "Iteration 21390 : Loss 3070.8529\n",
      "Iteration 21400 : Loss 3070.6954\n",
      "Iteration 21410 : Loss 3070.5381\n",
      "Iteration 21420 : Loss 3070.3810\n",
      "Iteration 21430 : Loss 3070.2240\n",
      "Iteration 21440 : Loss 3070.0671\n",
      "Iteration 21450 : Loss 3069.9105\n",
      "Iteration 21460 : Loss 3069.7540\n",
      "Iteration 21470 : Loss 3069.5976\n",
      "Iteration 21480 : Loss 3069.4414\n",
      "Iteration 21490 : Loss 3069.2854\n",
      "Iteration 21500 : Loss 3069.1295\n",
      "Iteration 21510 : Loss 3068.9738\n",
      "Iteration 21520 : Loss 3068.8182\n",
      "Iteration 21530 : Loss 3068.6628\n",
      "Iteration 21540 : Loss 3068.5075\n",
      "Iteration 21550 : Loss 3068.3524\n",
      "Iteration 21560 : Loss 3068.1975\n",
      "Iteration 21570 : Loss 3068.0427\n",
      "Iteration 21580 : Loss 3067.8881\n",
      "Iteration 21590 : Loss 3067.7336\n",
      "Iteration 21600 : Loss 3067.5793\n",
      "Iteration 21610 : Loss 3067.4251\n",
      "Iteration 21620 : Loss 3067.2711\n",
      "Iteration 21630 : Loss 3067.1173\n",
      "Iteration 21640 : Loss 3066.9636\n",
      "Iteration 21650 : Loss 3066.8100\n",
      "Iteration 21660 : Loss 3066.6567\n",
      "Iteration 21670 : Loss 3066.5034\n",
      "Iteration 21680 : Loss 3066.3504\n",
      "Iteration 21690 : Loss 3066.1974\n",
      "Iteration 21700 : Loss 3066.0447\n",
      "Iteration 21710 : Loss 3065.8920\n",
      "Iteration 21720 : Loss 3065.7396\n",
      "Iteration 21730 : Loss 3065.5873\n",
      "Iteration 21740 : Loss 3065.4351\n",
      "Iteration 21750 : Loss 3065.2831\n",
      "Iteration 21760 : Loss 3065.1313\n",
      "Iteration 21770 : Loss 3064.9796\n",
      "Iteration 21780 : Loss 3064.8280\n",
      "Iteration 21790 : Loss 3064.6766\n",
      "Iteration 21800 : Loss 3064.5254\n",
      "Iteration 21810 : Loss 3064.3743\n",
      "Iteration 21820 : Loss 3064.2234\n",
      "Iteration 21830 : Loss 3064.0726\n",
      "Iteration 21840 : Loss 3063.9220\n",
      "Iteration 21850 : Loss 3063.7715\n",
      "Iteration 21860 : Loss 3063.6211\n",
      "Iteration 21870 : Loss 3063.4710\n",
      "Iteration 21880 : Loss 3063.3209\n",
      "Iteration 21890 : Loss 3063.1710\n",
      "Iteration 21900 : Loss 3063.0213\n",
      "Iteration 21910 : Loss 3062.8717\n",
      "Iteration 21920 : Loss 3062.7223\n",
      "Iteration 21930 : Loss 3062.5730\n",
      "Iteration 21940 : Loss 3062.4239\n",
      "Iteration 21950 : Loss 3062.2749\n",
      "Iteration 21960 : Loss 3062.1261\n",
      "Iteration 21970 : Loss 3061.9774\n",
      "Iteration 21980 : Loss 3061.8289\n",
      "Iteration 21990 : Loss 3061.6805\n",
      "Iteration 22000 : Loss 3061.5323\n",
      "Iteration 22010 : Loss 3061.3842\n",
      "Iteration 22020 : Loss 3061.2362\n",
      "Iteration 22030 : Loss 3061.0884\n",
      "Iteration 22040 : Loss 3060.9408\n",
      "Iteration 22050 : Loss 3060.7933\n",
      "Iteration 22060 : Loss 3060.6460\n",
      "Iteration 22070 : Loss 3060.4988\n",
      "Iteration 22080 : Loss 3060.3517\n",
      "Iteration 22090 : Loss 3060.2048\n",
      "Iteration 22100 : Loss 3060.0580\n",
      "Iteration 22110 : Loss 3059.9114\n",
      "Iteration 22120 : Loss 3059.7650\n",
      "Iteration 22130 : Loss 3059.6186\n",
      "Iteration 22140 : Loss 3059.4725\n",
      "Iteration 22150 : Loss 3059.3264\n",
      "Iteration 22160 : Loss 3059.1806\n",
      "Iteration 22170 : Loss 3059.0348\n",
      "Iteration 22180 : Loss 3058.8892\n",
      "Iteration 22190 : Loss 3058.7438\n",
      "Iteration 22200 : Loss 3058.5985\n",
      "Iteration 22210 : Loss 3058.4533\n",
      "Iteration 22220 : Loss 3058.3083\n",
      "Iteration 22230 : Loss 3058.1635\n",
      "Iteration 22240 : Loss 3058.0187\n",
      "Iteration 22250 : Loss 3057.8742\n",
      "Iteration 22260 : Loss 3057.7297\n",
      "Iteration 22270 : Loss 3057.5855\n",
      "Iteration 22280 : Loss 3057.4413\n",
      "Iteration 22290 : Loss 3057.2973\n",
      "Iteration 22300 : Loss 3057.1535\n",
      "Iteration 22310 : Loss 3057.0097\n",
      "Iteration 22320 : Loss 3056.8662\n",
      "Iteration 22330 : Loss 3056.7227\n",
      "Iteration 22340 : Loss 3056.5795\n",
      "Iteration 22350 : Loss 3056.4363\n",
      "Iteration 22360 : Loss 3056.2933\n",
      "Iteration 22370 : Loss 3056.1505\n",
      "Iteration 22380 : Loss 3056.0077\n",
      "Iteration 22390 : Loss 3055.8652\n",
      "Iteration 22400 : Loss 3055.7227\n",
      "Iteration 22410 : Loss 3055.5805\n",
      "Iteration 22420 : Loss 3055.4383\n",
      "Iteration 22430 : Loss 3055.2963\n",
      "Iteration 22440 : Loss 3055.1544\n",
      "Iteration 22450 : Loss 3055.0127\n",
      "Iteration 22460 : Loss 3054.8711\n",
      "Iteration 22470 : Loss 3054.7297\n",
      "Iteration 22480 : Loss 3054.5884\n",
      "Iteration 22490 : Loss 3054.4472\n",
      "Iteration 22500 : Loss 3054.3062\n",
      "Iteration 22510 : Loss 3054.1653\n",
      "Iteration 22520 : Loss 3054.0246\n",
      "Iteration 22530 : Loss 3053.8840\n",
      "Iteration 22540 : Loss 3053.7435\n",
      "Iteration 22550 : Loss 3053.6032\n",
      "Iteration 22560 : Loss 3053.4630\n",
      "Iteration 22570 : Loss 3053.3230\n",
      "Iteration 22580 : Loss 3053.1831\n",
      "Iteration 22590 : Loss 3053.0433\n",
      "Iteration 22600 : Loss 3052.9037\n",
      "Iteration 22610 : Loss 3052.7642\n",
      "Iteration 22620 : Loss 3052.6248\n",
      "Iteration 22630 : Loss 3052.4856\n",
      "Iteration 22640 : Loss 3052.3466\n",
      "Iteration 22650 : Loss 3052.2076\n",
      "Iteration 22660 : Loss 3052.0688\n",
      "Iteration 22670 : Loss 3051.9302\n",
      "Iteration 22680 : Loss 3051.7916\n",
      "Iteration 22690 : Loss 3051.6533\n",
      "Iteration 22700 : Loss 3051.5150\n",
      "Iteration 22710 : Loss 3051.3769\n",
      "Iteration 22720 : Loss 3051.2389\n",
      "Iteration 22730 : Loss 3051.1011\n",
      "Iteration 22740 : Loss 3050.9634\n",
      "Iteration 22750 : Loss 3050.8258\n",
      "Iteration 22760 : Loss 3050.6884\n",
      "Iteration 22770 : Loss 3050.5511\n",
      "Iteration 22780 : Loss 3050.4139\n",
      "Iteration 22790 : Loss 3050.2769\n",
      "Iteration 22800 : Loss 3050.1400\n",
      "Iteration 22810 : Loss 3050.0033\n",
      "Iteration 22820 : Loss 3049.8667\n",
      "Iteration 22830 : Loss 3049.7302\n",
      "Iteration 22840 : Loss 3049.5938\n",
      "Iteration 22850 : Loss 3049.4576\n",
      "Iteration 22860 : Loss 3049.3216\n",
      "Iteration 22870 : Loss 3049.1856\n",
      "Iteration 22880 : Loss 3049.0498\n",
      "Iteration 22890 : Loss 3048.9141\n",
      "Iteration 22900 : Loss 3048.7786\n",
      "Iteration 22910 : Loss 3048.6432\n",
      "Iteration 22920 : Loss 3048.5079\n",
      "Iteration 22930 : Loss 3048.3728\n",
      "Iteration 22940 : Loss 3048.2378\n",
      "Iteration 22950 : Loss 3048.1029\n",
      "Iteration 22960 : Loss 3047.9682\n",
      "Iteration 22970 : Loss 3047.8336\n",
      "Iteration 22980 : Loss 3047.6991\n",
      "Iteration 22990 : Loss 3047.5648\n",
      "Iteration 23000 : Loss 3047.4306\n",
      "Iteration 23010 : Loss 3047.2965\n",
      "Iteration 23020 : Loss 3047.1625\n",
      "Iteration 23030 : Loss 3047.0287\n",
      "Iteration 23040 : Loss 3046.8951\n",
      "Iteration 23050 : Loss 3046.7615\n",
      "Iteration 23060 : Loss 3046.6281\n",
      "Iteration 23070 : Loss 3046.4948\n",
      "Iteration 23080 : Loss 3046.3617\n",
      "Iteration 23090 : Loss 3046.2287\n",
      "Iteration 23100 : Loss 3046.0958\n",
      "Iteration 23110 : Loss 3045.9630\n",
      "Iteration 23120 : Loss 3045.8304\n",
      "Iteration 23130 : Loss 3045.6979\n",
      "Iteration 23140 : Loss 3045.5655\n",
      "Iteration 23150 : Loss 3045.4333\n",
      "Iteration 23160 : Loss 3045.3012\n",
      "Iteration 23170 : Loss 3045.1692\n",
      "Iteration 23180 : Loss 3045.0374\n",
      "Iteration 23190 : Loss 3044.9056\n",
      "Iteration 23200 : Loss 3044.7741\n",
      "Iteration 23210 : Loss 3044.6426\n",
      "Iteration 23220 : Loss 3044.5113\n",
      "Iteration 23230 : Loss 3044.3801\n",
      "Iteration 23240 : Loss 3044.2490\n",
      "Iteration 23250 : Loss 3044.1181\n",
      "Iteration 23260 : Loss 3043.9873\n",
      "Iteration 23270 : Loss 3043.8566\n",
      "Iteration 23280 : Loss 3043.7260\n",
      "Iteration 23290 : Loss 3043.5956\n",
      "Iteration 23300 : Loss 3043.4653\n",
      "Iteration 23310 : Loss 3043.3351\n",
      "Iteration 23320 : Loss 3043.2051\n",
      "Iteration 23330 : Loss 3043.0752\n",
      "Iteration 23340 : Loss 3042.9454\n",
      "Iteration 23350 : Loss 3042.8157\n",
      "Iteration 23360 : Loss 3042.6862\n",
      "Iteration 23370 : Loss 3042.5568\n",
      "Iteration 23380 : Loss 3042.4275\n",
      "Iteration 23390 : Loss 3042.2984\n",
      "Iteration 23400 : Loss 3042.1693\n",
      "Iteration 23410 : Loss 3042.0404\n",
      "Iteration 23420 : Loss 3041.9117\n",
      "Iteration 23430 : Loss 3041.7830\n",
      "Iteration 23440 : Loss 3041.6545\n",
      "Iteration 23450 : Loss 3041.5261\n",
      "Iteration 23460 : Loss 3041.3978\n",
      "Iteration 23470 : Loss 3041.2697\n",
      "Iteration 23480 : Loss 3041.1417\n",
      "Iteration 23490 : Loss 3041.0138\n",
      "Iteration 23500 : Loss 3040.8860\n",
      "Iteration 23510 : Loss 3040.7584\n",
      "Iteration 23520 : Loss 3040.6309\n",
      "Iteration 23530 : Loss 3040.5035\n",
      "Iteration 23540 : Loss 3040.3762\n",
      "Iteration 23550 : Loss 3040.2491\n",
      "Iteration 23560 : Loss 3040.1221\n",
      "Iteration 23570 : Loss 3039.9952\n",
      "Iteration 23580 : Loss 3039.8684\n",
      "Iteration 23590 : Loss 3039.7418\n",
      "Iteration 23600 : Loss 3039.6153\n",
      "Iteration 23610 : Loss 3039.4889\n",
      "Iteration 23620 : Loss 3039.3626\n",
      "Iteration 23630 : Loss 3039.2365\n",
      "Iteration 23640 : Loss 3039.1104\n",
      "Iteration 23650 : Loss 3038.9845\n",
      "Iteration 23660 : Loss 3038.8588\n",
      "Iteration 23670 : Loss 3038.7331\n",
      "Iteration 23680 : Loss 3038.6076\n",
      "Iteration 23690 : Loss 3038.4822\n",
      "Iteration 23700 : Loss 3038.3569\n",
      "Iteration 23710 : Loss 3038.2317\n",
      "Iteration 23720 : Loss 3038.1067\n",
      "Iteration 23730 : Loss 3037.9818\n",
      "Iteration 23740 : Loss 3037.8570\n",
      "Iteration 23750 : Loss 3037.7323\n",
      "Iteration 23760 : Loss 3037.6077\n",
      "Iteration 23770 : Loss 3037.4833\n",
      "Iteration 23780 : Loss 3037.3590\n",
      "Iteration 23790 : Loss 3037.2348\n",
      "Iteration 23800 : Loss 3037.1107\n",
      "Iteration 23810 : Loss 3036.9868\n",
      "Iteration 23820 : Loss 3036.8630\n",
      "Iteration 23830 : Loss 3036.7393\n",
      "Iteration 23840 : Loss 3036.6157\n",
      "Iteration 23850 : Loss 3036.4922\n",
      "Iteration 23860 : Loss 3036.3689\n",
      "Iteration 23870 : Loss 3036.2457\n",
      "Iteration 23880 : Loss 3036.1226\n",
      "Iteration 23890 : Loss 3035.9996\n",
      "Iteration 23900 : Loss 3035.8767\n",
      "Iteration 23910 : Loss 3035.7540\n",
      "Iteration 23920 : Loss 3035.6313\n",
      "Iteration 23930 : Loss 3035.5088\n",
      "Iteration 23940 : Loss 3035.3865\n",
      "Iteration 23950 : Loss 3035.2642\n",
      "Iteration 23960 : Loss 3035.1421\n",
      "Iteration 23970 : Loss 3035.0200\n",
      "Iteration 23980 : Loss 3034.8981\n",
      "Iteration 23990 : Loss 3034.7763\n",
      "Iteration 24000 : Loss 3034.6547\n",
      "Iteration 24010 : Loss 3034.5331\n",
      "Iteration 24020 : Loss 3034.4117\n",
      "Iteration 24030 : Loss 3034.2904\n",
      "Iteration 24040 : Loss 3034.1692\n",
      "Iteration 24050 : Loss 3034.0481\n",
      "Iteration 24060 : Loss 3033.9271\n",
      "Iteration 24070 : Loss 3033.8063\n",
      "Iteration 24080 : Loss 3033.6856\n",
      "Iteration 24090 : Loss 3033.5650\n",
      "Iteration 24100 : Loss 3033.4445\n",
      "Iteration 24110 : Loss 3033.3241\n",
      "Iteration 24120 : Loss 3033.2038\n",
      "Iteration 24130 : Loss 3033.0837\n",
      "Iteration 24140 : Loss 3032.9637\n",
      "Iteration 24150 : Loss 3032.8438\n",
      "Iteration 24160 : Loss 3032.7240\n",
      "Iteration 24170 : Loss 3032.6043\n",
      "Iteration 24180 : Loss 3032.4848\n",
      "Iteration 24190 : Loss 3032.3653\n",
      "Iteration 24200 : Loss 3032.2460\n",
      "Iteration 24210 : Loss 3032.1268\n",
      "Iteration 24220 : Loss 3032.0077\n",
      "Iteration 24230 : Loss 3031.8887\n",
      "Iteration 24240 : Loss 3031.7699\n",
      "Iteration 24250 : Loss 3031.6511\n",
      "Iteration 24260 : Loss 3031.5325\n",
      "Iteration 24270 : Loss 3031.4140\n",
      "Iteration 24280 : Loss 3031.2956\n",
      "Iteration 24290 : Loss 3031.1773\n",
      "Iteration 24300 : Loss 3031.0591\n",
      "Iteration 24310 : Loss 3030.9411\n",
      "Iteration 24320 : Loss 3030.8231\n",
      "Iteration 24330 : Loss 3030.7053\n",
      "Iteration 24340 : Loss 3030.5876\n",
      "Iteration 24350 : Loss 3030.4700\n",
      "Iteration 24360 : Loss 3030.3525\n",
      "Iteration 24370 : Loss 3030.2351\n",
      "Iteration 24380 : Loss 3030.1179\n",
      "Iteration 24390 : Loss 3030.0007\n",
      "Iteration 24400 : Loss 3029.8837\n",
      "Iteration 24410 : Loss 3029.7668\n",
      "Iteration 24420 : Loss 3029.6500\n",
      "Iteration 24430 : Loss 3029.5333\n",
      "Iteration 24440 : Loss 3029.4167\n",
      "Iteration 24450 : Loss 3029.3003\n",
      "Iteration 24460 : Loss 3029.1839\n",
      "Iteration 24470 : Loss 3029.0677\n",
      "Iteration 24480 : Loss 3028.9516\n",
      "Iteration 24490 : Loss 3028.8356\n",
      "Iteration 24500 : Loss 3028.7197\n",
      "Iteration 24510 : Loss 3028.6039\n",
      "Iteration 24520 : Loss 3028.4882\n",
      "Iteration 24530 : Loss 3028.3726\n",
      "Iteration 24540 : Loss 3028.2572\n",
      "Iteration 24550 : Loss 3028.1419\n",
      "Iteration 24560 : Loss 3028.0266\n",
      "Iteration 24570 : Loss 3027.9115\n",
      "Iteration 24580 : Loss 3027.7965\n",
      "Iteration 24590 : Loss 3027.6816\n",
      "Iteration 24600 : Loss 3027.5668\n",
      "Iteration 24610 : Loss 3027.4522\n",
      "Iteration 24620 : Loss 3027.3376\n",
      "Iteration 24630 : Loss 3027.2231\n",
      "Iteration 24640 : Loss 3027.1088\n",
      "Iteration 24650 : Loss 3026.9946\n",
      "Iteration 24660 : Loss 3026.8805\n",
      "Iteration 24670 : Loss 3026.7665\n",
      "Iteration 24680 : Loss 3026.6526\n",
      "Iteration 24690 : Loss 3026.5388\n",
      "Iteration 24700 : Loss 3026.4251\n",
      "Iteration 24710 : Loss 3026.3115\n",
      "Iteration 24720 : Loss 3026.1981\n",
      "Iteration 24730 : Loss 3026.0847\n",
      "Iteration 24740 : Loss 3025.9715\n",
      "Iteration 24750 : Loss 3025.8584\n",
      "Iteration 24760 : Loss 3025.7453\n",
      "Iteration 24770 : Loss 3025.6324\n",
      "Iteration 24780 : Loss 3025.5196\n",
      "Iteration 24790 : Loss 3025.4069\n",
      "Iteration 24800 : Loss 3025.2944\n",
      "Iteration 24810 : Loss 3025.1819\n",
      "Iteration 24820 : Loss 3025.0695\n",
      "Iteration 24830 : Loss 3024.9573\n",
      "Iteration 24840 : Loss 3024.8451\n",
      "Iteration 24850 : Loss 3024.7331\n",
      "Iteration 24860 : Loss 3024.6211\n",
      "Iteration 24870 : Loss 3024.5093\n",
      "Iteration 24880 : Loss 3024.3976\n",
      "Iteration 24890 : Loss 3024.2860\n",
      "Iteration 24900 : Loss 3024.1745\n",
      "Iteration 24910 : Loss 3024.0631\n",
      "Iteration 24920 : Loss 3023.9518\n",
      "Iteration 24930 : Loss 3023.8406\n",
      "Iteration 24940 : Loss 3023.7295\n",
      "Iteration 24950 : Loss 3023.6186\n",
      "Iteration 24960 : Loss 3023.5077\n",
      "Iteration 24970 : Loss 3023.3970\n",
      "Iteration 24980 : Loss 3023.2863\n",
      "Iteration 24990 : Loss 3023.1758\n",
      "Iteration 25000 : Loss 3023.0654\n",
      "Iteration 25010 : Loss 3022.9550\n",
      "Iteration 25020 : Loss 3022.8448\n",
      "Iteration 25030 : Loss 3022.7347\n",
      "Iteration 25040 : Loss 3022.6247\n",
      "Iteration 25050 : Loss 3022.5148\n",
      "Iteration 25060 : Loss 3022.4050\n",
      "Iteration 25070 : Loss 3022.2953\n",
      "Iteration 25080 : Loss 3022.1857\n",
      "Iteration 25090 : Loss 3022.0762\n",
      "Iteration 25100 : Loss 3021.9669\n",
      "Iteration 25110 : Loss 3021.8576\n",
      "Iteration 25120 : Loss 3021.7484\n",
      "Iteration 25130 : Loss 3021.6394\n",
      "Iteration 25140 : Loss 3021.5304\n",
      "Iteration 25150 : Loss 3021.4216\n",
      "Iteration 25160 : Loss 3021.3128\n",
      "Iteration 25170 : Loss 3021.2042\n",
      "Iteration 25180 : Loss 3021.0957\n",
      "Iteration 25190 : Loss 3020.9872\n",
      "Iteration 25200 : Loss 3020.8789\n",
      "Iteration 25210 : Loss 3020.7707\n",
      "Iteration 25220 : Loss 3020.6626\n",
      "Iteration 25230 : Loss 3020.5545\n",
      "Iteration 25240 : Loss 3020.4466\n",
      "Iteration 25250 : Loss 3020.3388\n",
      "Iteration 25260 : Loss 3020.2311\n",
      "Iteration 25270 : Loss 3020.1235\n",
      "Iteration 25280 : Loss 3020.0160\n",
      "Iteration 25290 : Loss 3019.9086\n",
      "Iteration 25300 : Loss 3019.8013\n",
      "Iteration 25310 : Loss 3019.6942\n",
      "Iteration 25320 : Loss 3019.5871\n",
      "Iteration 25330 : Loss 3019.4801\n",
      "Iteration 25340 : Loss 3019.3732\n",
      "Iteration 25350 : Loss 3019.2664\n",
      "Iteration 25360 : Loss 3019.1598\n",
      "Iteration 25370 : Loss 3019.0532\n",
      "Iteration 25380 : Loss 3018.9467\n",
      "Iteration 25390 : Loss 3018.8404\n",
      "Iteration 25400 : Loss 3018.7341\n",
      "Iteration 25410 : Loss 3018.6279\n",
      "Iteration 25420 : Loss 3018.5219\n",
      "Iteration 25430 : Loss 3018.4159\n",
      "Iteration 25440 : Loss 3018.3101\n",
      "Iteration 25450 : Loss 3018.2043\n",
      "Iteration 25460 : Loss 3018.0986\n",
      "Iteration 25470 : Loss 3017.9931\n",
      "Iteration 25480 : Loss 3017.8876\n",
      "Iteration 25490 : Loss 3017.7823\n",
      "Iteration 25500 : Loss 3017.6770\n",
      "Iteration 25510 : Loss 3017.5719\n",
      "Iteration 25520 : Loss 3017.4668\n",
      "Iteration 25530 : Loss 3017.3619\n",
      "Iteration 25540 : Loss 3017.2570\n",
      "Iteration 25550 : Loss 3017.1523\n",
      "Iteration 25560 : Loss 3017.0477\n",
      "Iteration 25570 : Loss 3016.9431\n",
      "Iteration 25580 : Loss 3016.8387\n",
      "Iteration 25590 : Loss 3016.7343\n",
      "Iteration 25600 : Loss 3016.6301\n",
      "Iteration 25610 : Loss 3016.5259\n",
      "Iteration 25620 : Loss 3016.4219\n",
      "Iteration 25630 : Loss 3016.3179\n",
      "Iteration 25640 : Loss 3016.2141\n",
      "Iteration 25650 : Loss 3016.1103\n",
      "Iteration 25660 : Loss 3016.0067\n",
      "Iteration 25670 : Loss 3015.9031\n",
      "Iteration 25680 : Loss 3015.7997\n",
      "Iteration 25690 : Loss 3015.6963\n",
      "Iteration 25700 : Loss 3015.5931\n",
      "Iteration 25710 : Loss 3015.4899\n",
      "Iteration 25720 : Loss 3015.3869\n",
      "Iteration 25730 : Loss 3015.2839\n",
      "Iteration 25740 : Loss 3015.1811\n",
      "Iteration 25750 : Loss 3015.0783\n",
      "Iteration 25760 : Loss 3014.9756\n",
      "Iteration 25770 : Loss 3014.8731\n",
      "Iteration 25780 : Loss 3014.7706\n",
      "Iteration 25790 : Loss 3014.6682\n",
      "Iteration 25800 : Loss 3014.5660\n",
      "Iteration 25810 : Loss 3014.4638\n",
      "Iteration 25820 : Loss 3014.3617\n",
      "Iteration 25830 : Loss 3014.2597\n",
      "Iteration 25840 : Loss 3014.1579\n",
      "Iteration 25850 : Loss 3014.0561\n",
      "Iteration 25860 : Loss 3013.9544\n",
      "Iteration 25870 : Loss 3013.8528\n",
      "Iteration 25880 : Loss 3013.7513\n",
      "Iteration 25890 : Loss 3013.6499\n",
      "Iteration 25900 : Loss 3013.5486\n",
      "Iteration 25910 : Loss 3013.4474\n",
      "Iteration 25920 : Loss 3013.3463\n",
      "Iteration 25930 : Loss 3013.2453\n",
      "Iteration 25940 : Loss 3013.1444\n",
      "Iteration 25950 : Loss 3013.0436\n",
      "Iteration 25960 : Loss 3012.9429\n",
      "Iteration 25970 : Loss 3012.8422\n",
      "Iteration 25980 : Loss 3012.7417\n",
      "Iteration 25990 : Loss 3012.6413\n",
      "Iteration 26000 : Loss 3012.5409\n",
      "Iteration 26010 : Loss 3012.4407\n",
      "Iteration 26020 : Loss 3012.3406\n",
      "Iteration 26030 : Loss 3012.2405\n",
      "Iteration 26040 : Loss 3012.1406\n",
      "Iteration 26050 : Loss 3012.0407\n",
      "Iteration 26060 : Loss 3011.9409\n",
      "Iteration 26070 : Loss 3011.8413\n",
      "Iteration 26080 : Loss 3011.7417\n",
      "Iteration 26090 : Loss 3011.6422\n",
      "Iteration 26100 : Loss 3011.5428\n",
      "Iteration 26110 : Loss 3011.4435\n",
      "Iteration 26120 : Loss 3011.3444\n",
      "Iteration 26130 : Loss 3011.2453\n",
      "Iteration 26140 : Loss 3011.1463\n",
      "Iteration 26150 : Loss 3011.0473\n",
      "Iteration 26160 : Loss 3010.9485\n",
      "Iteration 26170 : Loss 3010.8498\n",
      "Iteration 26180 : Loss 3010.7512\n",
      "Iteration 26190 : Loss 3010.6526\n",
      "Iteration 26200 : Loss 3010.5542\n",
      "Iteration 26210 : Loss 3010.4559\n",
      "Iteration 26220 : Loss 3010.3576\n",
      "Iteration 26230 : Loss 3010.2594\n",
      "Iteration 26240 : Loss 3010.1614\n",
      "Iteration 26250 : Loss 3010.0634\n",
      "Iteration 26260 : Loss 3009.9655\n",
      "Iteration 26270 : Loss 3009.8677\n",
      "Iteration 26280 : Loss 3009.7700\n",
      "Iteration 26290 : Loss 3009.6724\n",
      "Iteration 26300 : Loss 3009.5749\n",
      "Iteration 26310 : Loss 3009.4775\n",
      "Iteration 26320 : Loss 3009.3802\n",
      "Iteration 26330 : Loss 3009.2830\n",
      "Iteration 26340 : Loss 3009.1858\n",
      "Iteration 26350 : Loss 3009.0888\n",
      "Iteration 26360 : Loss 3008.9918\n",
      "Iteration 26370 : Loss 3008.8950\n",
      "Iteration 26380 : Loss 3008.7982\n",
      "Iteration 26390 : Loss 3008.7015\n",
      "Iteration 26400 : Loss 3008.6049\n",
      "Iteration 26410 : Loss 3008.5084\n",
      "Iteration 26420 : Loss 3008.4120\n",
      "Iteration 26430 : Loss 3008.3157\n",
      "Iteration 26440 : Loss 3008.2195\n",
      "Iteration 26450 : Loss 3008.1234\n",
      "Iteration 26460 : Loss 3008.0274\n",
      "Iteration 26470 : Loss 3007.9314\n",
      "Iteration 26480 : Loss 3007.8356\n",
      "Iteration 26490 : Loss 3007.7398\n",
      "Iteration 26500 : Loss 3007.6441\n",
      "Iteration 26510 : Loss 3007.5485\n",
      "Iteration 26520 : Loss 3007.4530\n",
      "Iteration 26530 : Loss 3007.3576\n",
      "Iteration 26540 : Loss 3007.2623\n",
      "Iteration 26550 : Loss 3007.1671\n",
      "Iteration 26560 : Loss 3007.0720\n",
      "Iteration 26570 : Loss 3006.9770\n",
      "Iteration 26580 : Loss 3006.8820\n",
      "Iteration 26590 : Loss 3006.7871\n",
      "Iteration 26600 : Loss 3006.6924\n",
      "Iteration 26610 : Loss 3006.5977\n",
      "Iteration 26620 : Loss 3006.5031\n",
      "Iteration 26630 : Loss 3006.4086\n",
      "Iteration 26640 : Loss 3006.3142\n",
      "Iteration 26650 : Loss 3006.2199\n",
      "Iteration 26660 : Loss 3006.1257\n",
      "Iteration 26670 : Loss 3006.0315\n",
      "Iteration 26680 : Loss 3005.9375\n",
      "Iteration 26690 : Loss 3005.8435\n",
      "Iteration 26700 : Loss 3005.7496\n",
      "Iteration 26710 : Loss 3005.6558\n",
      "Iteration 26720 : Loss 3005.5622\n",
      "Iteration 26730 : Loss 3005.4685\n",
      "Iteration 26740 : Loss 3005.3750\n",
      "Iteration 26750 : Loss 3005.2816\n",
      "Iteration 26760 : Loss 3005.1883\n",
      "Iteration 26770 : Loss 3005.0950\n",
      "Iteration 26780 : Loss 3005.0018\n",
      "Iteration 26790 : Loss 3004.9088\n",
      "Iteration 26800 : Loss 3004.8158\n",
      "Iteration 26810 : Loss 3004.7229\n",
      "Iteration 26820 : Loss 3004.6301\n",
      "Iteration 26830 : Loss 3004.5373\n",
      "Iteration 26840 : Loss 3004.4447\n",
      "Iteration 26850 : Loss 3004.3522\n",
      "Iteration 26860 : Loss 3004.2597\n",
      "Iteration 26870 : Loss 3004.1673\n",
      "Iteration 26880 : Loss 3004.0750\n",
      "Iteration 26890 : Loss 3003.9828\n",
      "Iteration 26900 : Loss 3003.8907\n",
      "Iteration 26910 : Loss 3003.7987\n",
      "Iteration 26920 : Loss 3003.7068\n",
      "Iteration 26930 : Loss 3003.6149\n",
      "Iteration 26940 : Loss 3003.5232\n",
      "Iteration 26950 : Loss 3003.4315\n",
      "Iteration 26960 : Loss 3003.3399\n",
      "Iteration 26970 : Loss 3003.2484\n",
      "Iteration 26980 : Loss 3003.1570\n",
      "Iteration 26990 : Loss 3003.0656\n",
      "Iteration 27000 : Loss 3002.9744\n",
      "Iteration 27010 : Loss 3002.8832\n",
      "Iteration 27020 : Loss 3002.7922\n",
      "Iteration 27030 : Loss 3002.7012\n",
      "Iteration 27040 : Loss 3002.6103\n",
      "Iteration 27050 : Loss 3002.5195\n",
      "Iteration 27060 : Loss 3002.4287\n",
      "Iteration 27070 : Loss 3002.3381\n",
      "Iteration 27080 : Loss 3002.2475\n",
      "Iteration 27090 : Loss 3002.1571\n",
      "Iteration 27100 : Loss 3002.0667\n",
      "Iteration 27110 : Loss 3001.9764\n",
      "Iteration 27120 : Loss 3001.8862\n",
      "Iteration 27130 : Loss 3001.7960\n",
      "Iteration 27140 : Loss 3001.7060\n",
      "Iteration 27150 : Loss 3001.6160\n",
      "Iteration 27160 : Loss 3001.5261\n",
      "Iteration 27170 : Loss 3001.4364\n",
      "Iteration 27180 : Loss 3001.3466\n",
      "Iteration 27190 : Loss 3001.2570\n",
      "Iteration 27200 : Loss 3001.1675\n",
      "Iteration 27210 : Loss 3001.0780\n",
      "Iteration 27220 : Loss 3000.9887\n",
      "Iteration 27230 : Loss 3000.8994\n",
      "Iteration 27240 : Loss 3000.8102\n",
      "Iteration 27250 : Loss 3000.7211\n",
      "Iteration 27260 : Loss 3000.6320\n",
      "Iteration 27270 : Loss 3000.5431\n",
      "Iteration 27280 : Loss 3000.4542\n",
      "Iteration 27290 : Loss 3000.3655\n",
      "Iteration 27300 : Loss 3000.2768\n",
      "Iteration 27310 : Loss 3000.1881\n",
      "Iteration 27320 : Loss 3000.0996\n",
      "Iteration 27330 : Loss 3000.0112\n",
      "Iteration 27340 : Loss 2999.9228\n",
      "Iteration 27350 : Loss 2999.8345\n",
      "Iteration 27360 : Loss 2999.7463\n",
      "Iteration 27370 : Loss 2999.6582\n",
      "Iteration 27380 : Loss 2999.5702\n",
      "Iteration 27390 : Loss 2999.4822\n",
      "Iteration 27400 : Loss 2999.3944\n",
      "Iteration 27410 : Loss 2999.3066\n",
      "Iteration 27420 : Loss 2999.2189\n",
      "Iteration 27430 : Loss 2999.1313\n",
      "Iteration 27440 : Loss 2999.0438\n",
      "Iteration 27450 : Loss 2998.9563\n",
      "Iteration 27460 : Loss 2998.8689\n",
      "Iteration 27470 : Loss 2998.7817\n",
      "Iteration 27480 : Loss 2998.6944\n",
      "Iteration 27490 : Loss 2998.6073\n",
      "Iteration 27500 : Loss 2998.5203\n",
      "Iteration 27510 : Loss 2998.4333\n",
      "Iteration 27520 : Loss 2998.3465\n",
      "Iteration 27530 : Loss 2998.2597\n",
      "Iteration 27540 : Loss 2998.1729\n",
      "Iteration 27550 : Loss 2998.0863\n",
      "Iteration 27560 : Loss 2997.9998\n",
      "Iteration 27570 : Loss 2997.9133\n",
      "Iteration 27580 : Loss 2997.8269\n",
      "Iteration 27590 : Loss 2997.7406\n",
      "Iteration 27600 : Loss 2997.6544\n",
      "Iteration 27610 : Loss 2997.5682\n",
      "Iteration 27620 : Loss 2997.4822\n",
      "Iteration 27630 : Loss 2997.3962\n",
      "Iteration 27640 : Loss 2997.3103\n",
      "Iteration 27650 : Loss 2997.2245\n",
      "Iteration 27660 : Loss 2997.1387\n",
      "Iteration 27670 : Loss 2997.0531\n",
      "Iteration 27680 : Loss 2996.9675\n",
      "Iteration 27690 : Loss 2996.8820\n",
      "Iteration 27700 : Loss 2996.7966\n",
      "Iteration 27710 : Loss 2996.7112\n",
      "Iteration 27720 : Loss 2996.6260\n",
      "Iteration 27730 : Loss 2996.5408\n",
      "Iteration 27740 : Loss 2996.4557\n",
      "Iteration 27750 : Loss 2996.3707\n",
      "Iteration 27760 : Loss 2996.2858\n",
      "Iteration 27770 : Loss 2996.2009\n",
      "Iteration 27780 : Loss 2996.1161\n",
      "Iteration 27790 : Loss 2996.0314\n",
      "Iteration 27800 : Loss 2995.9468\n",
      "Iteration 27810 : Loss 2995.8623\n",
      "Iteration 27820 : Loss 2995.7778\n",
      "Iteration 27830 : Loss 2995.6934\n",
      "Iteration 27840 : Loss 2995.6091\n",
      "Iteration 27850 : Loss 2995.5249\n",
      "Iteration 27860 : Loss 2995.4408\n",
      "Iteration 27870 : Loss 2995.3567\n",
      "Iteration 27880 : Loss 2995.2727\n",
      "Iteration 27890 : Loss 2995.1888\n",
      "Iteration 27900 : Loss 2995.1050\n",
      "Iteration 27910 : Loss 2995.0212\n",
      "Iteration 27920 : Loss 2994.9375\n",
      "Iteration 27930 : Loss 2994.8540\n",
      "Iteration 27940 : Loss 2994.7704\n",
      "Iteration 27950 : Loss 2994.6870\n",
      "Iteration 27960 : Loss 2994.6036\n",
      "Iteration 27970 : Loss 2994.5204\n",
      "Iteration 27980 : Loss 2994.4372\n",
      "Iteration 27990 : Loss 2994.3540\n",
      "Iteration 28000 : Loss 2994.2710\n",
      "Iteration 28010 : Loss 2994.1880\n",
      "Iteration 28020 : Loss 2994.1051\n",
      "Iteration 28030 : Loss 2994.0223\n",
      "Iteration 28040 : Loss 2993.9396\n",
      "Iteration 28050 : Loss 2993.8569\n",
      "Iteration 28060 : Loss 2993.7744\n",
      "Iteration 28070 : Loss 2993.6919\n",
      "Iteration 28080 : Loss 2993.6094\n",
      "Iteration 28090 : Loss 2993.5271\n",
      "Iteration 28100 : Loss 2993.4448\n",
      "Iteration 28110 : Loss 2993.3626\n",
      "Iteration 28120 : Loss 2993.2805\n",
      "Iteration 28130 : Loss 2993.1985\n",
      "Iteration 28140 : Loss 2993.1165\n",
      "Iteration 28150 : Loss 2993.0346\n",
      "Iteration 28160 : Loss 2992.9528\n",
      "Iteration 28170 : Loss 2992.8711\n",
      "Iteration 28180 : Loss 2992.7894\n",
      "Iteration 28190 : Loss 2992.7078\n",
      "Iteration 28200 : Loss 2992.6263\n",
      "Iteration 28210 : Loss 2992.5449\n",
      "Iteration 28220 : Loss 2992.4635\n",
      "Iteration 28230 : Loss 2992.3823\n",
      "Iteration 28240 : Loss 2992.3011\n",
      "Iteration 28250 : Loss 2992.2200\n",
      "Iteration 28260 : Loss 2992.1389\n",
      "Iteration 28270 : Loss 2992.0579\n",
      "Iteration 28280 : Loss 2991.9770\n",
      "Iteration 28290 : Loss 2991.8962\n",
      "Iteration 28300 : Loss 2991.8155\n",
      "Iteration 28310 : Loss 2991.7348\n",
      "Iteration 28320 : Loss 2991.6542\n",
      "Iteration 28330 : Loss 2991.5737\n",
      "Iteration 28340 : Loss 2991.4932\n",
      "Iteration 28350 : Loss 2991.4129\n",
      "Iteration 28360 : Loss 2991.3326\n",
      "Iteration 28370 : Loss 2991.2524\n",
      "Iteration 28380 : Loss 2991.1722\n",
      "Iteration 28390 : Loss 2991.0922\n",
      "Iteration 28400 : Loss 2991.0122\n",
      "Iteration 28410 : Loss 2990.9322\n",
      "Iteration 28420 : Loss 2990.8524\n",
      "Iteration 28430 : Loss 2990.7726\n",
      "Iteration 28440 : Loss 2990.6929\n",
      "Iteration 28450 : Loss 2990.6133\n",
      "Iteration 28460 : Loss 2990.5338\n",
      "Iteration 28470 : Loss 2990.4543\n",
      "Iteration 28480 : Loss 2990.3749\n",
      "Iteration 28490 : Loss 2990.2956\n",
      "Iteration 28500 : Loss 2990.2163\n",
      "Iteration 28510 : Loss 2990.1372\n",
      "Iteration 28520 : Loss 2990.0581\n",
      "Iteration 28530 : Loss 2989.9790\n",
      "Iteration 28540 : Loss 2989.9001\n",
      "Iteration 28550 : Loss 2989.8212\n",
      "Iteration 28560 : Loss 2989.7424\n",
      "Iteration 28570 : Loss 2989.6637\n",
      "Iteration 28580 : Loss 2989.5850\n",
      "Iteration 28590 : Loss 2989.5064\n",
      "Iteration 28600 : Loss 2989.4279\n",
      "Iteration 28610 : Loss 2989.3495\n",
      "Iteration 28620 : Loss 2989.2711\n",
      "Iteration 28630 : Loss 2989.1928\n",
      "Iteration 28640 : Loss 2989.1146\n",
      "Iteration 28650 : Loss 2989.0364\n",
      "Iteration 28660 : Loss 2988.9583\n",
      "Iteration 28670 : Loss 2988.8803\n",
      "Iteration 28680 : Loss 2988.8024\n",
      "Iteration 28690 : Loss 2988.7246\n",
      "Iteration 28700 : Loss 2988.6468\n",
      "Iteration 28710 : Loss 2988.5691\n",
      "Iteration 28720 : Loss 2988.4914\n",
      "Iteration 28730 : Loss 2988.4139\n",
      "Iteration 28740 : Loss 2988.3364\n",
      "Iteration 28750 : Loss 2988.2589\n",
      "Iteration 28760 : Loss 2988.1816\n",
      "Iteration 28770 : Loss 2988.1043\n",
      "Iteration 28780 : Loss 2988.0271\n",
      "Iteration 28790 : Loss 2987.9500\n",
      "Iteration 28800 : Loss 2987.8729\n",
      "Iteration 28810 : Loss 2987.7959\n",
      "Iteration 28820 : Loss 2987.7190\n",
      "Iteration 28830 : Loss 2987.6422\n",
      "Iteration 28840 : Loss 2987.5654\n",
      "Iteration 28850 : Loss 2987.4887\n",
      "Iteration 28860 : Loss 2987.4120\n",
      "Iteration 28870 : Loss 2987.3355\n",
      "Iteration 28880 : Loss 2987.2590\n",
      "Iteration 28890 : Loss 2987.1826\n",
      "Iteration 28900 : Loss 2987.1062\n",
      "Iteration 28910 : Loss 2987.0300\n",
      "Iteration 28920 : Loss 2986.9537\n",
      "Iteration 28930 : Loss 2986.8776\n",
      "Iteration 28940 : Loss 2986.8015\n",
      "Iteration 28950 : Loss 2986.7256\n",
      "Iteration 28960 : Loss 2986.6496\n",
      "Iteration 28970 : Loss 2986.5738\n",
      "Iteration 28980 : Loss 2986.4980\n",
      "Iteration 28990 : Loss 2986.4223\n",
      "Iteration 29000 : Loss 2986.3466\n",
      "Iteration 29010 : Loss 2986.2711\n",
      "Iteration 29020 : Loss 2986.1956\n",
      "Iteration 29030 : Loss 2986.1201\n",
      "Iteration 29040 : Loss 2986.0448\n",
      "Iteration 29050 : Loss 2985.9695\n",
      "Iteration 29060 : Loss 2985.8943\n",
      "Iteration 29070 : Loss 2985.8191\n",
      "Iteration 29080 : Loss 2985.7440\n",
      "Iteration 29090 : Loss 2985.6690\n",
      "Iteration 29100 : Loss 2985.5941\n",
      "Iteration 29110 : Loss 2985.5192\n",
      "Iteration 29120 : Loss 2985.4444\n",
      "Iteration 29130 : Loss 2985.3697\n",
      "Iteration 29140 : Loss 2985.2950\n",
      "Iteration 29150 : Loss 2985.2204\n",
      "Iteration 29160 : Loss 2985.1459\n",
      "Iteration 29170 : Loss 2985.0715\n",
      "Iteration 29180 : Loss 2984.9971\n",
      "Iteration 29190 : Loss 2984.9228\n",
      "Iteration 29200 : Loss 2984.8485\n",
      "Iteration 29210 : Loss 2984.7743\n",
      "Iteration 29220 : Loss 2984.7002\n",
      "Iteration 29230 : Loss 2984.6262\n",
      "Iteration 29240 : Loss 2984.5522\n",
      "Iteration 29250 : Loss 2984.4783\n",
      "Iteration 29260 : Loss 2984.4045\n",
      "Iteration 29270 : Loss 2984.3307\n",
      "Iteration 29280 : Loss 2984.2570\n",
      "Iteration 29290 : Loss 2984.1834\n",
      "Iteration 29300 : Loss 2984.1098\n",
      "Iteration 29310 : Loss 2984.0363\n",
      "Iteration 29320 : Loss 2983.9629\n",
      "Iteration 29330 : Loss 2983.8895\n",
      "Iteration 29340 : Loss 2983.8163\n",
      "Iteration 29350 : Loss 2983.7430\n",
      "Iteration 29360 : Loss 2983.6699\n",
      "Iteration 29370 : Loss 2983.5968\n",
      "Iteration 29380 : Loss 2983.5238\n",
      "Iteration 29390 : Loss 2983.4508\n",
      "Iteration 29400 : Loss 2983.3780\n",
      "Iteration 29410 : Loss 2983.3051\n",
      "Iteration 29420 : Loss 2983.2324\n",
      "Iteration 29430 : Loss 2983.1597\n",
      "Iteration 29440 : Loss 2983.0871\n",
      "Iteration 29450 : Loss 2983.0146\n",
      "Iteration 29460 : Loss 2982.9421\n",
      "Iteration 29470 : Loss 2982.8697\n",
      "Iteration 29480 : Loss 2982.7973\n",
      "Iteration 29490 : Loss 2982.7251\n",
      "Iteration 29500 : Loss 2982.6528\n",
      "Iteration 29510 : Loss 2982.5807\n",
      "Iteration 29520 : Loss 2982.5086\n",
      "Iteration 29530 : Loss 2982.4366\n",
      "Iteration 29540 : Loss 2982.3647\n",
      "Iteration 29550 : Loss 2982.2928\n",
      "Iteration 29560 : Loss 2982.2210\n",
      "Iteration 29570 : Loss 2982.1493\n",
      "Iteration 29580 : Loss 2982.0776\n",
      "Iteration 29590 : Loss 2982.0060\n",
      "Iteration 29600 : Loss 2981.9344\n",
      "Iteration 29610 : Loss 2981.8629\n",
      "Iteration 29620 : Loss 2981.7915\n",
      "Iteration 29630 : Loss 2981.7202\n",
      "Iteration 29640 : Loss 2981.6489\n",
      "Iteration 29650 : Loss 2981.5777\n",
      "Iteration 29660 : Loss 2981.5066\n",
      "Iteration 29670 : Loss 2981.4355\n",
      "Iteration 29680 : Loss 2981.3645\n",
      "Iteration 29690 : Loss 2981.2935\n",
      "Iteration 29700 : Loss 2981.2226\n",
      "Iteration 29710 : Loss 2981.1518\n",
      "Iteration 29720 : Loss 2981.0811\n",
      "Iteration 29730 : Loss 2981.0104\n",
      "Iteration 29740 : Loss 2980.9397\n",
      "Iteration 29750 : Loss 2980.8692\n",
      "Iteration 29760 : Loss 2980.7987\n",
      "Iteration 29770 : Loss 2980.7283\n",
      "Iteration 29780 : Loss 2980.6579\n",
      "Iteration 29790 : Loss 2980.5876\n",
      "Iteration 29800 : Loss 2980.5174\n",
      "Iteration 29810 : Loss 2980.4472\n",
      "Iteration 29820 : Loss 2980.3771\n",
      "Iteration 29830 : Loss 2980.3071\n",
      "Iteration 29840 : Loss 2980.2371\n",
      "Iteration 29850 : Loss 2980.1672\n",
      "Iteration 29860 : Loss 2980.0974\n",
      "Iteration 29870 : Loss 2980.0276\n",
      "Iteration 29880 : Loss 2979.9579\n",
      "Iteration 29890 : Loss 2979.8882\n",
      "Iteration 29900 : Loss 2979.8186\n",
      "Iteration 29910 : Loss 2979.7491\n",
      "Iteration 29920 : Loss 2979.6797\n",
      "Iteration 29930 : Loss 2979.6103\n",
      "Iteration 29940 : Loss 2979.5409\n",
      "Iteration 29950 : Loss 2979.4717\n",
      "Iteration 29960 : Loss 2979.4025\n",
      "Iteration 29970 : Loss 2979.3333\n",
      "Iteration 29980 : Loss 2979.2643\n",
      "Iteration 29990 : Loss 2979.1953\n",
      "Iteration 30000 : Loss 2979.1263\n",
      "Iteration 30010 : Loss 2979.0574\n",
      "Iteration 30020 : Loss 2978.9886\n",
      "Iteration 30030 : Loss 2978.9199\n",
      "Iteration 30040 : Loss 2978.8512\n",
      "Iteration 30050 : Loss 2978.7825\n",
      "Iteration 30060 : Loss 2978.7140\n",
      "Iteration 30070 : Loss 2978.6455\n",
      "Iteration 30080 : Loss 2978.5770\n",
      "Iteration 30090 : Loss 2978.5087\n",
      "Iteration 30100 : Loss 2978.4404\n",
      "Iteration 30110 : Loss 2978.3721\n",
      "Iteration 30120 : Loss 2978.3039\n",
      "Iteration 30130 : Loss 2978.2358\n",
      "Iteration 30140 : Loss 2978.1677\n",
      "Iteration 30150 : Loss 2978.0998\n",
      "Iteration 30160 : Loss 2978.0318\n",
      "Iteration 30170 : Loss 2977.9639\n",
      "Iteration 30180 : Loss 2977.8961\n",
      "Iteration 30190 : Loss 2977.8284\n",
      "Iteration 30200 : Loss 2977.7607\n",
      "Iteration 30210 : Loss 2977.6931\n",
      "Iteration 30220 : Loss 2977.6255\n",
      "Iteration 30230 : Loss 2977.5580\n",
      "Iteration 30240 : Loss 2977.4906\n",
      "Iteration 30250 : Loss 2977.4232\n",
      "Iteration 30260 : Loss 2977.3559\n",
      "Iteration 30270 : Loss 2977.2887\n",
      "Iteration 30280 : Loss 2977.2215\n",
      "Iteration 30290 : Loss 2977.1544\n",
      "Iteration 30300 : Loss 2977.0873\n",
      "Iteration 30310 : Loss 2977.0203\n",
      "Iteration 30320 : Loss 2976.9533\n",
      "Iteration 30330 : Loss 2976.8865\n",
      "Iteration 30340 : Loss 2976.8197\n",
      "Iteration 30350 : Loss 2976.7529\n",
      "Iteration 30360 : Loss 2976.6862\n",
      "Iteration 30370 : Loss 2976.6196\n",
      "Iteration 30380 : Loss 2976.5530\n",
      "Iteration 30390 : Loss 2976.4865\n",
      "Iteration 30400 : Loss 2976.4200\n",
      "Iteration 30410 : Loss 2976.3537\n",
      "Iteration 30420 : Loss 2976.2873\n",
      "Iteration 30430 : Loss 2976.2211\n",
      "Iteration 30440 : Loss 2976.1549\n",
      "Iteration 30450 : Loss 2976.0887\n",
      "Iteration 30460 : Loss 2976.0226\n",
      "Iteration 30470 : Loss 2975.9566\n",
      "Iteration 30480 : Loss 2975.8907\n",
      "Iteration 30490 : Loss 2975.8248\n",
      "Iteration 30500 : Loss 2975.7589\n",
      "Iteration 30510 : Loss 2975.6931\n",
      "Iteration 30520 : Loss 2975.6274\n",
      "Iteration 30530 : Loss 2975.5618\n",
      "Iteration 30540 : Loss 2975.4962\n",
      "Iteration 30550 : Loss 2975.4306\n",
      "Iteration 30560 : Loss 2975.3652\n",
      "Iteration 30570 : Loss 2975.2997\n",
      "Iteration 30580 : Loss 2975.2344\n",
      "Iteration 30590 : Loss 2975.1691\n",
      "Iteration 30600 : Loss 2975.1039\n",
      "Iteration 30610 : Loss 2975.0387\n",
      "Iteration 30620 : Loss 2974.9736\n",
      "Iteration 30630 : Loss 2974.9085\n",
      "Iteration 30640 : Loss 2974.8435\n",
      "Iteration 30650 : Loss 2974.7786\n",
      "Iteration 30660 : Loss 2974.7137\n",
      "Iteration 30670 : Loss 2974.6489\n",
      "Iteration 30680 : Loss 2974.5841\n",
      "Iteration 30690 : Loss 2974.5194\n",
      "Iteration 30700 : Loss 2974.4548\n",
      "Iteration 30710 : Loss 2974.3902\n",
      "Iteration 30720 : Loss 2974.3257\n",
      "Iteration 30730 : Loss 2974.2612\n",
      "Iteration 30740 : Loss 2974.1968\n",
      "Iteration 30750 : Loss 2974.1325\n",
      "Iteration 30760 : Loss 2974.0682\n",
      "Iteration 30770 : Loss 2974.0040\n",
      "Iteration 30780 : Loss 2973.9398\n",
      "Iteration 30790 : Loss 2973.8757\n",
      "Iteration 30800 : Loss 2973.8116\n",
      "Iteration 30810 : Loss 2973.7476\n",
      "Iteration 30820 : Loss 2973.6837\n",
      "Iteration 30830 : Loss 2973.6198\n",
      "Iteration 30840 : Loss 2973.5560\n",
      "Iteration 30850 : Loss 2973.4923\n",
      "Iteration 30860 : Loss 2973.4286\n",
      "Iteration 30870 : Loss 2973.3649\n",
      "Iteration 30880 : Loss 2973.3014\n",
      "Iteration 30890 : Loss 2973.2378\n",
      "Iteration 30900 : Loss 2973.1744\n",
      "Iteration 30910 : Loss 2973.1110\n",
      "Iteration 30920 : Loss 2973.0476\n",
      "Iteration 30930 : Loss 2972.9843\n",
      "Iteration 30940 : Loss 2972.9211\n",
      "Iteration 30950 : Loss 2972.8579\n",
      "Iteration 30960 : Loss 2972.7948\n",
      "Iteration 30970 : Loss 2972.7318\n",
      "Iteration 30980 : Loss 2972.6688\n",
      "Iteration 30990 : Loss 2972.6058\n",
      "Iteration 31000 : Loss 2972.5429\n",
      "Iteration 31010 : Loss 2972.4801\n",
      "Iteration 31020 : Loss 2972.4173\n",
      "Iteration 31030 : Loss 2972.3546\n",
      "Iteration 31040 : Loss 2972.2920\n",
      "Iteration 31050 : Loss 2972.2294\n",
      "Iteration 31060 : Loss 2972.1668\n",
      "Iteration 31070 : Loss 2972.1044\n",
      "Iteration 31080 : Loss 2972.0419\n",
      "Iteration 31090 : Loss 2971.9796\n",
      "Iteration 31100 : Loss 2971.9173\n",
      "Iteration 31110 : Loss 2971.8550\n",
      "Iteration 31120 : Loss 2971.7928\n",
      "Iteration 31130 : Loss 2971.7307\n",
      "Iteration 31140 : Loss 2971.6686\n",
      "Iteration 31150 : Loss 2971.6066\n",
      "Iteration 31160 : Loss 2971.5446\n",
      "Iteration 31170 : Loss 2971.4827\n",
      "Iteration 31180 : Loss 2971.4208\n",
      "Iteration 31190 : Loss 2971.3590\n",
      "Iteration 31200 : Loss 2971.2973\n",
      "Iteration 31210 : Loss 2971.2356\n",
      "Iteration 31220 : Loss 2971.1740\n",
      "Iteration 31230 : Loss 2971.1124\n",
      "Iteration 31240 : Loss 2971.0509\n",
      "Iteration 31250 : Loss 2970.9894\n",
      "Iteration 31260 : Loss 2970.9280\n",
      "Iteration 31270 : Loss 2970.8667\n",
      "Iteration 31280 : Loss 2970.8054\n",
      "Iteration 31290 : Loss 2970.7442\n",
      "Iteration 31300 : Loss 2970.6830\n",
      "Iteration 31310 : Loss 2970.6218\n",
      "Iteration 31320 : Loss 2970.5608\n",
      "Iteration 31330 : Loss 2970.4998\n",
      "Iteration 31340 : Loss 2970.4388\n",
      "Iteration 31350 : Loss 2970.3779\n",
      "Iteration 31360 : Loss 2970.3171\n",
      "Iteration 31370 : Loss 2970.2563\n",
      "Iteration 31380 : Loss 2970.1955\n",
      "Iteration 31390 : Loss 2970.1349\n",
      "Iteration 31400 : Loss 2970.0742\n",
      "Iteration 31410 : Loss 2970.0137\n",
      "Iteration 31420 : Loss 2969.9532\n",
      "Iteration 31430 : Loss 2969.8927\n",
      "Iteration 31440 : Loss 2969.8323\n",
      "Iteration 31450 : Loss 2969.7720\n",
      "Iteration 31460 : Loss 2969.7117\n",
      "Iteration 31470 : Loss 2969.6514\n",
      "Iteration 31480 : Loss 2969.5913\n",
      "Iteration 31490 : Loss 2969.5311\n",
      "Iteration 31500 : Loss 2969.4711\n",
      "Iteration 31510 : Loss 2969.4110\n",
      "Iteration 31520 : Loss 2969.3511\n",
      "Iteration 31530 : Loss 2969.2912\n",
      "Iteration 31540 : Loss 2969.2313\n",
      "Iteration 31550 : Loss 2969.1715\n",
      "Iteration 31560 : Loss 2969.1118\n",
      "Iteration 31570 : Loss 2969.0521\n",
      "Iteration 31580 : Loss 2968.9925\n",
      "Iteration 31590 : Loss 2968.9329\n",
      "Iteration 31600 : Loss 2968.8734\n",
      "Iteration 31610 : Loss 2968.8139\n",
      "Iteration 31620 : Loss 2968.7545\n",
      "Iteration 31630 : Loss 2968.6951\n",
      "Iteration 31640 : Loss 2968.6358\n",
      "Iteration 31650 : Loss 2968.5766\n",
      "Iteration 31660 : Loss 2968.5174\n",
      "Iteration 31670 : Loss 2968.4582\n",
      "Iteration 31680 : Loss 2968.3991\n",
      "Iteration 31690 : Loss 2968.3401\n",
      "Iteration 31700 : Loss 2968.2811\n",
      "Iteration 31710 : Loss 2968.2222\n",
      "Iteration 31720 : Loss 2968.1633\n",
      "Iteration 31730 : Loss 2968.1045\n",
      "Iteration 31740 : Loss 2968.0457\n",
      "Iteration 31750 : Loss 2967.9870\n",
      "Iteration 31760 : Loss 2967.9283\n",
      "Iteration 31770 : Loss 2967.8697\n",
      "Iteration 31780 : Loss 2967.8112\n",
      "Iteration 31790 : Loss 2967.7527\n",
      "Iteration 31800 : Loss 2967.6942\n",
      "Iteration 31810 : Loss 2967.6358\n",
      "Iteration 31820 : Loss 2967.5775\n",
      "Iteration 31830 : Loss 2967.5192\n",
      "Iteration 31840 : Loss 2967.4610\n",
      "Iteration 31850 : Loss 2967.4028\n",
      "Iteration 31860 : Loss 2967.3446\n",
      "Iteration 31870 : Loss 2967.2866\n",
      "Iteration 31880 : Loss 2967.2285\n",
      "Iteration 31890 : Loss 2967.1706\n",
      "Iteration 31900 : Loss 2967.1127\n",
      "Iteration 31910 : Loss 2967.0548\n",
      "Iteration 31920 : Loss 2966.9970\n",
      "Iteration 31930 : Loss 2966.9392\n",
      "Iteration 31940 : Loss 2966.8815\n",
      "Iteration 31950 : Loss 2966.8239\n",
      "Iteration 31960 : Loss 2966.7663\n",
      "Iteration 31970 : Loss 2966.7087\n",
      "Iteration 31980 : Loss 2966.6512\n",
      "Iteration 31990 : Loss 2966.5938\n",
      "Iteration 32000 : Loss 2966.5364\n",
      "Iteration 32010 : Loss 2966.4790\n",
      "Iteration 32020 : Loss 2966.4217\n",
      "Iteration 32030 : Loss 2966.3645\n",
      "Iteration 32040 : Loss 2966.3073\n",
      "Iteration 32050 : Loss 2966.2502\n",
      "Iteration 32060 : Loss 2966.1931\n",
      "Iteration 32070 : Loss 2966.1361\n",
      "Iteration 32080 : Loss 2966.0791\n",
      "Iteration 32090 : Loss 2966.0222\n",
      "Iteration 32100 : Loss 2965.9653\n",
      "Iteration 32110 : Loss 2965.9085\n",
      "Iteration 32120 : Loss 2965.8517\n",
      "Iteration 32130 : Loss 2965.7950\n",
      "Iteration 32140 : Loss 2965.7383\n",
      "Iteration 32150 : Loss 2965.6817\n",
      "Iteration 32160 : Loss 2965.6252\n",
      "Iteration 32170 : Loss 2965.5686\n",
      "Iteration 32180 : Loss 2965.5122\n",
      "Iteration 32190 : Loss 2965.4558\n",
      "Iteration 32200 : Loss 2965.3994\n",
      "Iteration 32210 : Loss 2965.3431\n",
      "Iteration 32220 : Loss 2965.2869\n",
      "Iteration 32230 : Loss 2965.2307\n",
      "Iteration 32240 : Loss 2965.1745\n",
      "Iteration 32250 : Loss 2965.1184\n",
      "Iteration 32260 : Loss 2965.0623\n",
      "Iteration 32270 : Loss 2965.0063\n",
      "Iteration 32280 : Loss 2964.9504\n",
      "Iteration 32290 : Loss 2964.8945\n",
      "Iteration 32300 : Loss 2964.8387\n",
      "Iteration 32310 : Loss 2964.7829\n",
      "Iteration 32320 : Loss 2964.7271\n",
      "Iteration 32330 : Loss 2964.6714\n",
      "Iteration 32340 : Loss 2964.6158\n",
      "Iteration 32350 : Loss 2964.5602\n",
      "Iteration 32360 : Loss 2964.5046\n",
      "Iteration 32370 : Loss 2964.4491\n",
      "Iteration 32380 : Loss 2964.3937\n",
      "Iteration 32390 : Loss 2964.3383\n",
      "Iteration 32400 : Loss 2964.2830\n",
      "Iteration 32410 : Loss 2964.2277\n",
      "Iteration 32420 : Loss 2964.1724\n",
      "Iteration 32430 : Loss 2964.1172\n",
      "Iteration 32440 : Loss 2964.0621\n",
      "Iteration 32450 : Loss 2964.0070\n",
      "Iteration 32460 : Loss 2963.9520\n",
      "Iteration 32470 : Loss 2963.8970\n",
      "Iteration 32480 : Loss 2963.8420\n",
      "Iteration 32490 : Loss 2963.7871\n",
      "Iteration 32500 : Loss 2963.7323\n",
      "Iteration 32510 : Loss 2963.6775\n",
      "Iteration 32520 : Loss 2963.6228\n",
      "Iteration 32530 : Loss 2963.5681\n",
      "Iteration 32540 : Loss 2963.5134\n",
      "Iteration 32550 : Loss 2963.4588\n",
      "Iteration 32560 : Loss 2963.4043\n",
      "Iteration 32570 : Loss 2963.3498\n",
      "Iteration 32580 : Loss 2963.2953\n",
      "Iteration 32590 : Loss 2963.2409\n",
      "Iteration 32600 : Loss 2963.1866\n",
      "Iteration 32610 : Loss 2963.1323\n",
      "Iteration 32620 : Loss 2963.0780\n",
      "Iteration 32630 : Loss 2963.0238\n",
      "Iteration 32640 : Loss 2962.9697\n",
      "Iteration 32650 : Loss 2962.9156\n",
      "Iteration 32660 : Loss 2962.8615\n",
      "Iteration 32670 : Loss 2962.8075\n",
      "Iteration 32680 : Loss 2962.7536\n",
      "Iteration 32690 : Loss 2962.6997\n",
      "Iteration 32700 : Loss 2962.6458\n",
      "Iteration 32710 : Loss 2962.5920\n",
      "Iteration 32720 : Loss 2962.5383\n",
      "Iteration 32730 : Loss 2962.4845\n",
      "Iteration 32740 : Loss 2962.4309\n",
      "Iteration 32750 : Loss 2962.3773\n",
      "Iteration 32760 : Loss 2962.3237\n",
      "Iteration 32770 : Loss 2962.2702\n",
      "Iteration 32780 : Loss 2962.2167\n",
      "Iteration 32790 : Loss 2962.1633\n",
      "Iteration 32800 : Loss 2962.1099\n",
      "Iteration 32810 : Loss 2962.0566\n",
      "Iteration 32820 : Loss 2962.0033\n",
      "Iteration 32830 : Loss 2961.9501\n",
      "Iteration 32840 : Loss 2961.8969\n",
      "Iteration 32850 : Loss 2961.8438\n",
      "Iteration 32860 : Loss 2961.7907\n",
      "Iteration 32870 : Loss 2961.7377\n",
      "Iteration 32880 : Loss 2961.6847\n",
      "Iteration 32890 : Loss 2961.6318\n",
      "Iteration 32900 : Loss 2961.5789\n",
      "Iteration 32910 : Loss 2961.5260\n",
      "Iteration 32920 : Loss 2961.4732\n",
      "Iteration 32930 : Loss 2961.4205\n",
      "Iteration 32940 : Loss 2961.3678\n",
      "Iteration 32950 : Loss 2961.3151\n",
      "Iteration 32960 : Loss 2961.2625\n",
      "Iteration 32970 : Loss 2961.2100\n",
      "Iteration 32980 : Loss 2961.1575\n",
      "Iteration 32990 : Loss 2961.1050\n",
      "Iteration 33000 : Loss 2961.0526\n",
      "Iteration 33010 : Loss 2961.0002\n",
      "Iteration 33020 : Loss 2960.9479\n",
      "Iteration 33030 : Loss 2960.8956\n",
      "Iteration 33040 : Loss 2960.8434\n",
      "Iteration 33050 : Loss 2960.7912\n",
      "Iteration 33060 : Loss 2960.7391\n",
      "Iteration 33070 : Loss 2960.6870\n",
      "Iteration 33080 : Loss 2960.6350\n",
      "Iteration 33090 : Loss 2960.5830\n",
      "Iteration 33100 : Loss 2960.5311\n",
      "Iteration 33110 : Loss 2960.4792\n",
      "Iteration 33120 : Loss 2960.4273\n",
      "Iteration 33130 : Loss 2960.3755\n",
      "Iteration 33140 : Loss 2960.3238\n",
      "Iteration 33150 : Loss 2960.2720\n",
      "Iteration 33160 : Loss 2960.2204\n",
      "Iteration 33170 : Loss 2960.1688\n",
      "Iteration 33180 : Loss 2960.1172\n",
      "Iteration 33190 : Loss 2960.0657\n",
      "Iteration 33200 : Loss 2960.0142\n",
      "Iteration 33210 : Loss 2959.9628\n",
      "Iteration 33220 : Loss 2959.9114\n",
      "Iteration 33230 : Loss 2959.8601\n",
      "Iteration 33240 : Loss 2959.8088\n",
      "Iteration 33250 : Loss 2959.7575\n",
      "Iteration 33260 : Loss 2959.7063\n",
      "Iteration 33270 : Loss 2959.6552\n",
      "Iteration 33280 : Loss 2959.6041\n",
      "Iteration 33290 : Loss 2959.5530\n",
      "Iteration 33300 : Loss 2959.5020\n",
      "Iteration 33310 : Loss 2959.4510\n",
      "Iteration 33320 : Loss 2959.4001\n",
      "Iteration 33330 : Loss 2959.3492\n",
      "Iteration 33340 : Loss 2959.2984\n",
      "Iteration 33350 : Loss 2959.2476\n",
      "Iteration 33360 : Loss 2959.1969\n",
      "Iteration 33370 : Loss 2959.1462\n",
      "Iteration 33380 : Loss 2959.0956\n",
      "Iteration 33390 : Loss 2959.0450\n",
      "Iteration 33400 : Loss 2958.9944\n",
      "Iteration 33410 : Loss 2958.9439\n",
      "Iteration 33420 : Loss 2958.8934\n",
      "Iteration 33430 : Loss 2958.8430\n",
      "Iteration 33440 : Loss 2958.7927\n",
      "Iteration 33450 : Loss 2958.7423\n",
      "Iteration 33460 : Loss 2958.6920\n",
      "Iteration 33470 : Loss 2958.6418\n",
      "Iteration 33480 : Loss 2958.5916\n",
      "Iteration 33490 : Loss 2958.5415\n",
      "Iteration 33500 : Loss 2958.4914\n",
      "Iteration 33510 : Loss 2958.4413\n",
      "Iteration 33520 : Loss 2958.3913\n",
      "Iteration 33530 : Loss 2958.3413\n",
      "Iteration 33540 : Loss 2958.2914\n",
      "Iteration 33550 : Loss 2958.2415\n",
      "Iteration 33560 : Loss 2958.1917\n",
      "Iteration 33570 : Loss 2958.1419\n",
      "Iteration 33580 : Loss 2958.0922\n",
      "Iteration 33590 : Loss 2958.0425\n",
      "Iteration 33600 : Loss 2957.9928\n",
      "Iteration 33610 : Loss 2957.9432\n",
      "Iteration 33620 : Loss 2957.8937\n",
      "Iteration 33630 : Loss 2957.8442\n",
      "Iteration 33640 : Loss 2957.7947\n",
      "Iteration 33650 : Loss 2957.7453\n",
      "Iteration 33660 : Loss 2957.6959\n",
      "Iteration 33670 : Loss 2957.6465\n",
      "Iteration 33680 : Loss 2957.5972\n",
      "Iteration 33690 : Loss 2957.5480\n",
      "Iteration 33700 : Loss 2957.4988\n",
      "Iteration 33710 : Loss 2957.4496\n",
      "Iteration 33720 : Loss 2957.4005\n",
      "Iteration 33730 : Loss 2957.3514\n",
      "Iteration 33740 : Loss 2957.3024\n",
      "Iteration 33750 : Loss 2957.2534\n",
      "Iteration 33760 : Loss 2957.2045\n",
      "Iteration 33770 : Loss 2957.1556\n",
      "Iteration 33780 : Loss 2957.1067\n",
      "Iteration 33790 : Loss 2957.0579\n",
      "Iteration 33800 : Loss 2957.0092\n",
      "Iteration 33810 : Loss 2956.9604\n",
      "Iteration 33820 : Loss 2956.9118\n",
      "Iteration 33830 : Loss 2956.8631\n",
      "Iteration 33840 : Loss 2956.8145\n",
      "Iteration 33850 : Loss 2956.7660\n",
      "Iteration 33860 : Loss 2956.7175\n",
      "Iteration 33870 : Loss 2956.6690\n",
      "Iteration 33880 : Loss 2956.6206\n",
      "Iteration 33890 : Loss 2956.5722\n",
      "Iteration 33900 : Loss 2956.5239\n",
      "Iteration 33910 : Loss 2956.4756\n",
      "Iteration 33920 : Loss 2956.4274\n",
      "Iteration 33930 : Loss 2956.3792\n",
      "Iteration 33940 : Loss 2956.3310\n",
      "Iteration 33950 : Loss 2956.2829\n",
      "Iteration 33960 : Loss 2956.2349\n",
      "Iteration 33970 : Loss 2956.1868\n",
      "Iteration 33980 : Loss 2956.1389\n",
      "Iteration 33990 : Loss 2956.0909\n",
      "Iteration 34000 : Loss 2956.0430\n",
      "Iteration 34010 : Loss 2955.9952\n",
      "Iteration 34020 : Loss 2955.9474\n",
      "Iteration 34030 : Loss 2955.8996\n",
      "Iteration 34040 : Loss 2955.8519\n",
      "Iteration 34050 : Loss 2955.8042\n",
      "Iteration 34060 : Loss 2955.7565\n",
      "Iteration 34070 : Loss 2955.7089\n",
      "Iteration 34080 : Loss 2955.6614\n",
      "Iteration 34090 : Loss 2955.6139\n",
      "Iteration 34100 : Loss 2955.5664\n",
      "Iteration 34110 : Loss 2955.5190\n",
      "Iteration 34120 : Loss 2955.4716\n",
      "Iteration 34130 : Loss 2955.4243\n",
      "Iteration 34140 : Loss 2955.3770\n",
      "Iteration 34150 : Loss 2955.3297\n",
      "Iteration 34160 : Loss 2955.2825\n",
      "Iteration 34170 : Loss 2955.2353\n",
      "Iteration 34180 : Loss 2955.1882\n",
      "Iteration 34190 : Loss 2955.1411\n",
      "Iteration 34200 : Loss 2955.0941\n",
      "Iteration 34210 : Loss 2955.0471\n",
      "Iteration 34220 : Loss 2955.0001\n",
      "Iteration 34230 : Loss 2954.9532\n",
      "Iteration 34240 : Loss 2954.9063\n",
      "Iteration 34250 : Loss 2954.8595\n",
      "Iteration 34260 : Loss 2954.8127\n",
      "Iteration 34270 : Loss 2954.7659\n",
      "Iteration 34280 : Loss 2954.7192\n",
      "Iteration 34290 : Loss 2954.6726\n",
      "Iteration 34300 : Loss 2954.6260\n",
      "Iteration 34310 : Loss 2954.5794\n",
      "Iteration 34320 : Loss 2954.5328\n",
      "Iteration 34330 : Loss 2954.4863\n",
      "Iteration 34340 : Loss 2954.4399\n",
      "Iteration 34350 : Loss 2954.3935\n",
      "Iteration 34360 : Loss 2954.3471\n",
      "Iteration 34370 : Loss 2954.3008\n",
      "Iteration 34380 : Loss 2954.2545\n",
      "Iteration 34390 : Loss 2954.2082\n",
      "Iteration 34400 : Loss 2954.1620\n",
      "Iteration 34410 : Loss 2954.1158\n",
      "Iteration 34420 : Loss 2954.0697\n",
      "Iteration 34430 : Loss 2954.0236\n",
      "Iteration 34440 : Loss 2953.9776\n",
      "Iteration 34450 : Loss 2953.9316\n",
      "Iteration 34460 : Loss 2953.8856\n",
      "Iteration 34470 : Loss 2953.8397\n",
      "Iteration 34480 : Loss 2953.7938\n",
      "Iteration 34490 : Loss 2953.7480\n",
      "Iteration 34500 : Loss 2953.7022\n",
      "Iteration 34510 : Loss 2953.6564\n",
      "Iteration 34520 : Loss 2953.6107\n",
      "Iteration 34530 : Loss 2953.5651\n",
      "Iteration 34540 : Loss 2953.5194\n",
      "Iteration 34550 : Loss 2953.4738\n",
      "Iteration 34560 : Loss 2953.4283\n",
      "Iteration 34570 : Loss 2953.3828\n",
      "Iteration 34580 : Loss 2953.3373\n",
      "Iteration 34590 : Loss 2953.2919\n",
      "Iteration 34600 : Loss 2953.2465\n",
      "Iteration 34610 : Loss 2953.2011\n",
      "Iteration 34620 : Loss 2953.1558\n",
      "Iteration 34630 : Loss 2953.1106\n",
      "Iteration 34640 : Loss 2953.0653\n",
      "Iteration 34650 : Loss 2953.0202\n",
      "Iteration 34660 : Loss 2952.9750\n",
      "Iteration 34670 : Loss 2952.9299\n",
      "Iteration 34680 : Loss 2952.8848\n",
      "Iteration 34690 : Loss 2952.8398\n",
      "Iteration 34700 : Loss 2952.7948\n",
      "Iteration 34710 : Loss 2952.7499\n",
      "Iteration 34720 : Loss 2952.7050\n",
      "Iteration 34730 : Loss 2952.6601\n",
      "Iteration 34740 : Loss 2952.6153\n",
      "Iteration 34750 : Loss 2952.5705\n",
      "Iteration 34760 : Loss 2952.5258\n",
      "Iteration 34770 : Loss 2952.4811\n",
      "Iteration 34780 : Loss 2952.4364\n",
      "Iteration 34790 : Loss 2952.3918\n",
      "Iteration 34800 : Loss 2952.3472\n",
      "Iteration 34810 : Loss 2952.3027\n",
      "Iteration 34820 : Loss 2952.2582\n",
      "Iteration 34830 : Loss 2952.2137\n",
      "Iteration 34840 : Loss 2952.1693\n",
      "Iteration 34850 : Loss 2952.1249\n",
      "Iteration 34860 : Loss 2952.0805\n",
      "Iteration 34870 : Loss 2952.0362\n",
      "Iteration 34880 : Loss 2951.9920\n",
      "Iteration 34890 : Loss 2951.9478\n",
      "Iteration 34900 : Loss 2951.9036\n",
      "Iteration 34910 : Loss 2951.8594\n",
      "Iteration 34920 : Loss 2951.8153\n",
      "Iteration 34930 : Loss 2951.7712\n",
      "Iteration 34940 : Loss 2951.7272\n",
      "Iteration 34950 : Loss 2951.6832\n",
      "Iteration 34960 : Loss 2951.6393\n",
      "Iteration 34970 : Loss 2951.5954\n",
      "Iteration 34980 : Loss 2951.5515\n",
      "Iteration 34990 : Loss 2951.5077\n",
      "Iteration 35000 : Loss 2951.4639\n",
      "Iteration 35010 : Loss 2951.4201\n",
      "Iteration 35020 : Loss 2951.3764\n",
      "Iteration 35030 : Loss 2951.3327\n",
      "Iteration 35040 : Loss 2951.2891\n",
      "Iteration 35050 : Loss 2951.2455\n",
      "Iteration 35060 : Loss 2951.2019\n",
      "Iteration 35070 : Loss 2951.1584\n",
      "Iteration 35080 : Loss 2951.1149\n",
      "Iteration 35090 : Loss 2951.0715\n",
      "Iteration 35100 : Loss 2951.0281\n",
      "Iteration 35110 : Loss 2950.9847\n",
      "Iteration 35120 : Loss 2950.9414\n",
      "Iteration 35130 : Loss 2950.8981\n",
      "Iteration 35140 : Loss 2950.8548\n",
      "Iteration 35150 : Loss 2950.8116\n",
      "Iteration 35160 : Loss 2950.7684\n",
      "Iteration 35170 : Loss 2950.7253\n",
      "Iteration 35180 : Loss 2950.6822\n",
      "Iteration 35190 : Loss 2950.6392\n",
      "Iteration 35200 : Loss 2950.5961\n",
      "Iteration 35210 : Loss 2950.5532\n",
      "Iteration 35220 : Loss 2950.5102\n",
      "Iteration 35230 : Loss 2950.4673\n",
      "Iteration 35240 : Loss 2950.4244\n",
      "Iteration 35250 : Loss 2950.3816\n",
      "Iteration 35260 : Loss 2950.3388\n",
      "Iteration 35270 : Loss 2950.2961\n",
      "Iteration 35280 : Loss 2950.2534\n",
      "Iteration 35290 : Loss 2950.2107\n",
      "Iteration 35300 : Loss 2950.1680\n",
      "Iteration 35310 : Loss 2950.1254\n",
      "Iteration 35320 : Loss 2950.0829\n",
      "Iteration 35330 : Loss 2950.0404\n",
      "Iteration 35340 : Loss 2949.9979\n",
      "Iteration 35350 : Loss 2949.9554\n",
      "Iteration 35360 : Loss 2949.9130\n",
      "Iteration 35370 : Loss 2949.8706\n",
      "Iteration 35380 : Loss 2949.8283\n",
      "Iteration 35390 : Loss 2949.7860\n",
      "Iteration 35400 : Loss 2949.7437\n",
      "Iteration 35410 : Loss 2949.7015\n",
      "Iteration 35420 : Loss 2949.6593\n",
      "Iteration 35430 : Loss 2949.6172\n",
      "Iteration 35440 : Loss 2949.5751\n",
      "Iteration 35450 : Loss 2949.5330\n",
      "Iteration 35460 : Loss 2949.4910\n",
      "Iteration 35470 : Loss 2949.4490\n",
      "Iteration 35480 : Loss 2949.4070\n",
      "Iteration 35490 : Loss 2949.3651\n",
      "Iteration 35500 : Loss 2949.3232\n",
      "Iteration 35510 : Loss 2949.2813\n",
      "Iteration 35520 : Loss 2949.2395\n",
      "Iteration 35530 : Loss 2949.1978\n",
      "Iteration 35540 : Loss 2949.1560\n",
      "Iteration 35550 : Loss 2949.1143\n",
      "Iteration 35560 : Loss 2949.0727\n",
      "Iteration 35570 : Loss 2949.0310\n",
      "Iteration 35580 : Loss 2948.9894\n",
      "Iteration 35590 : Loss 2948.9479\n",
      "Iteration 35600 : Loss 2948.9064\n",
      "Iteration 35610 : Loss 2948.8649\n",
      "Iteration 35620 : Loss 2948.8235\n",
      "Iteration 35630 : Loss 2948.7821\n",
      "Iteration 35640 : Loss 2948.7407\n",
      "Iteration 35650 : Loss 2948.6994\n",
      "Iteration 35660 : Loss 2948.6581\n",
      "Iteration 35670 : Loss 2948.6168\n",
      "Iteration 35680 : Loss 2948.5756\n",
      "Iteration 35690 : Loss 2948.5344\n",
      "Iteration 35700 : Loss 2948.4932\n",
      "Iteration 35710 : Loss 2948.4521\n",
      "Iteration 35720 : Loss 2948.4111\n",
      "Iteration 35730 : Loss 2948.3700\n",
      "Iteration 35740 : Loss 2948.3290\n",
      "Iteration 35750 : Loss 2948.2881\n",
      "Iteration 35760 : Loss 2948.2471\n",
      "Iteration 35770 : Loss 2948.2062\n",
      "Iteration 35780 : Loss 2948.1654\n",
      "Iteration 35790 : Loss 2948.1246\n",
      "Iteration 35800 : Loss 2948.0838\n",
      "Iteration 35810 : Loss 2948.0430\n",
      "Iteration 35820 : Loss 2948.0023\n",
      "Iteration 35830 : Loss 2947.9616\n",
      "Iteration 35840 : Loss 2947.9210\n",
      "Iteration 35850 : Loss 2947.8804\n",
      "Iteration 35860 : Loss 2947.8398\n",
      "Iteration 35870 : Loss 2947.7993\n",
      "Iteration 35880 : Loss 2947.7588\n",
      "Iteration 35890 : Loss 2947.7183\n",
      "Iteration 35900 : Loss 2947.6779\n",
      "Iteration 35910 : Loss 2947.6375\n",
      "Iteration 35920 : Loss 2947.5972\n",
      "Iteration 35930 : Loss 2947.5569\n",
      "Iteration 35940 : Loss 2947.5166\n",
      "Iteration 35950 : Loss 2947.4763\n",
      "Iteration 35960 : Loss 2947.4361\n",
      "Iteration 35970 : Loss 2947.3959\n",
      "Iteration 35980 : Loss 2947.3558\n",
      "Iteration 35990 : Loss 2947.3157\n",
      "Iteration 36000 : Loss 2947.2756\n",
      "Iteration 36010 : Loss 2947.2356\n",
      "Iteration 36020 : Loss 2947.1956\n",
      "Iteration 36030 : Loss 2947.1556\n",
      "Iteration 36040 : Loss 2947.1157\n",
      "Iteration 36050 : Loss 2947.0758\n",
      "Iteration 36060 : Loss 2947.0360\n",
      "Iteration 36070 : Loss 2946.9962\n",
      "Iteration 36080 : Loss 2946.9564\n",
      "Iteration 36090 : Loss 2946.9166\n",
      "Iteration 36100 : Loss 2946.8769\n",
      "Iteration 36110 : Loss 2946.8372\n",
      "Iteration 36120 : Loss 2946.7976\n",
      "Iteration 36130 : Loss 2946.7580\n",
      "Iteration 36140 : Loss 2946.7184\n",
      "Iteration 36150 : Loss 2946.6789\n",
      "Iteration 36160 : Loss 2946.6394\n",
      "Iteration 36170 : Loss 2946.5999\n",
      "Iteration 36180 : Loss 2946.5605\n",
      "Iteration 36190 : Loss 2946.5211\n",
      "Iteration 36200 : Loss 2946.4817\n",
      "Iteration 36210 : Loss 2946.4424\n",
      "Iteration 36220 : Loss 2946.4031\n",
      "Iteration 36230 : Loss 2946.3638\n",
      "Iteration 36240 : Loss 2946.3246\n",
      "Iteration 36250 : Loss 2946.2854\n",
      "Iteration 36260 : Loss 2946.2463\n",
      "Iteration 36270 : Loss 2946.2071\n",
      "Iteration 36280 : Loss 2946.1680\n",
      "Iteration 36290 : Loss 2946.1290\n",
      "Iteration 36300 : Loss 2946.0900\n",
      "Iteration 36310 : Loss 2946.0510\n",
      "Iteration 36320 : Loss 2946.0120\n",
      "Iteration 36330 : Loss 2945.9731\n",
      "Iteration 36340 : Loss 2945.9343\n",
      "Iteration 36350 : Loss 2945.8954\n",
      "Iteration 36360 : Loss 2945.8566\n",
      "Iteration 36370 : Loss 2945.8178\n",
      "Iteration 36380 : Loss 2945.7791\n",
      "Iteration 36390 : Loss 2945.7404\n",
      "Iteration 36400 : Loss 2945.7017\n",
      "Iteration 36410 : Loss 2945.6631\n",
      "Iteration 36420 : Loss 2945.6245\n",
      "Iteration 36430 : Loss 2945.5859\n",
      "Iteration 36440 : Loss 2945.5473\n",
      "Iteration 36450 : Loss 2945.5088\n",
      "Iteration 36460 : Loss 2945.4704\n",
      "Iteration 36470 : Loss 2945.4319\n",
      "Iteration 36480 : Loss 2945.3935\n",
      "Iteration 36490 : Loss 2945.3552\n",
      "Iteration 36500 : Loss 2945.3168\n",
      "Iteration 36510 : Loss 2945.2785\n",
      "Iteration 36520 : Loss 2945.2403\n",
      "Iteration 36530 : Loss 2945.2020\n",
      "Iteration 36540 : Loss 2945.1638\n",
      "Iteration 36550 : Loss 2945.1257\n",
      "Iteration 36560 : Loss 2945.0876\n",
      "Iteration 36570 : Loss 2945.0495\n",
      "Iteration 36580 : Loss 2945.0114\n",
      "Iteration 36590 : Loss 2944.9734\n",
      "Iteration 36600 : Loss 2944.9354\n",
      "Iteration 36610 : Loss 2944.8974\n",
      "Iteration 36620 : Loss 2944.8595\n",
      "Iteration 36630 : Loss 2944.8216\n",
      "Iteration 36640 : Loss 2944.7837\n",
      "Iteration 36650 : Loss 2944.7459\n",
      "Iteration 36660 : Loss 2944.7081\n",
      "Iteration 36670 : Loss 2944.6703\n",
      "Iteration 36680 : Loss 2944.6326\n",
      "Iteration 36690 : Loss 2944.5949\n",
      "Iteration 36700 : Loss 2944.5572\n",
      "Iteration 36710 : Loss 2944.5196\n",
      "Iteration 36720 : Loss 2944.4820\n",
      "Iteration 36730 : Loss 2944.4445\n",
      "Iteration 36740 : Loss 2944.4069\n",
      "Iteration 36750 : Loss 2944.3694\n",
      "Iteration 36760 : Loss 2944.3320\n",
      "Iteration 36770 : Loss 2944.2945\n",
      "Iteration 36780 : Loss 2944.2571\n",
      "Iteration 36790 : Loss 2944.2198\n",
      "Iteration 36800 : Loss 2944.1824\n",
      "Iteration 36810 : Loss 2944.1452\n",
      "Iteration 36820 : Loss 2944.1079\n",
      "Iteration 36830 : Loss 2944.0707\n",
      "Iteration 36840 : Loss 2944.0335\n",
      "Iteration 36850 : Loss 2943.9963\n",
      "Iteration 36860 : Loss 2943.9591\n",
      "Iteration 36870 : Loss 2943.9220\n",
      "Iteration 36880 : Loss 2943.8850\n",
      "Iteration 36890 : Loss 2943.8479\n",
      "Iteration 36900 : Loss 2943.8109\n",
      "Iteration 36910 : Loss 2943.7740\n",
      "Iteration 36920 : Loss 2943.7370\n",
      "Iteration 36930 : Loss 2943.7001\n",
      "Iteration 36940 : Loss 2943.6632\n",
      "Iteration 36950 : Loss 2943.6264\n",
      "Iteration 36960 : Loss 2943.5896\n",
      "Iteration 36970 : Loss 2943.5528\n",
      "Iteration 36980 : Loss 2943.5161\n",
      "Iteration 36990 : Loss 2943.4794\n",
      "Iteration 37000 : Loss 2943.4427\n",
      "Iteration 37010 : Loss 2943.4060\n",
      "Iteration 37020 : Loss 2943.3694\n",
      "Iteration 37030 : Loss 2943.3328\n",
      "Iteration 37040 : Loss 2943.2963\n",
      "Iteration 37050 : Loss 2943.2598\n",
      "Iteration 37060 : Loss 2943.2233\n",
      "Iteration 37070 : Loss 2943.1868\n",
      "Iteration 37080 : Loss 2943.1504\n",
      "Iteration 37090 : Loss 2943.1140\n",
      "Iteration 37100 : Loss 2943.0777\n",
      "Iteration 37110 : Loss 2943.0413\n",
      "Iteration 37120 : Loss 2943.0050\n",
      "Iteration 37130 : Loss 2942.9688\n",
      "Iteration 37140 : Loss 2942.9326\n",
      "Iteration 37150 : Loss 2942.8964\n",
      "Iteration 37160 : Loss 2942.8602\n",
      "Iteration 37170 : Loss 2942.8241\n",
      "Iteration 37180 : Loss 2942.7880\n",
      "Iteration 37190 : Loss 2942.7519\n",
      "Iteration 37200 : Loss 2942.7158\n",
      "Iteration 37210 : Loss 2942.6798\n",
      "Iteration 37220 : Loss 2942.6439\n",
      "Iteration 37230 : Loss 2942.6079\n",
      "Iteration 37240 : Loss 2942.5720\n",
      "Iteration 37250 : Loss 2942.5361\n",
      "Iteration 37260 : Loss 2942.5003\n",
      "Iteration 37270 : Loss 2942.4644\n",
      "Iteration 37280 : Loss 2942.4287\n",
      "Iteration 37290 : Loss 2942.3929\n",
      "Iteration 37300 : Loss 2942.3572\n",
      "Iteration 37310 : Loss 2942.3215\n",
      "Iteration 37320 : Loss 2942.2858\n",
      "Iteration 37330 : Loss 2942.2502\n",
      "Iteration 37340 : Loss 2942.2146\n",
      "Iteration 37350 : Loss 2942.1790\n",
      "Iteration 37360 : Loss 2942.1435\n",
      "Iteration 37370 : Loss 2942.1080\n",
      "Iteration 37380 : Loss 2942.0725\n",
      "Iteration 37390 : Loss 2942.0371\n",
      "Iteration 37400 : Loss 2942.0017\n",
      "Iteration 37410 : Loss 2941.9663\n",
      "Iteration 37420 : Loss 2941.9309\n",
      "Iteration 37430 : Loss 2941.8956\n",
      "Iteration 37440 : Loss 2941.8603\n",
      "Iteration 37450 : Loss 2941.8251\n",
      "Iteration 37460 : Loss 2941.7898\n",
      "Iteration 37470 : Loss 2941.7546\n",
      "Iteration 37480 : Loss 2941.7195\n",
      "Iteration 37490 : Loss 2941.6844\n",
      "Iteration 37500 : Loss 2941.6493\n",
      "Iteration 37510 : Loss 2941.6142\n",
      "Iteration 37520 : Loss 2941.5791\n",
      "Iteration 37530 : Loss 2941.5441\n",
      "Iteration 37540 : Loss 2941.5091\n",
      "Iteration 37550 : Loss 2941.4742\n",
      "Iteration 37560 : Loss 2941.4393\n",
      "Iteration 37570 : Loss 2941.4044\n",
      "Iteration 37580 : Loss 2941.3695\n",
      "Iteration 37590 : Loss 2941.3347\n",
      "Iteration 37600 : Loss 2941.2999\n",
      "Iteration 37610 : Loss 2941.2652\n",
      "Iteration 37620 : Loss 2941.2304\n",
      "Iteration 37630 : Loss 2941.1957\n",
      "Iteration 37640 : Loss 2941.1610\n",
      "Iteration 37650 : Loss 2941.1264\n",
      "Iteration 37660 : Loss 2941.0918\n",
      "Iteration 37670 : Loss 2941.0572\n",
      "Iteration 37680 : Loss 2941.0226\n",
      "Iteration 37690 : Loss 2940.9881\n",
      "Iteration 37700 : Loss 2940.9536\n",
      "Iteration 37710 : Loss 2940.9192\n",
      "Iteration 37720 : Loss 2940.8847\n",
      "Iteration 37730 : Loss 2940.8503\n",
      "Iteration 37740 : Loss 2940.8160\n",
      "Iteration 37750 : Loss 2940.7816\n",
      "Iteration 37760 : Loss 2940.7473\n",
      "Iteration 37770 : Loss 2940.7130\n",
      "Iteration 37780 : Loss 2940.6788\n",
      "Iteration 37790 : Loss 2940.6446\n",
      "Iteration 37800 : Loss 2940.6104\n",
      "Iteration 37810 : Loss 2940.5762\n",
      "Iteration 37820 : Loss 2940.5421\n",
      "Iteration 37830 : Loss 2940.5080\n",
      "Iteration 37840 : Loss 2940.4739\n",
      "Iteration 37850 : Loss 2940.4399\n",
      "Iteration 37860 : Loss 2940.4059\n",
      "Iteration 37870 : Loss 2940.3719\n",
      "Iteration 37880 : Loss 2940.3379\n",
      "Iteration 37890 : Loss 2940.3040\n",
      "Iteration 37900 : Loss 2940.2701\n",
      "Iteration 37910 : Loss 2940.2362\n",
      "Iteration 37920 : Loss 2940.2024\n",
      "Iteration 37930 : Loss 2940.1686\n",
      "Iteration 37940 : Loss 2940.1348\n",
      "Iteration 37950 : Loss 2940.1011\n",
      "Iteration 37960 : Loss 2940.0674\n",
      "Iteration 37970 : Loss 2940.0337\n",
      "Iteration 37980 : Loss 2940.0000\n",
      "Iteration 37990 : Loss 2939.9664\n",
      "Iteration 38000 : Loss 2939.9328\n",
      "Iteration 38010 : Loss 2939.8992\n",
      "Iteration 38020 : Loss 2939.8657\n",
      "Iteration 38030 : Loss 2939.8322\n",
      "Iteration 38040 : Loss 2939.7987\n",
      "Iteration 38050 : Loss 2939.7652\n",
      "Iteration 38060 : Loss 2939.7318\n",
      "Iteration 38070 : Loss 2939.6984\n",
      "Iteration 38080 : Loss 2939.6651\n",
      "Iteration 38090 : Loss 2939.6317\n",
      "Iteration 38100 : Loss 2939.5984\n",
      "Iteration 38110 : Loss 2939.5652\n",
      "Iteration 38120 : Loss 2939.5319\n",
      "Iteration 38130 : Loss 2939.4987\n",
      "Iteration 38140 : Loss 2939.4655\n",
      "Iteration 38150 : Loss 2939.4323\n",
      "Iteration 38160 : Loss 2939.3992\n",
      "Iteration 38170 : Loss 2939.3661\n",
      "Iteration 38180 : Loss 2939.3330\n",
      "Iteration 38190 : Loss 2939.3000\n",
      "Iteration 38200 : Loss 2939.2670\n",
      "Iteration 38210 : Loss 2939.2340\n",
      "Iteration 38220 : Loss 2939.2010\n",
      "Iteration 38230 : Loss 2939.1681\n",
      "Iteration 38240 : Loss 2939.1352\n",
      "Iteration 38250 : Loss 2939.1023\n",
      "Iteration 38260 : Loss 2939.0695\n",
      "Iteration 38270 : Loss 2939.0367\n",
      "Iteration 38280 : Loss 2939.0039\n",
      "Iteration 38290 : Loss 2938.9711\n",
      "Iteration 38300 : Loss 2938.9384\n",
      "Iteration 38310 : Loss 2938.9057\n",
      "Iteration 38320 : Loss 2938.8730\n",
      "Iteration 38330 : Loss 2938.8404\n",
      "Iteration 38340 : Loss 2938.8078\n",
      "Iteration 38350 : Loss 2938.7752\n",
      "Iteration 38360 : Loss 2938.7426\n",
      "Iteration 38370 : Loss 2938.7101\n",
      "Iteration 38380 : Loss 2938.6776\n",
      "Iteration 38390 : Loss 2938.6451\n",
      "Iteration 38400 : Loss 2938.6127\n",
      "Iteration 38410 : Loss 2938.5803\n",
      "Iteration 38420 : Loss 2938.5479\n",
      "Iteration 38430 : Loss 2938.5155\n",
      "Iteration 38440 : Loss 2938.4832\n",
      "Iteration 38450 : Loss 2938.4509\n",
      "Iteration 38460 : Loss 2938.4186\n",
      "Iteration 38470 : Loss 2938.3864\n",
      "Iteration 38480 : Loss 2938.3541\n",
      "Iteration 38490 : Loss 2938.3219\n",
      "Iteration 38500 : Loss 2938.2898\n",
      "Iteration 38510 : Loss 2938.2577\n",
      "Iteration 38520 : Loss 2938.2255\n",
      "Iteration 38530 : Loss 2938.1935\n",
      "Iteration 38540 : Loss 2938.1614\n",
      "Iteration 38550 : Loss 2938.1294\n",
      "Iteration 38560 : Loss 2938.0974\n",
      "Iteration 38570 : Loss 2938.0654\n",
      "Iteration 38580 : Loss 2938.0335\n",
      "Iteration 38590 : Loss 2938.0016\n",
      "Iteration 38600 : Loss 2937.9697\n",
      "Iteration 38610 : Loss 2937.9378\n",
      "Iteration 38620 : Loss 2937.9060\n",
      "Iteration 38630 : Loss 2937.8742\n",
      "Iteration 38640 : Loss 2937.8424\n",
      "Iteration 38650 : Loss 2937.8107\n",
      "Iteration 38660 : Loss 2937.7790\n",
      "Iteration 38670 : Loss 2937.7473\n",
      "Iteration 38680 : Loss 2937.7156\n",
      "Iteration 38690 : Loss 2937.6840\n",
      "Iteration 38700 : Loss 2937.6524\n",
      "Iteration 38710 : Loss 2937.6208\n",
      "Iteration 38720 : Loss 2937.5892\n",
      "Iteration 38730 : Loss 2937.5577\n",
      "Iteration 38740 : Loss 2937.5262\n",
      "Iteration 38750 : Loss 2937.4948\n",
      "Iteration 38760 : Loss 2937.4633\n",
      "Iteration 38770 : Loss 2937.4319\n",
      "Iteration 38780 : Loss 2937.4005\n",
      "Iteration 38790 : Loss 2937.3691\n",
      "Iteration 38800 : Loss 2937.3378\n",
      "Iteration 38810 : Loss 2937.3065\n",
      "Iteration 38820 : Loss 2937.2752\n",
      "Iteration 38830 : Loss 2937.2440\n",
      "Iteration 38840 : Loss 2937.2128\n",
      "Iteration 38850 : Loss 2937.1816\n",
      "Iteration 38860 : Loss 2937.1504\n",
      "Iteration 38870 : Loss 2937.1192\n",
      "Iteration 38880 : Loss 2937.0881\n",
      "Iteration 38890 : Loss 2937.0570\n",
      "Iteration 38900 : Loss 2937.0260\n",
      "Iteration 38910 : Loss 2936.9949\n",
      "Iteration 38920 : Loss 2936.9639\n",
      "Iteration 38930 : Loss 2936.9330\n",
      "Iteration 38940 : Loss 2936.9020\n",
      "Iteration 38950 : Loss 2936.8711\n",
      "Iteration 38960 : Loss 2936.8402\n",
      "Iteration 38970 : Loss 2936.8093\n",
      "Iteration 38980 : Loss 2936.7784\n",
      "Iteration 38990 : Loss 2936.7476\n",
      "Iteration 39000 : Loss 2936.7168\n",
      "Iteration 39010 : Loss 2936.6861\n",
      "Iteration 39020 : Loss 2936.6553\n",
      "Iteration 39030 : Loss 2936.6246\n",
      "Iteration 39040 : Loss 2936.5939\n",
      "Iteration 39050 : Loss 2936.5633\n",
      "Iteration 39060 : Loss 2936.5326\n",
      "Iteration 39070 : Loss 2936.5020\n",
      "Iteration 39080 : Loss 2936.4714\n",
      "Iteration 39090 : Loss 2936.4409\n",
      "Iteration 39100 : Loss 2936.4104\n",
      "Iteration 39110 : Loss 2936.3799\n",
      "Iteration 39120 : Loss 2936.3494\n",
      "Iteration 39130 : Loss 2936.3189\n",
      "Iteration 39140 : Loss 2936.2885\n",
      "Iteration 39150 : Loss 2936.2581\n",
      "Iteration 39160 : Loss 2936.2278\n",
      "Iteration 39170 : Loss 2936.1974\n",
      "Iteration 39180 : Loss 2936.1671\n",
      "Iteration 39190 : Loss 2936.1368\n",
      "Iteration 39200 : Loss 2936.1065\n",
      "Iteration 39210 : Loss 2936.0763\n",
      "Iteration 39220 : Loss 2936.0461\n",
      "Iteration 39230 : Loss 2936.0159\n",
      "Iteration 39240 : Loss 2935.9858\n",
      "Iteration 39250 : Loss 2935.9556\n",
      "Iteration 39260 : Loss 2935.9255\n",
      "Iteration 39270 : Loss 2935.8954\n",
      "Iteration 39280 : Loss 2935.8654\n",
      "Iteration 39290 : Loss 2935.8354\n",
      "Iteration 39300 : Loss 2935.8054\n",
      "Iteration 39310 : Loss 2935.7754\n",
      "Iteration 39320 : Loss 2935.7454\n",
      "Iteration 39330 : Loss 2935.7155\n",
      "Iteration 39340 : Loss 2935.6856\n",
      "Iteration 39350 : Loss 2935.6557\n",
      "Iteration 39360 : Loss 2935.6259\n",
      "Iteration 39370 : Loss 2935.5961\n",
      "Iteration 39380 : Loss 2935.5663\n",
      "Iteration 39390 : Loss 2935.5365\n",
      "Iteration 39400 : Loss 2935.5068\n",
      "Iteration 39410 : Loss 2935.4770\n",
      "Iteration 39420 : Loss 2935.4473\n",
      "Iteration 39430 : Loss 2935.4177\n",
      "Iteration 39440 : Loss 2935.3880\n",
      "Iteration 39450 : Loss 2935.3584\n",
      "Iteration 39460 : Loss 2935.3288\n",
      "Iteration 39470 : Loss 2935.2993\n",
      "Iteration 39480 : Loss 2935.2697\n",
      "Iteration 39490 : Loss 2935.2402\n",
      "Iteration 39500 : Loss 2935.2107\n",
      "Iteration 39510 : Loss 2935.1813\n",
      "Iteration 39520 : Loss 2935.1518\n",
      "Iteration 39530 : Loss 2935.1224\n",
      "Iteration 39540 : Loss 2935.0930\n",
      "Iteration 39550 : Loss 2935.0637\n",
      "Iteration 39560 : Loss 2935.0343\n",
      "Iteration 39570 : Loss 2935.0050\n",
      "Iteration 39580 : Loss 2934.9758\n",
      "Iteration 39590 : Loss 2934.9465\n",
      "Iteration 39600 : Loss 2934.9173\n",
      "Iteration 39610 : Loss 2934.8881\n",
      "Iteration 39620 : Loss 2934.8589\n",
      "Iteration 39630 : Loss 2934.8297\n",
      "Iteration 39640 : Loss 2934.8006\n",
      "Iteration 39650 : Loss 2934.7715\n",
      "Iteration 39660 : Loss 2934.7424\n",
      "Iteration 39670 : Loss 2934.7133\n",
      "Iteration 39680 : Loss 2934.6843\n",
      "Iteration 39690 : Loss 2934.6553\n",
      "Iteration 39700 : Loss 2934.6263\n",
      "Iteration 39710 : Loss 2934.5974\n",
      "Iteration 39720 : Loss 2934.5684\n",
      "Iteration 39730 : Loss 2934.5395\n",
      "Iteration 39740 : Loss 2934.5106\n",
      "Iteration 39750 : Loss 2934.4818\n",
      "Iteration 39760 : Loss 2934.4530\n",
      "Iteration 39770 : Loss 2934.4242\n",
      "Iteration 39780 : Loss 2934.3954\n",
      "Iteration 39790 : Loss 2934.3666\n",
      "Iteration 39800 : Loss 2934.3379\n",
      "Iteration 39810 : Loss 2934.3092\n",
      "Iteration 39820 : Loss 2934.2805\n",
      "Iteration 39830 : Loss 2934.2518\n",
      "Iteration 39840 : Loss 2934.2232\n",
      "Iteration 39850 : Loss 2934.1946\n",
      "Iteration 39860 : Loss 2934.1660\n",
      "Iteration 39870 : Loss 2934.1375\n",
      "Iteration 39880 : Loss 2934.1089\n",
      "Iteration 39890 : Loss 2934.0804\n",
      "Iteration 39900 : Loss 2934.0519\n",
      "Iteration 39910 : Loss 2934.0235\n",
      "Iteration 39920 : Loss 2933.9950\n",
      "Iteration 39930 : Loss 2933.9666\n",
      "Iteration 39940 : Loss 2933.9382\n",
      "Iteration 39950 : Loss 2933.9099\n",
      "Iteration 39960 : Loss 2933.8815\n",
      "Iteration 39970 : Loss 2933.8532\n",
      "Iteration 39980 : Loss 2933.8249\n",
      "Iteration 39990 : Loss 2933.7967\n",
      "Iteration 40000 : Loss 2933.7684\n",
      "Iteration 40010 : Loss 2933.7402\n",
      "Iteration 40020 : Loss 2933.7120\n",
      "Iteration 40030 : Loss 2933.6839\n",
      "Iteration 40040 : Loss 2933.6557\n",
      "Iteration 40050 : Loss 2933.6276\n",
      "Iteration 40060 : Loss 2933.5995\n",
      "Iteration 40070 : Loss 2933.5714\n",
      "Iteration 40080 : Loss 2933.5434\n",
      "Iteration 40090 : Loss 2933.5154\n",
      "Iteration 40100 : Loss 2933.4874\n",
      "Iteration 40110 : Loss 2933.4594\n",
      "Iteration 40120 : Loss 2933.4314\n",
      "Iteration 40130 : Loss 2933.4035\n",
      "Iteration 40140 : Loss 2933.3756\n",
      "Iteration 40150 : Loss 2933.3477\n",
      "Iteration 40160 : Loss 2933.3199\n",
      "Iteration 40170 : Loss 2933.2921\n",
      "Iteration 40180 : Loss 2933.2643\n",
      "Iteration 40190 : Loss 2933.2365\n",
      "Iteration 40200 : Loss 2933.2087\n",
      "Iteration 40210 : Loss 2933.1810\n",
      "Iteration 40220 : Loss 2933.1533\n",
      "Iteration 40230 : Loss 2933.1256\n",
      "Iteration 40240 : Loss 2933.0979\n",
      "Iteration 40250 : Loss 2933.0703\n",
      "Iteration 40260 : Loss 2933.0427\n",
      "Iteration 40270 : Loss 2933.0151\n",
      "Iteration 40280 : Loss 2932.9875\n",
      "Iteration 40290 : Loss 2932.9600\n",
      "Iteration 40300 : Loss 2932.9325\n",
      "Iteration 40310 : Loss 2932.9050\n",
      "Iteration 40320 : Loss 2932.8775\n",
      "Iteration 40330 : Loss 2932.8500\n",
      "Iteration 40340 : Loss 2932.8226\n",
      "Iteration 40350 : Loss 2932.7952\n",
      "Iteration 40360 : Loss 2932.7678\n",
      "Iteration 40370 : Loss 2932.7405\n",
      "Iteration 40380 : Loss 2932.7131\n",
      "Iteration 40390 : Loss 2932.6858\n",
      "Iteration 40400 : Loss 2932.6586\n",
      "Iteration 40410 : Loss 2932.6313\n",
      "Iteration 40420 : Loss 2932.6041\n",
      "Iteration 40430 : Loss 2932.5768\n",
      "Iteration 40440 : Loss 2932.5497\n",
      "Iteration 40450 : Loss 2932.5225\n",
      "Iteration 40460 : Loss 2932.4953\n",
      "Iteration 40470 : Loss 2932.4682\n",
      "Iteration 40480 : Loss 2932.4411\n",
      "Iteration 40490 : Loss 2932.4141\n",
      "Iteration 40500 : Loss 2932.3870\n",
      "Iteration 40510 : Loss 2932.3600\n",
      "Iteration 40520 : Loss 2932.3330\n",
      "Iteration 40530 : Loss 2932.3060\n",
      "Iteration 40540 : Loss 2932.2790\n",
      "Iteration 40550 : Loss 2932.2521\n",
      "Iteration 40560 : Loss 2932.2252\n",
      "Iteration 40570 : Loss 2932.1983\n",
      "Iteration 40580 : Loss 2932.1714\n",
      "Iteration 40590 : Loss 2932.1446\n",
      "Iteration 40600 : Loss 2932.1178\n",
      "Iteration 40610 : Loss 2932.0910\n",
      "Iteration 40620 : Loss 2932.0642\n",
      "Iteration 40630 : Loss 2932.0375\n",
      "Iteration 40640 : Loss 2932.0107\n",
      "Iteration 40650 : Loss 2931.9840\n",
      "Iteration 40660 : Loss 2931.9574\n",
      "Iteration 40670 : Loss 2931.9307\n",
      "Iteration 40680 : Loss 2931.9041\n",
      "Iteration 40690 : Loss 2931.8775\n",
      "Iteration 40700 : Loss 2931.8509\n",
      "Iteration 40710 : Loss 2931.8243\n",
      "Iteration 40720 : Loss 2931.7978\n",
      "Iteration 40730 : Loss 2931.7712\n",
      "Iteration 40740 : Loss 2931.7447\n",
      "Iteration 40750 : Loss 2931.7183\n",
      "Iteration 40760 : Loss 2931.6918\n",
      "Iteration 40770 : Loss 2931.6654\n",
      "Iteration 40780 : Loss 2931.6390\n",
      "Iteration 40790 : Loss 2931.6126\n",
      "Iteration 40800 : Loss 2931.5862\n",
      "Iteration 40810 : Loss 2931.5599\n",
      "Iteration 40820 : Loss 2931.5336\n",
      "Iteration 40830 : Loss 2931.5073\n",
      "Iteration 40840 : Loss 2931.4810\n",
      "Iteration 40850 : Loss 2931.4548\n",
      "Iteration 40860 : Loss 2931.4285\n",
      "Iteration 40870 : Loss 2931.4023\n",
      "Iteration 40880 : Loss 2931.3761\n",
      "Iteration 40890 : Loss 2931.3500\n",
      "Iteration 40900 : Loss 2931.3238\n",
      "Iteration 40910 : Loss 2931.2977\n",
      "Iteration 40920 : Loss 2931.2716\n",
      "Iteration 40930 : Loss 2931.2456\n",
      "Iteration 40940 : Loss 2931.2195\n",
      "Iteration 40950 : Loss 2931.1935\n",
      "Iteration 40960 : Loss 2931.1675\n",
      "Iteration 40970 : Loss 2931.1415\n",
      "Iteration 40980 : Loss 2931.1156\n",
      "Iteration 40990 : Loss 2931.0896\n",
      "Iteration 41000 : Loss 2931.0637\n",
      "Iteration 41010 : Loss 2931.0378\n",
      "Iteration 41020 : Loss 2931.0119\n",
      "Iteration 41030 : Loss 2930.9861\n",
      "Iteration 41040 : Loss 2930.9603\n",
      "Iteration 41050 : Loss 2930.9345\n",
      "Iteration 41060 : Loss 2930.9087\n",
      "Iteration 41070 : Loss 2930.8829\n",
      "Iteration 41080 : Loss 2930.8572\n",
      "Iteration 41090 : Loss 2930.8315\n",
      "Iteration 41100 : Loss 2930.8058\n",
      "Iteration 41110 : Loss 2930.7801\n",
      "Iteration 41120 : Loss 2930.7545\n",
      "Iteration 41130 : Loss 2930.7288\n",
      "Iteration 41140 : Loss 2930.7032\n",
      "Iteration 41150 : Loss 2930.6777\n",
      "Iteration 41160 : Loss 2930.6521\n",
      "Iteration 41170 : Loss 2930.6266\n",
      "Iteration 41180 : Loss 2930.6010\n",
      "Iteration 41190 : Loss 2930.5755\n",
      "Iteration 41200 : Loss 2930.5501\n",
      "Iteration 41210 : Loss 2930.5246\n",
      "Iteration 41220 : Loss 2930.4992\n",
      "Iteration 41230 : Loss 2930.4738\n",
      "Iteration 41240 : Loss 2930.4484\n",
      "Iteration 41250 : Loss 2930.4230\n",
      "Iteration 41260 : Loss 2930.3977\n",
      "Iteration 41270 : Loss 2930.3724\n",
      "Iteration 41280 : Loss 2930.3471\n",
      "Iteration 41290 : Loss 2930.3218\n",
      "Iteration 41300 : Loss 2930.2965\n",
      "Iteration 41310 : Loss 2930.2713\n",
      "Iteration 41320 : Loss 2930.2461\n",
      "Iteration 41330 : Loss 2930.2209\n",
      "Iteration 41340 : Loss 2930.1957\n",
      "Iteration 41350 : Loss 2930.1706\n",
      "Iteration 41360 : Loss 2930.1454\n",
      "Iteration 41370 : Loss 2930.1203\n",
      "Iteration 41380 : Loss 2930.0952\n",
      "Iteration 41390 : Loss 2930.0702\n",
      "Iteration 41400 : Loss 2930.0451\n",
      "Iteration 41410 : Loss 2930.0201\n",
      "Iteration 41420 : Loss 2929.9951\n",
      "Iteration 41430 : Loss 2929.9701\n",
      "Iteration 41440 : Loss 2929.9452\n",
      "Iteration 41450 : Loss 2929.9202\n",
      "Iteration 41460 : Loss 2929.8953\n",
      "Iteration 41470 : Loss 2929.8704\n",
      "Iteration 41480 : Loss 2929.8456\n",
      "Iteration 41490 : Loss 2929.8207\n",
      "Iteration 41500 : Loss 2929.7959\n",
      "Iteration 41510 : Loss 2929.7711\n",
      "Iteration 41520 : Loss 2929.7463\n",
      "Iteration 41530 : Loss 2929.7215\n",
      "Iteration 41540 : Loss 2929.6968\n",
      "Iteration 41550 : Loss 2929.6721\n",
      "Iteration 41560 : Loss 2929.6474\n",
      "Iteration 41570 : Loss 2929.6227\n",
      "Iteration 41580 : Loss 2929.5980\n",
      "Iteration 41590 : Loss 2929.5734\n",
      "Iteration 41600 : Loss 2929.5488\n",
      "Iteration 41610 : Loss 2929.5242\n",
      "Iteration 41620 : Loss 2929.4996\n",
      "Iteration 41630 : Loss 2929.4750\n",
      "Iteration 41640 : Loss 2929.4505\n",
      "Iteration 41650 : Loss 2929.4260\n",
      "Iteration 41660 : Loss 2929.4015\n",
      "Iteration 41670 : Loss 2929.3770\n",
      "Iteration 41680 : Loss 2929.3526\n",
      "Iteration 41690 : Loss 2929.3281\n",
      "Iteration 41700 : Loss 2929.3037\n",
      "Iteration 41710 : Loss 2929.2793\n",
      "Iteration 41720 : Loss 2929.2550\n",
      "Iteration 41730 : Loss 2929.2306\n",
      "Iteration 41740 : Loss 2929.2063\n",
      "Iteration 41750 : Loss 2929.1820\n",
      "Iteration 41760 : Loss 2929.1577\n",
      "Iteration 41770 : Loss 2929.1334\n",
      "Iteration 41780 : Loss 2929.1092\n",
      "Iteration 41790 : Loss 2929.0850\n",
      "Iteration 41800 : Loss 2929.0608\n",
      "Iteration 41810 : Loss 2929.0366\n",
      "Iteration 41820 : Loss 2929.0124\n",
      "Iteration 41830 : Loss 2928.9883\n",
      "Iteration 41840 : Loss 2928.9642\n",
      "Iteration 41850 : Loss 2928.9401\n",
      "Iteration 41860 : Loss 2928.9160\n",
      "Iteration 41870 : Loss 2928.8919\n",
      "Iteration 41880 : Loss 2928.8679\n",
      "Iteration 41890 : Loss 2928.8439\n",
      "Iteration 41900 : Loss 2928.8199\n",
      "Iteration 41910 : Loss 2928.7959\n",
      "Iteration 41920 : Loss 2928.7720\n",
      "Iteration 41930 : Loss 2928.7480\n",
      "Iteration 41940 : Loss 2928.7241\n",
      "Iteration 41950 : Loss 2928.7002\n",
      "Iteration 41960 : Loss 2928.6763\n",
      "Iteration 41970 : Loss 2928.6525\n",
      "Iteration 41980 : Loss 2928.6286\n",
      "Iteration 41990 : Loss 2928.6048\n",
      "Iteration 42000 : Loss 2928.5810\n",
      "Iteration 42010 : Loss 2928.5573\n",
      "Iteration 42020 : Loss 2928.5335\n",
      "Iteration 42030 : Loss 2928.5098\n",
      "Iteration 42040 : Loss 2928.4861\n",
      "Iteration 42050 : Loss 2928.4624\n",
      "Iteration 42060 : Loss 2928.4387\n",
      "Iteration 42070 : Loss 2928.4151\n",
      "Iteration 42080 : Loss 2928.3914\n",
      "Iteration 42090 : Loss 2928.3678\n",
      "Iteration 42100 : Loss 2928.3442\n",
      "Iteration 42110 : Loss 2928.3206\n",
      "Iteration 42120 : Loss 2928.2971\n",
      "Iteration 42130 : Loss 2928.2736\n",
      "Iteration 42140 : Loss 2928.2500\n",
      "Iteration 42150 : Loss 2928.2266\n",
      "Iteration 42160 : Loss 2928.2031\n",
      "Iteration 42170 : Loss 2928.1796\n",
      "Iteration 42180 : Loss 2928.1562\n",
      "Iteration 42190 : Loss 2928.1328\n",
      "Iteration 42200 : Loss 2928.1094\n",
      "Iteration 42210 : Loss 2928.0860\n",
      "Iteration 42220 : Loss 2928.0627\n",
      "Iteration 42230 : Loss 2928.0393\n",
      "Iteration 42240 : Loss 2928.0160\n",
      "Iteration 42250 : Loss 2927.9927\n",
      "Iteration 42260 : Loss 2927.9695\n",
      "Iteration 42270 : Loss 2927.9462\n",
      "Iteration 42280 : Loss 2927.9230\n",
      "Iteration 42290 : Loss 2927.8998\n",
      "Iteration 42300 : Loss 2927.8766\n",
      "Iteration 42310 : Loss 2927.8534\n",
      "Iteration 42320 : Loss 2927.8302\n",
      "Iteration 42330 : Loss 2927.8071\n",
      "Iteration 42340 : Loss 2927.7840\n",
      "Iteration 42350 : Loss 2927.7609\n",
      "Iteration 42360 : Loss 2927.7378\n",
      "Iteration 42370 : Loss 2927.7147\n",
      "Iteration 42380 : Loss 2927.6917\n",
      "Iteration 42390 : Loss 2927.6687\n",
      "Iteration 42400 : Loss 2927.6457\n",
      "Iteration 42410 : Loss 2927.6227\n",
      "Iteration 42420 : Loss 2927.5998\n",
      "Iteration 42430 : Loss 2927.5768\n",
      "Iteration 42440 : Loss 2927.5539\n",
      "Iteration 42450 : Loss 2927.5310\n",
      "Iteration 42460 : Loss 2927.5081\n",
      "Iteration 42470 : Loss 2927.4852\n",
      "Iteration 42480 : Loss 2927.4624\n",
      "Iteration 42490 : Loss 2927.4396\n",
      "Iteration 42500 : Loss 2927.4168\n",
      "Iteration 42510 : Loss 2927.3940\n",
      "Iteration 42520 : Loss 2927.3712\n",
      "Iteration 42530 : Loss 2927.3485\n",
      "Iteration 42540 : Loss 2927.3258\n",
      "Iteration 42550 : Loss 2927.3030\n",
      "Iteration 42560 : Loss 2927.2804\n",
      "Iteration 42570 : Loss 2927.2577\n",
      "Iteration 42580 : Loss 2927.2350\n",
      "Iteration 42590 : Loss 2927.2124\n",
      "Iteration 42600 : Loss 2927.1898\n",
      "Iteration 42610 : Loss 2927.1672\n",
      "Iteration 42620 : Loss 2927.1446\n",
      "Iteration 42630 : Loss 2927.1221\n",
      "Iteration 42640 : Loss 2927.0995\n",
      "Iteration 42650 : Loss 2927.0770\n",
      "Iteration 42660 : Loss 2927.0545\n",
      "Iteration 42670 : Loss 2927.0320\n",
      "Iteration 42680 : Loss 2927.0096\n",
      "Iteration 42690 : Loss 2926.9871\n",
      "Iteration 42700 : Loss 2926.9647\n",
      "Iteration 42710 : Loss 2926.9423\n",
      "Iteration 42720 : Loss 2926.9199\n",
      "Iteration 42730 : Loss 2926.8976\n",
      "Iteration 42740 : Loss 2926.8752\n",
      "Iteration 42750 : Loss 2926.8529\n",
      "Iteration 42760 : Loss 2926.8306\n",
      "Iteration 42770 : Loss 2926.8083\n",
      "Iteration 42780 : Loss 2926.7860\n",
      "Iteration 42790 : Loss 2926.7638\n",
      "Iteration 42800 : Loss 2926.7415\n",
      "Iteration 42810 : Loss 2926.7193\n",
      "Iteration 42820 : Loss 2926.6971\n",
      "Iteration 42830 : Loss 2926.6750\n",
      "Iteration 42840 : Loss 2926.6528\n",
      "Iteration 42850 : Loss 2926.6307\n",
      "Iteration 42860 : Loss 2926.6085\n",
      "Iteration 42870 : Loss 2926.5864\n",
      "Iteration 42880 : Loss 2926.5644\n",
      "Iteration 42890 : Loss 2926.5423\n",
      "Iteration 42900 : Loss 2926.5203\n",
      "Iteration 42910 : Loss 2926.4982\n",
      "Iteration 42920 : Loss 2926.4762\n",
      "Iteration 42930 : Loss 2926.4542\n",
      "Iteration 42940 : Loss 2926.4323\n",
      "Iteration 42950 : Loss 2926.4103\n",
      "Iteration 42960 : Loss 2926.3884\n",
      "Iteration 42970 : Loss 2926.3665\n",
      "Iteration 42980 : Loss 2926.3446\n",
      "Iteration 42990 : Loss 2926.3227\n",
      "Iteration 43000 : Loss 2926.3008\n",
      "Iteration 43010 : Loss 2926.2790\n",
      "Iteration 43020 : Loss 2926.2572\n",
      "Iteration 43030 : Loss 2926.2354\n",
      "Iteration 43040 : Loss 2926.2136\n",
      "Iteration 43050 : Loss 2926.1918\n",
      "Iteration 43060 : Loss 2926.1700\n",
      "Iteration 43070 : Loss 2926.1483\n",
      "Iteration 43080 : Loss 2926.1266\n",
      "Iteration 43090 : Loss 2926.1049\n",
      "Iteration 43100 : Loss 2926.0832\n",
      "Iteration 43110 : Loss 2926.0616\n",
      "Iteration 43120 : Loss 2926.0399\n",
      "Iteration 43130 : Loss 2926.0183\n",
      "Iteration 43140 : Loss 2925.9967\n",
      "Iteration 43150 : Loss 2925.9751\n",
      "Iteration 43160 : Loss 2925.9536\n",
      "Iteration 43170 : Loss 2925.9320\n",
      "Iteration 43180 : Loss 2925.9105\n",
      "Iteration 43190 : Loss 2925.8890\n",
      "Iteration 43200 : Loss 2925.8675\n",
      "Iteration 43210 : Loss 2925.8460\n",
      "Iteration 43220 : Loss 2925.8245\n",
      "Iteration 43230 : Loss 2925.8031\n",
      "Iteration 43240 : Loss 2925.7817\n",
      "Iteration 43250 : Loss 2925.7603\n",
      "Iteration 43260 : Loss 2925.7389\n",
      "Iteration 43270 : Loss 2925.7175\n",
      "Iteration 43280 : Loss 2925.6962\n",
      "Iteration 43290 : Loss 2925.6748\n",
      "Iteration 43300 : Loss 2925.6535\n",
      "Iteration 43310 : Loss 2925.6322\n",
      "Iteration 43320 : Loss 2925.6110\n",
      "Iteration 43330 : Loss 2925.5897\n",
      "Iteration 43340 : Loss 2925.5685\n",
      "Iteration 43350 : Loss 2925.5472\n",
      "Iteration 43360 : Loss 2925.5260\n",
      "Iteration 43370 : Loss 2925.5048\n",
      "Iteration 43380 : Loss 2925.4837\n",
      "Iteration 43390 : Loss 2925.4625\n",
      "Iteration 43400 : Loss 2925.4414\n",
      "Iteration 43410 : Loss 2925.4203\n",
      "Iteration 43420 : Loss 2925.3992\n",
      "Iteration 43430 : Loss 2925.3781\n",
      "Iteration 43440 : Loss 2925.3570\n",
      "Iteration 43450 : Loss 2925.3360\n",
      "Iteration 43460 : Loss 2925.3149\n",
      "Iteration 43470 : Loss 2925.2939\n",
      "Iteration 43480 : Loss 2925.2729\n",
      "Iteration 43490 : Loss 2925.2520\n",
      "Iteration 43500 : Loss 2925.2310\n",
      "Iteration 43510 : Loss 2925.2101\n",
      "Iteration 43520 : Loss 2925.1891\n",
      "Iteration 43530 : Loss 2925.1682\n",
      "Iteration 43540 : Loss 2925.1474\n",
      "Iteration 43550 : Loss 2925.1265\n",
      "Iteration 43560 : Loss 2925.1056\n",
      "Iteration 43570 : Loss 2925.0848\n",
      "Iteration 43580 : Loss 2925.0640\n",
      "Iteration 43590 : Loss 2925.0432\n",
      "Iteration 43600 : Loss 2925.0224\n",
      "Iteration 43610 : Loss 2925.0016\n",
      "Iteration 43620 : Loss 2924.9809\n",
      "Iteration 43630 : Loss 2924.9602\n",
      "Iteration 43640 : Loss 2924.9394\n",
      "Iteration 43650 : Loss 2924.9187\n",
      "Iteration 43660 : Loss 2924.8981\n",
      "Iteration 43670 : Loss 2924.8774\n",
      "Iteration 43680 : Loss 2924.8568\n",
      "Iteration 43690 : Loss 2924.8361\n",
      "Iteration 43700 : Loss 2924.8155\n",
      "Iteration 43710 : Loss 2924.7949\n",
      "Iteration 43720 : Loss 2924.7744\n",
      "Iteration 43730 : Loss 2924.7538\n",
      "Iteration 43740 : Loss 2924.7333\n",
      "Iteration 43750 : Loss 2924.7128\n",
      "Iteration 43760 : Loss 2924.6922\n",
      "Iteration 43770 : Loss 2924.6718\n",
      "Iteration 43780 : Loss 2924.6513\n",
      "Iteration 43790 : Loss 2924.6308\n",
      "Iteration 43800 : Loss 2924.6104\n",
      "Iteration 43810 : Loss 2924.5900\n",
      "Iteration 43820 : Loss 2924.5696\n",
      "Iteration 43830 : Loss 2924.5492\n",
      "Iteration 43840 : Loss 2924.5288\n",
      "Iteration 43850 : Loss 2924.5085\n",
      "Iteration 43860 : Loss 2924.4881\n",
      "Iteration 43870 : Loss 2924.4678\n",
      "Iteration 43880 : Loss 2924.4475\n",
      "Iteration 43890 : Loss 2924.4272\n",
      "Iteration 43900 : Loss 2924.4070\n",
      "Iteration 43910 : Loss 2924.3867\n",
      "Iteration 43920 : Loss 2924.3665\n",
      "Iteration 43930 : Loss 2924.3463\n",
      "Iteration 43940 : Loss 2924.3261\n",
      "Iteration 43950 : Loss 2924.3059\n",
      "Iteration 43960 : Loss 2924.2857\n",
      "Iteration 43970 : Loss 2924.2656\n",
      "Iteration 43980 : Loss 2924.2455\n",
      "Iteration 43990 : Loss 2924.2254\n",
      "Iteration 44000 : Loss 2924.2053\n",
      "Iteration 44010 : Loss 2924.1852\n",
      "Iteration 44020 : Loss 2924.1651\n",
      "Iteration 44030 : Loss 2924.1451\n",
      "Iteration 44040 : Loss 2924.1250\n",
      "Iteration 44050 : Loss 2924.1050\n",
      "Iteration 44060 : Loss 2924.0850\n",
      "Iteration 44070 : Loss 2924.0651\n",
      "Iteration 44080 : Loss 2924.0451\n",
      "Iteration 44090 : Loss 2924.0252\n",
      "Iteration 44100 : Loss 2924.0052\n",
      "Iteration 44110 : Loss 2923.9853\n",
      "Iteration 44120 : Loss 2923.9654\n",
      "Iteration 44130 : Loss 2923.9455\n",
      "Iteration 44140 : Loss 2923.9257\n",
      "Iteration 44150 : Loss 2923.9058\n",
      "Iteration 44160 : Loss 2923.8860\n",
      "Iteration 44170 : Loss 2923.8662\n",
      "Iteration 44180 : Loss 2923.8464\n",
      "Iteration 44190 : Loss 2923.8266\n",
      "Iteration 44200 : Loss 2923.8069\n",
      "Iteration 44210 : Loss 2923.7871\n",
      "Iteration 44220 : Loss 2923.7674\n",
      "Iteration 44230 : Loss 2923.7477\n",
      "Iteration 44240 : Loss 2923.7280\n",
      "Iteration 44250 : Loss 2923.7083\n",
      "Iteration 44260 : Loss 2923.6886\n",
      "Iteration 44270 : Loss 2923.6690\n",
      "Iteration 44280 : Loss 2923.6494\n",
      "Iteration 44290 : Loss 2923.6298\n",
      "Iteration 44300 : Loss 2923.6102\n",
      "Iteration 44310 : Loss 2923.5906\n",
      "Iteration 44320 : Loss 2923.5710\n",
      "Iteration 44330 : Loss 2923.5515\n",
      "Iteration 44340 : Loss 2923.5319\n",
      "Iteration 44350 : Loss 2923.5124\n",
      "Iteration 44360 : Loss 2923.4929\n",
      "Iteration 44370 : Loss 2923.4734\n",
      "Iteration 44380 : Loss 2923.4540\n",
      "Iteration 44390 : Loss 2923.4345\n",
      "Iteration 44400 : Loss 2923.4151\n",
      "Iteration 44410 : Loss 2923.3957\n",
      "Iteration 44420 : Loss 2923.3763\n",
      "Iteration 44430 : Loss 2923.3569\n",
      "Iteration 44440 : Loss 2923.3375\n",
      "Iteration 44450 : Loss 2923.3182\n",
      "Iteration 44460 : Loss 2923.2988\n",
      "Iteration 44470 : Loss 2923.2795\n",
      "Iteration 44480 : Loss 2923.2602\n",
      "Iteration 44490 : Loss 2923.2409\n",
      "Iteration 44500 : Loss 2923.2216\n",
      "Iteration 44510 : Loss 2923.2024\n",
      "Iteration 44520 : Loss 2923.1831\n",
      "Iteration 44530 : Loss 2923.1639\n",
      "Iteration 44540 : Loss 2923.1447\n",
      "Iteration 44550 : Loss 2923.1255\n",
      "Iteration 44560 : Loss 2923.1063\n",
      "Iteration 44570 : Loss 2923.0872\n",
      "Iteration 44580 : Loss 2923.0680\n",
      "Iteration 44590 : Loss 2923.0489\n",
      "Iteration 44600 : Loss 2923.0298\n",
      "Iteration 44610 : Loss 2923.0107\n",
      "Iteration 44620 : Loss 2922.9916\n",
      "Iteration 44630 : Loss 2922.9726\n",
      "Iteration 44640 : Loss 2922.9535\n",
      "Iteration 44650 : Loss 2922.9345\n",
      "Iteration 44660 : Loss 2922.9155\n",
      "Iteration 44670 : Loss 2922.8965\n",
      "Iteration 44680 : Loss 2922.8775\n",
      "Iteration 44690 : Loss 2922.8585\n",
      "Iteration 44700 : Loss 2922.8395\n",
      "Iteration 44710 : Loss 2922.8206\n",
      "Iteration 44720 : Loss 2922.8017\n",
      "Iteration 44730 : Loss 2922.7828\n",
      "Iteration 44740 : Loss 2922.7639\n",
      "Iteration 44750 : Loss 2922.7450\n",
      "Iteration 44760 : Loss 2922.7262\n",
      "Iteration 44770 : Loss 2922.7073\n",
      "Iteration 44780 : Loss 2922.6885\n",
      "Iteration 44790 : Loss 2922.6697\n",
      "Iteration 44800 : Loss 2922.6509\n",
      "Iteration 44810 : Loss 2922.6321\n",
      "Iteration 44820 : Loss 2922.6133\n",
      "Iteration 44830 : Loss 2922.5946\n",
      "Iteration 44840 : Loss 2922.5759\n",
      "Iteration 44850 : Loss 2922.5571\n",
      "Iteration 44860 : Loss 2922.5384\n",
      "Iteration 44870 : Loss 2922.5197\n",
      "Iteration 44880 : Loss 2922.5011\n",
      "Iteration 44890 : Loss 2922.4824\n",
      "Iteration 44900 : Loss 2922.4638\n",
      "Iteration 44910 : Loss 2922.4452\n",
      "Iteration 44920 : Loss 2922.4265\n",
      "Iteration 44930 : Loss 2922.4080\n",
      "Iteration 44940 : Loss 2922.3894\n",
      "Iteration 44950 : Loss 2922.3708\n",
      "Iteration 44960 : Loss 2922.3523\n",
      "Iteration 44970 : Loss 2922.3337\n",
      "Iteration 44980 : Loss 2922.3152\n",
      "Iteration 44990 : Loss 2922.2967\n",
      "Iteration 45000 : Loss 2922.2782\n",
      "Iteration 45010 : Loss 2922.2598\n",
      "Iteration 45020 : Loss 2922.2413\n",
      "Iteration 45030 : Loss 2922.2229\n",
      "Iteration 45040 : Loss 2922.2044\n",
      "Iteration 45050 : Loss 2922.1860\n",
      "Iteration 45060 : Loss 2922.1676\n",
      "Iteration 45070 : Loss 2922.1493\n",
      "Iteration 45080 : Loss 2922.1309\n",
      "Iteration 45090 : Loss 2922.1126\n",
      "Iteration 45100 : Loss 2922.0942\n",
      "Iteration 45110 : Loss 2922.0759\n",
      "Iteration 45120 : Loss 2922.0576\n",
      "Iteration 45130 : Loss 2922.0393\n",
      "Iteration 45140 : Loss 2922.0210\n",
      "Iteration 45150 : Loss 2922.0028\n",
      "Iteration 45160 : Loss 2921.9845\n",
      "Iteration 45170 : Loss 2921.9663\n",
      "Iteration 45180 : Loss 2921.9481\n",
      "Iteration 45190 : Loss 2921.9299\n",
      "Iteration 45200 : Loss 2921.9117\n",
      "Iteration 45210 : Loss 2921.8936\n",
      "Iteration 45220 : Loss 2921.8754\n",
      "Iteration 45230 : Loss 2921.8573\n",
      "Iteration 45240 : Loss 2921.8392\n",
      "Iteration 45250 : Loss 2921.8211\n",
      "Iteration 45260 : Loss 2921.8030\n",
      "Iteration 45270 : Loss 2921.7849\n",
      "Iteration 45280 : Loss 2921.7668\n",
      "Iteration 45290 : Loss 2921.7488\n",
      "Iteration 45300 : Loss 2921.7308\n",
      "Iteration 45310 : Loss 2921.7127\n",
      "Iteration 45320 : Loss 2921.6947\n",
      "Iteration 45330 : Loss 2921.6768\n",
      "Iteration 45340 : Loss 2921.6588\n",
      "Iteration 45350 : Loss 2921.6408\n",
      "Iteration 45360 : Loss 2921.6229\n",
      "Iteration 45370 : Loss 2921.6050\n",
      "Iteration 45380 : Loss 2921.5871\n",
      "Iteration 45390 : Loss 2921.5692\n",
      "Iteration 45400 : Loss 2921.5513\n",
      "Iteration 45410 : Loss 2921.5334\n",
      "Iteration 45420 : Loss 2921.5156\n",
      "Iteration 45430 : Loss 2921.4977\n",
      "Iteration 45440 : Loss 2921.4799\n",
      "Iteration 45450 : Loss 2921.4621\n",
      "Iteration 45460 : Loss 2921.4443\n",
      "Iteration 45470 : Loss 2921.4265\n",
      "Iteration 45480 : Loss 2921.4088\n",
      "Iteration 45490 : Loss 2921.3910\n",
      "Iteration 45500 : Loss 2921.3733\n",
      "Iteration 45510 : Loss 2921.3556\n",
      "Iteration 45520 : Loss 2921.3379\n",
      "Iteration 45530 : Loss 2921.3202\n",
      "Iteration 45540 : Loss 2921.3025\n",
      "Iteration 45550 : Loss 2921.2848\n",
      "Iteration 45560 : Loss 2921.2672\n",
      "Iteration 45570 : Loss 2921.2496\n",
      "Iteration 45580 : Loss 2921.2319\n",
      "Iteration 45590 : Loss 2921.2143\n",
      "Iteration 45600 : Loss 2921.1967\n",
      "Iteration 45610 : Loss 2921.1792\n",
      "Iteration 45620 : Loss 2921.1616\n",
      "Iteration 45630 : Loss 2921.1441\n",
      "Iteration 45640 : Loss 2921.1265\n",
      "Iteration 45650 : Loss 2921.1090\n",
      "Iteration 45660 : Loss 2921.0915\n",
      "Iteration 45670 : Loss 2921.0740\n",
      "Iteration 45680 : Loss 2921.0566\n",
      "Iteration 45690 : Loss 2921.0391\n",
      "Iteration 45700 : Loss 2921.0217\n",
      "Iteration 45710 : Loss 2921.0042\n",
      "Iteration 45720 : Loss 2920.9868\n",
      "Iteration 45730 : Loss 2920.9694\n",
      "Iteration 45740 : Loss 2920.9521\n",
      "Iteration 45750 : Loss 2920.9347\n",
      "Iteration 45760 : Loss 2920.9173\n",
      "Iteration 45770 : Loss 2920.9000\n",
      "Iteration 45780 : Loss 2920.8827\n",
      "Iteration 45790 : Loss 2920.8653\n",
      "Iteration 45800 : Loss 2920.8480\n",
      "Iteration 45810 : Loss 2920.8308\n",
      "Iteration 45820 : Loss 2920.8135\n",
      "Iteration 45830 : Loss 2920.7962\n",
      "Iteration 45840 : Loss 2920.7790\n",
      "Iteration 45850 : Loss 2920.7618\n",
      "Iteration 45860 : Loss 2920.7446\n",
      "Iteration 45870 : Loss 2920.7274\n",
      "Iteration 45880 : Loss 2920.7102\n",
      "Iteration 45890 : Loss 2920.6930\n",
      "Iteration 45900 : Loss 2920.6758\n",
      "Iteration 45910 : Loss 2920.6587\n",
      "Iteration 45920 : Loss 2920.6416\n",
      "Iteration 45930 : Loss 2920.6245\n",
      "Iteration 45940 : Loss 2920.6074\n",
      "Iteration 45950 : Loss 2920.5903\n",
      "Iteration 45960 : Loss 2920.5732\n",
      "Iteration 45970 : Loss 2920.5562\n",
      "Iteration 45980 : Loss 2920.5391\n",
      "Iteration 45990 : Loss 2920.5221\n",
      "Iteration 46000 : Loss 2920.5051\n",
      "Iteration 46010 : Loss 2920.4881\n",
      "Iteration 46020 : Loss 2920.4711\n",
      "Iteration 46030 : Loss 2920.4541\n",
      "Iteration 46040 : Loss 2920.4372\n",
      "Iteration 46050 : Loss 2920.4202\n",
      "Iteration 46060 : Loss 2920.4033\n",
      "Iteration 46070 : Loss 2920.3864\n",
      "Iteration 46080 : Loss 2920.3695\n",
      "Iteration 46090 : Loss 2920.3526\n",
      "Iteration 46100 : Loss 2920.3357\n",
      "Iteration 46110 : Loss 2920.3188\n",
      "Iteration 46120 : Loss 2920.3020\n",
      "Iteration 46130 : Loss 2920.2852\n",
      "Iteration 46140 : Loss 2920.2683\n",
      "Iteration 46150 : Loss 2920.2515\n",
      "Iteration 46160 : Loss 2920.2347\n",
      "Iteration 46170 : Loss 2920.2180\n",
      "Iteration 46180 : Loss 2920.2012\n",
      "Iteration 46190 : Loss 2920.1844\n",
      "Iteration 46200 : Loss 2920.1677\n",
      "Iteration 46210 : Loss 2920.1510\n",
      "Iteration 46220 : Loss 2920.1343\n",
      "Iteration 46230 : Loss 2920.1176\n",
      "Iteration 46240 : Loss 2920.1009\n",
      "Iteration 46250 : Loss 2920.0842\n",
      "Iteration 46260 : Loss 2920.0676\n",
      "Iteration 46270 : Loss 2920.0509\n",
      "Iteration 46280 : Loss 2920.0343\n",
      "Iteration 46290 : Loss 2920.0177\n",
      "Iteration 46300 : Loss 2920.0011\n",
      "Iteration 46310 : Loss 2919.9845\n",
      "Iteration 46320 : Loss 2919.9679\n",
      "Iteration 46330 : Loss 2919.9514\n",
      "Iteration 46340 : Loss 2919.9348\n",
      "Iteration 46350 : Loss 2919.9183\n",
      "Iteration 46360 : Loss 2919.9018\n",
      "Iteration 46370 : Loss 2919.8853\n",
      "Iteration 46380 : Loss 2919.8688\n",
      "Iteration 46390 : Loss 2919.8523\n",
      "Iteration 46400 : Loss 2919.8359\n",
      "Iteration 46410 : Loss 2919.8194\n",
      "Iteration 46420 : Loss 2919.8030\n",
      "Iteration 46430 : Loss 2919.7866\n",
      "Iteration 46440 : Loss 2919.7701\n",
      "Iteration 46450 : Loss 2919.7538\n",
      "Iteration 46460 : Loss 2919.7374\n",
      "Iteration 46470 : Loss 2919.7210\n",
      "Iteration 46480 : Loss 2919.7047\n",
      "Iteration 46490 : Loss 2919.6883\n",
      "Iteration 46500 : Loss 2919.6720\n",
      "Iteration 46510 : Loss 2919.6557\n",
      "Iteration 46520 : Loss 2919.6394\n",
      "Iteration 46530 : Loss 2919.6231\n",
      "Iteration 46540 : Loss 2919.6068\n",
      "Iteration 46550 : Loss 2919.5905\n",
      "Iteration 46560 : Loss 2919.5743\n",
      "Iteration 46570 : Loss 2919.5581\n",
      "Iteration 46580 : Loss 2919.5418\n",
      "Iteration 46590 : Loss 2919.5256\n",
      "Iteration 46600 : Loss 2919.5094\n",
      "Iteration 46610 : Loss 2919.4933\n",
      "Iteration 46620 : Loss 2919.4771\n",
      "Iteration 46630 : Loss 2919.4609\n",
      "Iteration 46640 : Loss 2919.4448\n",
      "Iteration 46650 : Loss 2919.4287\n",
      "Iteration 46660 : Loss 2919.4126\n",
      "Iteration 46670 : Loss 2919.3965\n",
      "Iteration 46680 : Loss 2919.3804\n",
      "Iteration 46690 : Loss 2919.3643\n",
      "Iteration 46700 : Loss 2919.3482\n",
      "Iteration 46710 : Loss 2919.3322\n",
      "Iteration 46720 : Loss 2919.3161\n",
      "Iteration 46730 : Loss 2919.3001\n",
      "Iteration 46740 : Loss 2919.2841\n",
      "Iteration 46750 : Loss 2919.2681\n",
      "Iteration 46760 : Loss 2919.2521\n",
      "Iteration 46770 : Loss 2919.2362\n",
      "Iteration 46780 : Loss 2919.2202\n",
      "Iteration 46790 : Loss 2919.2043\n",
      "Iteration 46800 : Loss 2919.1883\n",
      "Iteration 46810 : Loss 2919.1724\n",
      "Iteration 46820 : Loss 2919.1565\n",
      "Iteration 46830 : Loss 2919.1406\n",
      "Iteration 46840 : Loss 2919.1248\n",
      "Iteration 46850 : Loss 2919.1089\n",
      "Iteration 46860 : Loss 2919.0930\n",
      "Iteration 46870 : Loss 2919.0772\n",
      "Iteration 46880 : Loss 2919.0614\n",
      "Iteration 46890 : Loss 2919.0456\n",
      "Iteration 46900 : Loss 2919.0298\n",
      "Iteration 46910 : Loss 2919.0140\n",
      "Iteration 46920 : Loss 2918.9982\n",
      "Iteration 46930 : Loss 2918.9824\n",
      "Iteration 46940 : Loss 2918.9667\n",
      "Iteration 46950 : Loss 2918.9510\n",
      "Iteration 46960 : Loss 2918.9352\n",
      "Iteration 46970 : Loss 2918.9195\n",
      "Iteration 46980 : Loss 2918.9038\n",
      "Iteration 46990 : Loss 2918.8881\n",
      "Iteration 47000 : Loss 2918.8725\n",
      "Iteration 47010 : Loss 2918.8568\n",
      "Iteration 47020 : Loss 2918.8412\n",
      "Iteration 47030 : Loss 2918.8255\n",
      "Iteration 47040 : Loss 2918.8099\n",
      "Iteration 47050 : Loss 2918.7943\n",
      "Iteration 47060 : Loss 2918.7787\n",
      "Iteration 47070 : Loss 2918.7631\n",
      "Iteration 47080 : Loss 2918.7476\n",
      "Iteration 47090 : Loss 2918.7320\n",
      "Iteration 47100 : Loss 2918.7165\n",
      "Iteration 47110 : Loss 2918.7009\n",
      "Iteration 47120 : Loss 2918.6854\n",
      "Iteration 47130 : Loss 2918.6699\n",
      "Iteration 47140 : Loss 2918.6544\n",
      "Iteration 47150 : Loss 2918.6389\n",
      "Iteration 47160 : Loss 2918.6235\n",
      "Iteration 47170 : Loss 2918.6080\n",
      "Iteration 47180 : Loss 2918.5926\n",
      "Iteration 47190 : Loss 2918.5772\n",
      "Iteration 47200 : Loss 2918.5617\n",
      "Iteration 47210 : Loss 2918.5463\n",
      "Iteration 47220 : Loss 2918.5309\n",
      "Iteration 47230 : Loss 2918.5156\n",
      "Iteration 47240 : Loss 2918.5002\n",
      "Iteration 47250 : Loss 2918.4848\n",
      "Iteration 47260 : Loss 2918.4695\n",
      "Iteration 47270 : Loss 2918.4542\n",
      "Iteration 47280 : Loss 2918.4389\n",
      "Iteration 47290 : Loss 2918.4236\n",
      "Iteration 47300 : Loss 2918.4083\n",
      "Iteration 47310 : Loss 2918.3930\n",
      "Iteration 47320 : Loss 2918.3777\n",
      "Iteration 47330 : Loss 2918.3625\n",
      "Iteration 47340 : Loss 2918.3472\n",
      "Iteration 47350 : Loss 2918.3320\n",
      "Iteration 47360 : Loss 2918.3168\n",
      "Iteration 47370 : Loss 2918.3016\n",
      "Iteration 47380 : Loss 2918.2864\n",
      "Iteration 47390 : Loss 2918.2712\n",
      "Iteration 47400 : Loss 2918.2560\n",
      "Iteration 47410 : Loss 2918.2409\n",
      "Iteration 47420 : Loss 2918.2257\n",
      "Iteration 47430 : Loss 2918.2106\n",
      "Iteration 47440 : Loss 2918.1955\n",
      "Iteration 47450 : Loss 2918.1804\n",
      "Iteration 47460 : Loss 2918.1653\n",
      "Iteration 47470 : Loss 2918.1502\n",
      "Iteration 47480 : Loss 2918.1352\n",
      "Iteration 47490 : Loss 2918.1201\n",
      "Iteration 47500 : Loss 2918.1051\n",
      "Iteration 47510 : Loss 2918.0900\n",
      "Iteration 47520 : Loss 2918.0750\n",
      "Iteration 47530 : Loss 2918.0600\n",
      "Iteration 47540 : Loss 2918.0450\n",
      "Iteration 47550 : Loss 2918.0300\n",
      "Iteration 47560 : Loss 2918.0151\n",
      "Iteration 47570 : Loss 2918.0001\n",
      "Iteration 47580 : Loss 2917.9852\n",
      "Iteration 47590 : Loss 2917.9702\n",
      "Iteration 47600 : Loss 2917.9553\n",
      "Iteration 47610 : Loss 2917.9404\n",
      "Iteration 47620 : Loss 2917.9255\n",
      "Iteration 47630 : Loss 2917.9106\n",
      "Iteration 47640 : Loss 2917.8957\n",
      "Iteration 47650 : Loss 2917.8809\n",
      "Iteration 47660 : Loss 2917.8660\n",
      "Iteration 47670 : Loss 2917.8512\n",
      "Iteration 47680 : Loss 2917.8364\n",
      "Iteration 47690 : Loss 2917.8216\n",
      "Iteration 47700 : Loss 2917.8068\n",
      "Iteration 47710 : Loss 2917.7920\n",
      "Iteration 47720 : Loss 2917.7772\n",
      "Iteration 47730 : Loss 2917.7624\n",
      "Iteration 47740 : Loss 2917.7477\n",
      "Iteration 47750 : Loss 2917.7329\n",
      "Iteration 47760 : Loss 2917.7182\n",
      "Iteration 47770 : Loss 2917.7035\n",
      "Iteration 47780 : Loss 2917.6888\n",
      "Iteration 47790 : Loss 2917.6741\n",
      "Iteration 47800 : Loss 2917.6594\n",
      "Iteration 47810 : Loss 2917.6448\n",
      "Iteration 47820 : Loss 2917.6301\n",
      "Iteration 47830 : Loss 2917.6155\n",
      "Iteration 47840 : Loss 2917.6008\n",
      "Iteration 47850 : Loss 2917.5862\n",
      "Iteration 47860 : Loss 2917.5716\n",
      "Iteration 47870 : Loss 2917.5570\n",
      "Iteration 47880 : Loss 2917.5424\n",
      "Iteration 47890 : Loss 2917.5279\n",
      "Iteration 47900 : Loss 2917.5133\n",
      "Iteration 47910 : Loss 2917.4987\n",
      "Iteration 47920 : Loss 2917.4842\n",
      "Iteration 47930 : Loss 2917.4697\n",
      "Iteration 47940 : Loss 2917.4552\n",
      "Iteration 47950 : Loss 2917.4407\n",
      "Iteration 47960 : Loss 2917.4262\n",
      "Iteration 47970 : Loss 2917.4117\n",
      "Iteration 47980 : Loss 2917.3972\n",
      "Iteration 47990 : Loss 2917.3828\n",
      "Iteration 48000 : Loss 2917.3683\n",
      "Iteration 48010 : Loss 2917.3539\n",
      "Iteration 48020 : Loss 2917.3395\n",
      "Iteration 48030 : Loss 2917.3251\n",
      "Iteration 48040 : Loss 2917.3107\n",
      "Iteration 48050 : Loss 2917.2963\n",
      "Iteration 48060 : Loss 2917.2819\n",
      "Iteration 48070 : Loss 2917.2676\n",
      "Iteration 48080 : Loss 2917.2532\n",
      "Iteration 48090 : Loss 2917.2389\n",
      "Iteration 48100 : Loss 2917.2246\n",
      "Iteration 48110 : Loss 2917.2103\n",
      "Iteration 48120 : Loss 2917.1959\n",
      "Iteration 48130 : Loss 2917.1817\n",
      "Iteration 48140 : Loss 2917.1674\n",
      "Iteration 48150 : Loss 2917.1531\n",
      "Iteration 48160 : Loss 2917.1389\n",
      "Iteration 48170 : Loss 2917.1246\n",
      "Iteration 48180 : Loss 2917.1104\n",
      "Iteration 48190 : Loss 2917.0962\n",
      "Iteration 48200 : Loss 2917.0819\n",
      "Iteration 48210 : Loss 2917.0678\n",
      "Iteration 48220 : Loss 2917.0536\n",
      "Iteration 48230 : Loss 2917.0394\n",
      "Iteration 48240 : Loss 2917.0252\n",
      "Iteration 48250 : Loss 2917.0111\n",
      "Iteration 48260 : Loss 2916.9969\n",
      "Iteration 48270 : Loss 2916.9828\n",
      "Iteration 48280 : Loss 2916.9687\n",
      "Iteration 48290 : Loss 2916.9546\n",
      "Iteration 48300 : Loss 2916.9405\n",
      "Iteration 48310 : Loss 2916.9264\n",
      "Iteration 48320 : Loss 2916.9123\n",
      "Iteration 48330 : Loss 2916.8983\n",
      "Iteration 48340 : Loss 2916.8842\n",
      "Iteration 48350 : Loss 2916.8702\n",
      "Iteration 48360 : Loss 2916.8562\n",
      "Iteration 48370 : Loss 2916.8421\n",
      "Iteration 48380 : Loss 2916.8281\n",
      "Iteration 48390 : Loss 2916.8141\n",
      "Iteration 48400 : Loss 2916.8002\n",
      "Iteration 48410 : Loss 2916.7862\n",
      "Iteration 48420 : Loss 2916.7722\n",
      "Iteration 48430 : Loss 2916.7583\n",
      "Iteration 48440 : Loss 2916.7444\n",
      "Iteration 48450 : Loss 2916.7304\n",
      "Iteration 48460 : Loss 2916.7165\n",
      "Iteration 48470 : Loss 2916.7026\n",
      "Iteration 48480 : Loss 2916.6887\n",
      "Iteration 48490 : Loss 2916.6748\n",
      "Iteration 48500 : Loss 2916.6610\n",
      "Iteration 48510 : Loss 2916.6471\n",
      "Iteration 48520 : Loss 2916.6333\n",
      "Iteration 48530 : Loss 2916.6194\n",
      "Iteration 48540 : Loss 2916.6056\n",
      "Iteration 48550 : Loss 2916.5918\n",
      "Iteration 48560 : Loss 2916.5780\n",
      "Iteration 48570 : Loss 2916.5642\n",
      "Iteration 48580 : Loss 2916.5504\n",
      "Iteration 48590 : Loss 2916.5367\n",
      "Iteration 48600 : Loss 2916.5229\n",
      "Iteration 48610 : Loss 2916.5092\n",
      "Iteration 48620 : Loss 2916.4954\n",
      "Iteration 48630 : Loss 2916.4817\n",
      "Iteration 48640 : Loss 2916.4680\n",
      "Iteration 48650 : Loss 2916.4543\n",
      "Iteration 48660 : Loss 2916.4406\n",
      "Iteration 48670 : Loss 2916.4269\n",
      "Iteration 48680 : Loss 2916.4133\n",
      "Iteration 48690 : Loss 2916.3996\n",
      "Iteration 48700 : Loss 2916.3860\n",
      "Iteration 48710 : Loss 2916.3723\n",
      "Iteration 48720 : Loss 2916.3587\n",
      "Iteration 48730 : Loss 2916.3451\n",
      "Iteration 48740 : Loss 2916.3315\n",
      "Iteration 48750 : Loss 2916.3179\n",
      "Iteration 48760 : Loss 2916.3043\n",
      "Iteration 48770 : Loss 2916.2908\n",
      "Iteration 48780 : Loss 2916.2772\n",
      "Iteration 48790 : Loss 2916.2637\n",
      "Iteration 48800 : Loss 2916.2501\n",
      "Iteration 48810 : Loss 2916.2366\n",
      "Iteration 48820 : Loss 2916.2231\n",
      "Iteration 48830 : Loss 2916.2096\n",
      "Iteration 48840 : Loss 2916.1961\n",
      "Iteration 48850 : Loss 2916.1826\n",
      "Iteration 48860 : Loss 2916.1691\n",
      "Iteration 48870 : Loss 2916.1557\n",
      "Iteration 48880 : Loss 2916.1422\n",
      "Iteration 48890 : Loss 2916.1288\n",
      "Iteration 48900 : Loss 2916.1154\n",
      "Iteration 48910 : Loss 2916.1019\n",
      "Iteration 48920 : Loss 2916.0885\n",
      "Iteration 48930 : Loss 2916.0751\n",
      "Iteration 48940 : Loss 2916.0618\n",
      "Iteration 48950 : Loss 2916.0484\n",
      "Iteration 48960 : Loss 2916.0350\n",
      "Iteration 48970 : Loss 2916.0217\n",
      "Iteration 48980 : Loss 2916.0083\n",
      "Iteration 48990 : Loss 2915.9950\n",
      "Iteration 49000 : Loss 2915.9817\n",
      "Iteration 49010 : Loss 2915.9684\n",
      "Iteration 49020 : Loss 2915.9551\n",
      "Iteration 49030 : Loss 2915.9418\n",
      "Iteration 49040 : Loss 2915.9285\n",
      "Iteration 49050 : Loss 2915.9153\n",
      "Iteration 49060 : Loss 2915.9020\n",
      "Iteration 49070 : Loss 2915.8888\n",
      "Iteration 49080 : Loss 2915.8755\n",
      "Iteration 49090 : Loss 2915.8623\n",
      "Iteration 49100 : Loss 2915.8491\n",
      "Iteration 49110 : Loss 2915.8359\n",
      "Iteration 49120 : Loss 2915.8227\n",
      "Iteration 49130 : Loss 2915.8095\n",
      "Iteration 49140 : Loss 2915.7964\n",
      "Iteration 49150 : Loss 2915.7832\n",
      "Iteration 49160 : Loss 2915.7700\n",
      "Iteration 49170 : Loss 2915.7569\n",
      "Iteration 49180 : Loss 2915.7438\n",
      "Iteration 49190 : Loss 2915.7307\n",
      "Iteration 49200 : Loss 2915.7176\n",
      "Iteration 49210 : Loss 2915.7045\n",
      "Iteration 49220 : Loss 2915.6914\n",
      "Iteration 49230 : Loss 2915.6783\n",
      "Iteration 49240 : Loss 2915.6652\n",
      "Iteration 49250 : Loss 2915.6522\n",
      "Iteration 49260 : Loss 2915.6391\n",
      "Iteration 49270 : Loss 2915.6261\n",
      "Iteration 49280 : Loss 2915.6131\n",
      "Iteration 49290 : Loss 2915.6001\n",
      "Iteration 49300 : Loss 2915.5871\n",
      "Iteration 49310 : Loss 2915.5741\n",
      "Iteration 49320 : Loss 2915.5611\n",
      "Iteration 49330 : Loss 2915.5481\n",
      "Iteration 49340 : Loss 2915.5352\n",
      "Iteration 49350 : Loss 2915.5222\n",
      "Iteration 49360 : Loss 2915.5093\n",
      "Iteration 49370 : Loss 2915.4964\n",
      "Iteration 49380 : Loss 2915.4835\n",
      "Iteration 49390 : Loss 2915.4705\n",
      "Iteration 49400 : Loss 2915.4577\n",
      "Iteration 49410 : Loss 2915.4448\n",
      "Iteration 49420 : Loss 2915.4319\n",
      "Iteration 49430 : Loss 2915.4190\n",
      "Iteration 49440 : Loss 2915.4062\n",
      "Iteration 49450 : Loss 2915.3933\n",
      "Iteration 49460 : Loss 2915.3805\n",
      "Iteration 49470 : Loss 2915.3677\n",
      "Iteration 49480 : Loss 2915.3549\n",
      "Iteration 49490 : Loss 2915.3421\n",
      "Iteration 49500 : Loss 2915.3293\n",
      "Iteration 49510 : Loss 2915.3165\n",
      "Iteration 49520 : Loss 2915.3037\n",
      "Iteration 49530 : Loss 2915.2909\n",
      "Iteration 49540 : Loss 2915.2782\n",
      "Iteration 49550 : Loss 2915.2654\n",
      "Iteration 49560 : Loss 2915.2527\n",
      "Iteration 49570 : Loss 2915.2400\n",
      "Iteration 49580 : Loss 2915.2273\n",
      "Iteration 49590 : Loss 2915.2146\n",
      "Iteration 49600 : Loss 2915.2019\n",
      "Iteration 49610 : Loss 2915.1892\n",
      "Iteration 49620 : Loss 2915.1765\n",
      "Iteration 49630 : Loss 2915.1639\n",
      "Iteration 49640 : Loss 2915.1512\n",
      "Iteration 49650 : Loss 2915.1386\n",
      "Iteration 49660 : Loss 2915.1260\n",
      "Iteration 49670 : Loss 2915.1133\n",
      "Iteration 49680 : Loss 2915.1007\n",
      "Iteration 49690 : Loss 2915.0881\n",
      "Iteration 49700 : Loss 2915.0755\n",
      "Iteration 49710 : Loss 2915.0630\n",
      "Iteration 49720 : Loss 2915.0504\n",
      "Iteration 49730 : Loss 2915.0378\n",
      "Iteration 49740 : Loss 2915.0253\n",
      "Iteration 49750 : Loss 2915.0127\n",
      "Iteration 49760 : Loss 2915.0002\n",
      "Iteration 49770 : Loss 2914.9877\n",
      "Iteration 49780 : Loss 2914.9752\n",
      "Iteration 49790 : Loss 2914.9627\n",
      "Iteration 49800 : Loss 2914.9502\n",
      "Iteration 49810 : Loss 2914.9377\n",
      "Iteration 49820 : Loss 2914.9253\n",
      "Iteration 49830 : Loss 2914.9128\n",
      "Iteration 49840 : Loss 2914.9003\n",
      "Iteration 49850 : Loss 2914.8879\n",
      "Iteration 49860 : Loss 2914.8755\n",
      "Iteration 49870 : Loss 2914.8631\n",
      "Iteration 49880 : Loss 2914.8507\n",
      "Iteration 49890 : Loss 2914.8383\n",
      "Iteration 49900 : Loss 2914.8259\n",
      "Iteration 49910 : Loss 2914.8135\n",
      "Iteration 49920 : Loss 2914.8011\n",
      "Iteration 49930 : Loss 2914.7888\n",
      "Iteration 49940 : Loss 2914.7764\n",
      "Iteration 49950 : Loss 2914.7641\n",
      "Iteration 49960 : Loss 2914.7517\n",
      "Iteration 49970 : Loss 2914.7394\n",
      "Iteration 49980 : Loss 2914.7271\n",
      "Iteration 49990 : Loss 2914.7148\n",
      "Iteration 50000 : Loss 2914.7025\n"
     ]
    }
   ],
   "source": [
    "\n",
    "losses = []\n",
    "\n",
    "# 200000 번 진행 \n",
    "for i in range(1, 50001):\n",
    "    dW, db = gradient(X_train, W, b, y_train)\n",
    "    dW = dW.astype(np.float64)\n",
    "    db = float(db)\n",
    "    \n",
    "    W -= LEARNING_RATE3 * dW # 마치 w' = w - Lg의 과정임. 이로써 dw가 0이 되는 값을 찾아감\n",
    "    b -= LEARNING_RATE3 * db\n",
    "    L = loss(X_train, W, b, y_train)\n",
    "    losses.append(L)\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i, L))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" LEARNING_RATE 변형 시도\n",
    "# LEARNING_RATE2로 진행\n",
    "\n",
    "losses2 = []\n",
    "# \n",
    "# 2000 번 진행 LEaring\n",
    "for i in range(1, 2001):\n",
    "    dW, db = gradient(X_train, W, b, y_train)\n",
    "    dW = dW.astype(np.float64)\n",
    "    db = float(db)\n",
    "    \n",
    "    W -= LEARNING_RATE2 * dW # 마치 w' = w - Lg의 과정임. 이로써 dw가 0이 되는 값을 찾아감\n",
    "    b -= LEARNING_RATE2 * db\n",
    "    L = loss(X_train, W, b, y_train)\n",
    "    losses2.append(L)\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i, L))\n",
    "\n",
    "\n",
    "# LEARNING_RATE3로 진행\n",
    "\n",
    "losses3 = []\n",
    "\n",
    "# 2000 번 진행 LEaring\n",
    "for i in range(1, 2001):\n",
    "    dW, db = gradient(X_train, W, b, y_train)\n",
    "    dW = dW.astype(np.float64)\n",
    "    db = float(db)\n",
    "    \n",
    "    W -= LEARNING_RATE3 * dW # 마치 w' = w - Lg의 과정임. 이로써 dw가 0이 되는 값을 찾아감\n",
    "    b -= LEARNING_RATE3 * db\n",
    "    L = loss(X_train, W, b, y_train)\n",
    "    losses3.append(L)\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i, L))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(10) test 데이터에 대한 성능 확인하기  \n",
    "test 데이터에 대한 성능을 확인해주세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2866.8403\n"
     ]
    }
   ],
   "source": [
    "# 1번째 base 모델\n",
    "prediction1 = model_diabetes(X_test, W, b)\n",
    "test_loss = MSE(prediction1, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(11) 정답 데이터와 예측한 데이터 시각화하기  \n",
    "x축에는 X 데이터의 첫 번째 컬럼을, y축에는 정답인 target 데이터를 넣어서 모델이 예측한 데이터를 시각화해 주세요.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x11a205190>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sex feature를 삭제한 data\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(X_test[:, 0], y_test, label=\"Actual values\")\n",
    "plt.scatter(X_test[:, 0], prediction2, c='red', label=\"Predicted values\")\n",
    "\n",
    "plt.xlabel(\"First Feature of X_test = Age\")  # X_test의 첫 번째 특성 = age\n",
    "plt.ylabel(\"Target Value = Diabets level\")  # 타겟 값 = 당뇨수치\n",
    "\n",
    "plt.title(\"Actual vs Predicted Values\")\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss 줄이기 추가 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (9)-2. X column에서 데이터 선별해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sex data 제외 \n",
    "df_X_filter1 = np.delete(df_X, 1, axis=1)  # 1번 인덱스 열(sex) 제거\n",
    "\n",
    "LEARNING_RATE3 = 0.01\n",
    "\n",
    "W = np.random.rand(9)\n",
    "b = np.random.rand()\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(df_X_filter1, df_y, test_size=0.2, random_state=30)\n",
    "\n",
    "# sex data 제외, feature 9개로 감소 \n",
    "\n",
    "losses_filter1 = []\n",
    "\n",
    "def model_diabetes2(X, W, b): # y = w1x1 + w2x2 ... w10x10 + b \n",
    "                             # 우리는 결국 w와 b 값을 찾고 싶은 것\n",
    "    predictions = 0\n",
    "    for i in range(9):\n",
    "        predictions += X[:, i] * W[i]\n",
    "    predictions += b\n",
    "    return predictions\n",
    "\n",
    "\n",
    "\n",
    "# filter 모델 2000 번 진행 \n",
    "for i in range(1, 20001):\n",
    "    dW, db = gradient(X_train2, W, b, y_train2)\n",
    "    dW = dW.astype(np.float64)\n",
    "    db = float(db)\n",
    "    \n",
    "    W -= LEARNING_RATE3 * dW # 마치 w' = w - Lg의 과정임. 이로써 dw가 0이 되는 값을 찾아감\n",
    "    b -= LEARNING_RATE3 * db\n",
    "    L = loss(X_train2, W, b, y_train2)\n",
    "    losses_filter1.append(L)\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i, L))\n",
    "        \n",
    "# 2번째 sex data 제외 모델 성능 확인\n",
    "prediction2 = model_diabetes2(X_test2, W, b)\n",
    "test_loss = MSE(prediction2, y_test2)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y 데이터 정규화 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y data의 min Max 값\n",
    "print('min_df_y: {} ' 'Max_df_y: {}'.format(min(df_y), max(df_y))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y 데이터 정규화, sex 컬럼 제외 기준\n",
    "y_train_normalized = y_train / 346\n",
    "y_test_normalized = y_test / 346\n",
    "\n",
    "\n",
    "# 학습률과 가중치 초기화\n",
    "LEARNING_RATE = 0.01\n",
    "W = np.random.rand(9)\n",
    "b = np.random.rand()\n",
    "\n",
    "losses_normalized = []\n",
    "\n",
    "# normalized data, filter 모델 20000 번 진행 \n",
    "for i in range(1, 200000):\n",
    "    dW, db = gradient(X_train2, W, b, y_train2)\n",
    "    dW = dW.astype(np.float64)\n",
    "    db = float(db)\n",
    "    \n",
    "    W -= LEARNING_RATE * dW # 마치 w' = w - Lg의 과정임. 이로써 dw가 0이 되는 값을 찾아감\n",
    "    b -= LEARNING_RATE * db\n",
    "    L = loss(X_train2, W, b, y_train2)\n",
    "    losses_filter1.append(L)\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i, L))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2번째 sex data 제외 모델 \n",
    "prediction3_normalized = model_diabetes2(X_test2, W, b)\n",
    "test_loss = MSE(prediction3_normalized, y_test2)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다시 기본 예제 돌려보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
